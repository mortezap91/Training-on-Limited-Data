{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vV6DxGkPeQDD"
      },
      "source": [
        "***Challenge 1***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72hfgxjTd_lk"
      },
      "source": [
        "Here the goal is to train on 25 samples. In this preliminary testbed the evaluation will be done on a 2000 sample validation set. Note in the end the final evaluation will be done on the full CIFAR-10 test set as well as potentially a separate dataset. The validation samples here should not be used for training in any way, the final evaluation will provide only random samples of 25 from a datasource that is not the CIFAR-10 training data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zk0Ilt_-duk2"
      },
      "source": [
        "Feel free to modify this testbed to your liking, including the normalization transformations etc. Note however the final evaluation testbed will have a rigid set of components where you will need to place your answer. The only constraint is the data. Refer to the full project instructions for more information.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWyBTUe3idZI"
      },
      "source": [
        "Setup training functions. Again you are free to fully modify this testbed in your prototyping within the constraints of the data used. You can use tools outside of pytorch for training models if desired as well although the torchvision dataloaders will still be useful for interacting with the cifar-10 dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N7soYNWEedl9"
      },
      "outputs": [],
      "source": [
        "def train(model, device, train_loader, optimizer, epoch, display=True):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.cross_entropy(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    if display:\n",
        "      print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "          epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "          100. * batch_idx / len(train_loader), loss.item()))\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.cross_entropy(output, target, size_average=False).item() # sum up batch loss\n",
        "            pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n",
        "    return 100. * correct / len(test_loader.dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G4hpe7QbQFnr"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Net(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.layers = nn.ModuleList()\n",
        "\n",
        "        self.layers+=[nn.Conv2d(3, 16,  kernel_size=3) ,\n",
        "                      nn.ReLU(inplace=True)]\n",
        "        self.layers+=[nn.Conv2d(16, 16,  kernel_size=3, stride=2),\n",
        "                      nn.ReLU(inplace=True)]\n",
        "        self.layers+=[nn.Conv2d(16, 32,  kernel_size=3),\n",
        "                      nn.ReLU(inplace=True)]\n",
        "        self.layers+=[nn.Conv2d(32, 32,  kernel_size=3, stride=2),\n",
        "                      nn.ReLU(inplace=True)]\n",
        "        self.fc = nn.Linear(32*5*5, 10)\n",
        "    def forward(self, x):\n",
        "        for i in range(len(self.layers)):\n",
        "          x = self.layers[i](x)\n",
        "        x = x.view(-1, 32*5*5)\n",
        "        x = self.fc(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jPjWBE4MerTX"
      },
      "source": [
        "The below tries  2 random problem instances. In your development you may choose to prototype with 1 problem instances but keep in mind for small sample problems the variance is high so continously evaluating on several subsets will be important."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        },
        "id": "6v7xU1HMelJ3",
        "outputId": "9772d83e-a2e1-4ee5-a11c-b9babb14c4c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Num Samples For Training 50 Num Samples For Val 400\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-cc95a393e653>\u001b[0m in \u001b[0;36m<cell line: 25>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m   \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m   \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m   optimizer = torch.optim.SGD(model.parameters(),lr=0.01, momentum=0.9,\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Net' is not defined"
          ]
        }
      ],
      "source": [
        "from numpy.random import RandomState\n",
        "import numpy as np\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Subset\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "\n",
        "normalize = transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))\n",
        "\n",
        "transform_val = transforms.Compose([transforms.ToTensor(), normalize]) #careful to keep this one same\n",
        "transform_train = transforms.Compose([transforms.ToTensor(), normalize])\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "##### Cifar Data\n",
        "cifar_data = datasets.CIFAR10(root='.',train=True, transform=transform_train, download=True)\n",
        "\n",
        "#We need two copies of this due to weird dataset api\n",
        "cifar_data_val = datasets.CIFAR10(root='.',train=True, transform=transform_val, download=True)\n",
        "\n",
        "\n",
        "accs = []\n",
        "\n",
        "for seed in range(1, 5):\n",
        "  prng = RandomState(seed)\n",
        "  random_permute = prng.permutation(np.arange(0, 1000))\n",
        "  classes =  prng.permutation(np.arange(0,10))\n",
        "  indx_train = np.concatenate([np.where(np.array(cifar_data.targets) == classe)[0][random_permute[0:25]] for classe in classes[0:2]])\n",
        "  indx_val = np.concatenate([np.where(np.array(cifar_data.targets) == classe)[0][random_permute[25:225]] for classe in classes[0:2]])\n",
        "\n",
        "\n",
        "  train_data = Subset(cifar_data, indx_train)\n",
        "  val_data = Subset(cifar_data_val, indx_val)\n",
        "\n",
        "  print('Num Samples For Training %d Num Samples For Val %d'%(train_data.indices.shape[0],val_data.indices.shape[0]))\n",
        "\n",
        "  train_loader = torch.utils.data.DataLoader(train_data,\n",
        "                                             batch_size=128,\n",
        "                                             shuffle=True)\n",
        "\n",
        "  val_loader = torch.utils.data.DataLoader(val_data,\n",
        "                                           batch_size=128,\n",
        "                                           shuffle=False)\n",
        "\n",
        "\n",
        "  model = Net()\n",
        "  model.to(device)\n",
        "  optimizer = torch.optim.SGD(model.parameters(),lr=0.01, momentum=0.9,\n",
        "                              weight_decay=0.0005)\n",
        "  for epoch in range(100):\n",
        "    train(model, device, train_loader, optimizer, epoch, display=epoch%5==0)\n",
        "\n",
        "  accs.append(test(model, device, val_loader))\n",
        "\n",
        "accs = np.array(accs)\n",
        "print('Acc over 5 instances: %.2f +- %.2f'%(accs.mean(),accs.std()))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8GlJkOdfYY0"
      },
      "source": [
        "***Challenge 2***\n",
        "\n",
        "You may use the same testbed but without the constraints on external datasets or models trained on exeternal datasets. You may not however use any of the CIFAR-10 training set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MHDKkBCFQEO0"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "def Offline_aug(x,y):\n",
        "  y1 = y.squeeze()\n",
        "  assert x.shape[0]==y1.shape[0]\n",
        "  tr_x = []\n",
        "  tr_y = []\n",
        "  for i in range(x.shape[0]):\n",
        "    x_ = transforms.ToPILImage()(x[i])\n",
        "    x1 = transforms.RandomRotation(40)(x_)\n",
        "    x2 = transforms.RandomHorizontalFlip()(x_)\n",
        "    x3 = transforms.RandomVerticalFlip()(x_)\n",
        "    x4 = transforms.ColorJitter()(x_)\n",
        "    x5 = transforms.AutoAugment()(x_)\n",
        "    tr_x.append(transforms.ToTensor()(x_))\n",
        "    tr_x.append(transforms.ToTensor()(x1))\n",
        "    tr_x.append(transforms.ToTensor()(x2))\n",
        "    tr_y.append(y1[i])\n",
        "    tr_y.append(y1[i])\n",
        "    tr_y.append(y1[i])\n",
        "  tr_x = torch.stack(tr_x)\n",
        "  tr_y = torch.stack(tr_y)\n",
        "  return tr_x,tr_y\n",
        "\n",
        "class CustomTensorDataset(Dataset):\n",
        "    \"\"\"TensorDataset with support of transforms.\n",
        "    \"\"\"\n",
        "    def __init__(self, tensors, transform=None):\n",
        "        assert all(tensors[0].size(0) == tensor.size(0) for tensor in tensors)\n",
        "        self.tensors = tensors\n",
        "        self.transform = transform\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        x = self.tensors[0][index]\n",
        "\n",
        "        if self.transform:\n",
        "            x = self.transform(x)\n",
        "\n",
        "        y = self.tensors[1][index]\n",
        "\n",
        "        return x, y\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.tensors[0].size(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t8LRLTaGt1g2",
        "outputId": "15f1982f-8fab-40e8-f256-44ff553eb03f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting efficientnet-pytorch\n",
            "  Downloading efficientnet_pytorch-0.7.1.tar.gz (21 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from efficientnet-pytorch) (2.0.0+cu118)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch) (3.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch) (1.11.1)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch) (2.0.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch) (4.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch) (3.12.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch) (3.1.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->efficientnet-pytorch) (16.0.2)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->efficientnet-pytorch) (3.25.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->efficientnet-pytorch) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->efficientnet-pytorch) (1.3.0)\n",
            "Building wheels for collected packages: efficientnet-pytorch\n",
            "  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.7.1-py3-none-any.whl size=16444 sha256=8c9ba3d2c567256b18baa0e41d6545da4c76b5d93da0d8a21586a7d44512b831\n",
            "  Stored in directory: /root/.cache/pip/wheels/03/3f/e9/911b1bc46869644912bda90a56bcf7b960f20b5187feea3baf\n",
            "Successfully built efficientnet-pytorch\n",
            "Installing collected packages: efficientnet-pytorch\n",
            "Successfully installed efficientnet-pytorch-0.7.1\n"
          ]
        }
      ],
      "source": [
        "!pip install efficientnet-pytorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "dktQa6mWiHYZ",
        "outputId": "b9b07197-7e6d-4097-955a-66ad1e17871b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "seed number is 5\n",
            "Num Samples For Training 50 Num Samples For Val 400\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://github.com/lukemelas/EfficientNet-PyTorch/releases/download/1.0/efficientnet-b1-f1951068.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet-b1-f1951068.pth\n",
            "100%|██████████| 30.1M/30.1M [00:00<00:00, 176MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded pretrained weights for efficientnet-b1\n",
            "Train Epoch: 0 [88/150 (80%)]\tLoss: 172.086456\n",
            "\n",
            "Test set: Average loss: 0.0460, Accuracy: 307/400 (76.75%)\n",
            "\n",
            "76.75\n",
            "Train Epoch: 1 [88/150 (80%)]\tLoss: 170.285538\n",
            "\n",
            "Test set: Average loss: 0.0106, Accuracy: 373/400 (93.25%)\n",
            "\n",
            "93.25\n",
            "Train Epoch: 2 [88/150 (80%)]\tLoss: 169.713120\n",
            "\n",
            "Test set: Average loss: 0.0051, Accuracy: 381/400 (95.25%)\n",
            "\n",
            "95.25\n",
            "Train Epoch: 3 [88/150 (80%)]\tLoss: 169.602020\n",
            "\n",
            "Test set: Average loss: 0.0051, Accuracy: 374/400 (93.50%)\n",
            "\n",
            "93.5\n",
            "Train Epoch: 4 [88/150 (80%)]\tLoss: 169.566971\n",
            "\n",
            "Test set: Average loss: 0.0044, Accuracy: 380/400 (95.00%)\n",
            "\n",
            "95.0\n",
            "Train Epoch: 5 [88/150 (80%)]\tLoss: 169.513657\n",
            "\n",
            "Test set: Average loss: 0.0038, Accuracy: 382/400 (95.50%)\n",
            "\n",
            "95.5\n",
            "Train Epoch: 6 [88/150 (80%)]\tLoss: 169.505707\n",
            "\n",
            "Test set: Average loss: 0.0036, Accuracy: 382/400 (95.50%)\n",
            "\n",
            "95.5\n",
            "Train Epoch: 7 [88/150 (80%)]\tLoss: 169.489517\n",
            "\n",
            "Test set: Average loss: 0.0039, Accuracy: 382/400 (95.50%)\n",
            "\n",
            "95.5\n",
            "Train Epoch: 8 [88/150 (80%)]\tLoss: 169.516418\n",
            "\n",
            "Test set: Average loss: 0.0042, Accuracy: 381/400 (95.25%)\n",
            "\n",
            "95.25\n",
            "Train Epoch: 9 [88/150 (80%)]\tLoss: 169.492767\n",
            "\n",
            "Test set: Average loss: 0.0041, Accuracy: 381/400 (95.25%)\n",
            "\n",
            "95.25\n",
            "seed number is 6\n",
            "Num Samples For Training 50 Num Samples For Val 400\n",
            "Loaded pretrained weights for efficientnet-b1\n",
            "Train Epoch: 0 [88/150 (80%)]\tLoss: 172.142456\n",
            "\n",
            "Test set: Average loss: 0.0442, Accuracy: 282/400 (70.50%)\n",
            "\n",
            "70.5\n",
            "Train Epoch: 1 [88/150 (80%)]\tLoss: 170.181992\n",
            "\n",
            "Test set: Average loss: 0.0096, Accuracy: 377/400 (94.25%)\n",
            "\n",
            "94.25\n",
            "Train Epoch: 2 [88/150 (80%)]\tLoss: 169.743484\n",
            "\n",
            "Test set: Average loss: 0.0046, Accuracy: 381/400 (95.25%)\n",
            "\n",
            "95.25\n",
            "Train Epoch: 3 [88/150 (80%)]\tLoss: 169.519882\n",
            "\n",
            "Test set: Average loss: 0.0040, Accuracy: 381/400 (95.25%)\n",
            "\n",
            "95.25\n"
          ]
        }
      ],
      "source": [
        "import torchvision.models as models\n",
        "import torch.nn as nn\n",
        "from numpy.random import RandomState\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Subset\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "import pickle\n",
        "from efficientnet_pytorch import EfficientNet\n",
        "\n",
        "#torch.cuda.memory_summary(device=None, abbreviated=False)\n",
        "\n",
        "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                  std=[0.229, 0.224, 0.225])\n",
        "resize = transforms.Resize(224)\n",
        "\n",
        "# We resize images to allow using imagenet pre-trained models, is there a better way?\n",
        "resize = transforms.Resize(224)\n",
        "\n",
        "transform_val = transforms.Compose([resize, transforms.ToTensor(), normalize]) #careful to keep this one same\n",
        "transform_train = transforms.Compose([transforms.ToPILImage(), resize, transforms.ToTensor(), normalize])\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "print(device) # you will really need gpu's for this part\n",
        "\n",
        "##### Cifar Data\n",
        "cifar_data = datasets.CIFAR10(root='.',train=True, transform=transforms.ToTensor(), download=True)\n",
        "cifar_data100 = datasets.CIFAR100(root='.',train=True, transform=transform_val, download=True)\n",
        "\n",
        "#We need two copies of this due to weird dataset api\n",
        "cifar_data_val = datasets.CIFAR10(root='.',train=True, transform=transform_val, download=True)\n",
        "\n",
        "accs = []\n",
        "l2_lambda = 0.001\n",
        "step = 1\n",
        "num_epoch = 4\n",
        "\n",
        "Valid_accuracies = []\n",
        "Valid_loss = []\n",
        "Train_loss = []\n",
        "\n",
        "for seed in range(1,25):\n",
        "  print('seed number is %d'%(seed))\n",
        "  prng = RandomState(seed)\n",
        "  random_permute = prng.permutation(np.arange(0, 5000))\n",
        "  classes =  prng.permutation(np.arange(0,10))\n",
        "  classes1 =  prng.permutation(np.arange(0,100))\n",
        "  indx_train = np.concatenate([np.where(np.array(cifar_data.targets) == classe)[0][random_permute[0:25]] for classe in classes[0:2]])\n",
        "  indx_val = np.concatenate([np.where(np.array(cifar_data.targets) == classe)[0][random_permute[25:225]] for classe in classes[0:2]])\n",
        "  indx_train100 = np.concatenate([np.where(np.array(cifar_data100.targets) == classe)[0] for classe in classes[0:2]])\n",
        "\n",
        "  train_data = Subset(cifar_data, indx_train)\n",
        "  val_data = Subset(cifar_data_val, indx_val)\n",
        "  train_data100 = Subset(cifar_data100,indx_train100)\n",
        "\n",
        "  print('Num Samples For Training %d Num Samples For Val %d'%(train_data.indices.shape[0],val_data.indices.shape[0]))\n",
        "\n",
        "  train_loader = torch.utils.data.DataLoader(train_data,\n",
        "                                             batch_size=50,\n",
        "                                             shuffle=True)\n",
        "\n",
        "  train_loader100 = torch.utils.data.DataLoader(train_data100,\n",
        "                                             batch_size=200,#200\n",
        "                                             shuffle=True)\n",
        "  val_loader = torch.utils.data.DataLoader(val_data,\n",
        "                                           batch_size=32,\n",
        "                                           shuffle=True)\n",
        "\n",
        "  trainset = []\n",
        "  targetset = []\n",
        "  for data, target in (train_loader):\n",
        "    trainset.append(data)\n",
        "    targetset.append(target)\n",
        "  trainset = torch.stack(trainset).squeeze()\n",
        "  targetset = torch.stack(targetset)\n",
        "\n",
        "  X,Y = Offline_aug(trainset,targetset)\n",
        "\n",
        "  train_dataset_normal = CustomTensorDataset(tensors=(X, Y), transform=transform_train)\n",
        "\n",
        "  train_loader_new = torch.utils.data.DataLoader(train_dataset_normal,\n",
        "                                              batch_size=32,\n",
        "                                              shuffle=True)\n",
        "\n",
        "\n",
        "  #model = models.alexnet(pretrained=True)\n",
        "  model = EfficientNet.from_pretrained('efficientnet-b1')\n",
        "  for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "  #model.classifier = nn.Linear(256 * 6 * 6, 10)\n",
        "  model ._fc= torch.nn.Linear(in_features=model._fc.in_features, out_features=10, bias=True)\n",
        "  optimizer = torch.optim.SGD(model._fc.parameters(),\n",
        "                                lr=0.01, momentum=0.9,\n",
        "                              weight_decay=0.0005)\n",
        "\n",
        " # model.to(device)\n",
        "  model.train()\n",
        "  train_loss = 0\n",
        "  for epoch in range(num_epoch):\n",
        "    for batch_idx, ((data, target),(data100,target100)) in enumerate(zip(train_loader_new,train_loader100)):\n",
        "\n",
        "      #data = data.to(device)\n",
        "      #target = target.to(device)\n",
        "      #data100 = data100.to(device)\n",
        "      #target100 = target100.to(device)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      output = (model(data.float())).squeeze()\n",
        "      output100 = (model(data100.float())).squeeze()\n",
        "\n",
        "      loss1 = F.cross_entropy(output, target)\n",
        "      loss2 = F.cross_entropy(output100, target100)\n",
        "\n",
        "      l2_norm = sum(p.pow(2.0).sum()\n",
        "                  for p in model.parameters())\n",
        "\n",
        "      loss = loss1 + l2_lambda * l2_norm +loss2  #L2 Regularization\n",
        "      train_loss +=loss\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "    train_loss /= len(train_loader_new.dataset)\n",
        "    Train_loss.append(train_loss)\n",
        "    #print(f\"Loss at epoch {epoch} = {mean_loss}\")\n",
        "    if epoch%step==0:\n",
        "      print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "        epoch, batch_idx * len(data), len(train_loader_new.dataset),\n",
        "      100. * batch_idx / len(train_loader_new), loss.item()))\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for data, target in val_loader:\n",
        "           # data, target = data.to(device), target.to(device)\n",
        "            output = model(data.float())\n",
        "            test_loss += F.cross_entropy(output, target).item() # sum up batch loss\n",
        "            pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "\n",
        "\n",
        "    #print(f\"Loss at epoch {epoch} = {mean_loss}\")\n",
        "    test_loss /= len(val_loader.dataset)\n",
        "    Valid_loss.append(test_loss)\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
        "        test_loss, correct, len(val_loader.dataset),\n",
        "        100. * correct / len(val_loader.dataset)))\n",
        "    print(100. * correct / len(val_loader.dataset))\n",
        "    Valid_accuracies.append(correct / len(val_loader.dataset))\n",
        "  with open('/content/drive/MyDrive/Results2/Train_loss', 'wb') as fp:\n",
        "      pickle.dump(Train_loss, fp)\n",
        "  with open('/content/drive/MyDrive/Results2/Valid_loss', 'wb') as fp:\n",
        "      pickle.dump(Valid_loss, fp)\n",
        "  with open('/content/drive/MyDrive/Results2/Valid_accuracy', 'wb') as fp:\n",
        "      pickle.dump(Valid_accuracies, fp)\n",
        "print(f'Mean Train Acc over 25 seeds: '\\\n",
        "      f'{np.mean(Valid_accuracies):.2%} '\\\n",
        "      f'+- {np.std(Valid_accuracies):.2}')\n",
        "\n",
        "print(f'Mean Loss Acc over 25 seeds: '\\\n",
        "      f'{np.mean(Valid_loss):.2%} '\\\n",
        "      f'+- {np.std(Valid_loss):.2}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "id": "C0Xjy0T7jfd9",
        "outputId": "d3629671-c7e4-4ed8-cc86-cddd5ac933b9"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-638f8b21165b>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/Results2/Valid_loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m       \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Train_loss' is not defined"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "with open('/content/drive/MyDrive/Results2/Valid_loss', 'wb') as fp:\n",
        "      pickle.dump(Train_loss, fp)\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LoZrLwhFEQkk",
        "outputId": "db685412-1551-487f-e436-1cfa28b18184"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mean Train Acc over 25 seeds: 81.53% +- 0.15\n",
            "Mean Loss Acc over 25 seeds: 1.83% +- 0.015\n"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "with open('/content/drive/MyDrive/Results2/Valid_accuracy','rb') as f:\n",
        "    x = pickle.load(f)\n",
        "\n",
        "with open('/content/drive/MyDrive/Results2/Valid_loss','rb') as f:\n",
        "    x2 = pickle.load(f)\n",
        "#x1 = torch.stack(x)\n",
        "#plt.plot(x1.detach().cpu())\n",
        "print(f'Mean Train Acc over 25 seeds: '\\\n",
        "      f'{np.mean(x):.2%} '\\\n",
        "      f'+- {np.std(x):.2}')\n",
        "\n",
        "print(f'Mean Loss Acc over 25 seeds: '\\\n",
        "      f'{np.mean(x2):.2%} '\\\n",
        "      f'+- {np.std(x2):.2}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240
        },
        "id": "I_qnufZqv3WL",
        "outputId": "c2807a71-2dbe-4333-9b9f-c1ad32fa0da6"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-3c3386018d7c>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m print(f'Mean Train Acc over 25 seeds: '\\\n\u001b[0;32m----> 2\u001b[0;31m       \u001b[0;34mf'{np.mean(Valid_accuracies):.2%} '\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m       f'+- {np.std(Valid_accuracies):.2}')\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m print(f'Mean Loss Acc over 25 seeds: '\\\n",
            "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
          ]
        }
      ],
      "source": [
        "print(f'Mean Train Acc over 25 seeds: '\\\n",
        "      f'{np.mean(Valid_accuracies):.2%} '\\\n",
        "      f'+- {np.std(Valid_accuracies):.2}')\n",
        "\n",
        "print(f'Mean Loss Acc over 25 seeds: '\\\n",
        "      f'{np.mean(Valid_loss):.2%} '\\\n",
        "      f'+- {np.std(Valid_loss):.2}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_OkfsJWx0WMO"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJnkXwsmp90i"
      },
      "source": [
        "# New section"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}