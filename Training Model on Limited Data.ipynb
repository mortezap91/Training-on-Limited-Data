{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "GijiWxSfvEmH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy.random import RandomState\n",
        "import numpy as np\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Subset\n",
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import Dataset, TensorDataset\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "#!pip install git+https://github.com/aleju/imgaug\n",
        "import random\n",
        "import pickle"
      ],
      "metadata": {
        "id": "-D8D1wp21Xb8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model and necesarry classes"
      ],
      "metadata": {
        "id": "eepDQ9LA1bMH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "c = 16\n",
        "h1 = 4\n",
        "w1 = 4\n",
        "ch = 3\n",
        "h = 4\n",
        "w = 4\n",
        "class Net1(torch.nn.Module):\n",
        "  def __init__ (self):\n",
        "    super(Net1,self).__init__()\n",
        "    self.conv1 = nn.Conv2d(in_channels=ch, out_channels=32, kernel_size=3, stride=1, padding='same')\n",
        "    self.bn1 = nn.BatchNorm2d(32)\n",
        "    self.pool1 = nn.MaxPool2d(kernel_size = 2)\n",
        "    self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding='same')\n",
        "    self.bn2 = nn.BatchNorm2d(64)\n",
        "    self.pool2 = nn.MaxPool2d(kernel_size = 2)\n",
        "    self.conv3 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding='same')\n",
        "    self.bn3 = nn.BatchNorm2d(64)\n",
        "    self.pool3 = nn.MaxPool2d(kernel_size = 2)\n",
        "    self.conv4 = nn.Conv2d(in_channels=64, out_channels=16, kernel_size=3, stride=1, padding='same')\n",
        "    self.bn4 = nn.BatchNorm2d(16)\n",
        "    self.pool4 = nn.MaxPool2d(kernel_size = 2)\n",
        "\n",
        "    self.downsample1 = nn.Conv2d(ch, 64, kernel_size=1, stride=2, bias=False)\n",
        "\n",
        "    self.downsample2 = nn.Conv2d(64, 16, kernel_size=1, stride=2, bias=False)\n",
        "    self.downsample3 = nn.Conv2d(ch, 16, kernel_size=1, stride=4, bias=False)\n",
        "\n",
        "    self.fc1 = nn.Linear(in_features=c*h*w, out_features=128)\n",
        "    self.drop = nn.Dropout(p=0.25)\n",
        "    self.fc2 = nn.Linear(in_features=128, out_features=10)\n",
        "    self.relu = nn.ReLU()\n",
        "    nn.init.normal_(self.conv1.weight)\n",
        "    nn.init.normal_(self.conv2.weight)\n",
        "    nn.init.normal_(self.conv3.weight)\n",
        "    nn.init.normal_(self.conv4.weight)\n",
        "    nn.init.normal_(self.fc1.weight)\n",
        "    nn.init.normal_(self.fc2.weight)\n",
        "    nn.init.normal_(self.downsample1.weight)\n",
        "    nn.init.normal_(self.downsample2.weight)\n",
        "    nn.init.normal_(self.downsample3.weight)\n",
        "    nn.init.zeros_(self.conv1.bias)\n",
        "    nn.init.zeros_(self.conv2.bias)\n",
        "    nn.init.zeros_(self.conv3.bias)\n",
        "    nn.init.zeros_(self.conv4.bias)\n",
        "    nn.init.zeros_(self.fc1.bias)\n",
        "    nn.init.zeros_(self.fc2.bias)\n",
        "\n",
        "    # Random initialization\n",
        "  def initialize_weight(self):\n",
        "    nn.init.normal_(self.conv1.weight)\n",
        "    nn.init.normal_(self.conv2.weight)\n",
        "    nn.init.normal_(self.conv3.weight)\n",
        "    nn.init.normal_(self.fc1.weight)\n",
        "    nn.init.normal_(self.fc2.weight)\n",
        "\n",
        "  def forward(self, x):\n",
        "    residual = x\n",
        "    x = self.conv1(x)\n",
        "    x = self.bn1(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.pool1(x)\n",
        "    x = self.conv2(x)\n",
        "    x = self.bn2(x)\n",
        "    residual1 = self.downsample1(residual)\n",
        "    x += residual1\n",
        "    residual1 = x\n",
        "    x = self.relu(x)\n",
        "    x = self.pool2(x)\n",
        "    x = self.conv3(x)\n",
        "    x = self.bn3(x)\n",
        "    x = self.relu(x)\n",
        "    #x = self.pool3(x)\n",
        "    x = self.conv4(x)\n",
        "    x = self.bn4(x)\n",
        "    residual1 = self.downsample2(residual1)\n",
        "    residual2 = self.downsample3(residual)\n",
        "    x += residual1+residual2\n",
        "    x = self.relu(x)\n",
        "    x = self.pool4(x)\n",
        "    x = x.view(-1, c*h*w)\n",
        "    x = self.fc1(x)\n",
        "    x = self.drop(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.fc2(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "\n",
        "class Net2(torch.nn.Module):\n",
        "  def __init__ (self):\n",
        "    super(Net2,self).__init__()\n",
        "    self.conv1 = nn.Conv2d(in_channels=ch, out_channels=32, kernel_size=3, stride=1, padding='same')\n",
        "    self.bn1 = nn.BatchNorm2d(32)\n",
        "    self.pool1 = nn.MaxPool2d(kernel_size = 2)\n",
        "    self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding='same')\n",
        "    self.bn2 = nn.BatchNorm2d(64)\n",
        "    self.pool2 = nn.MaxPool2d(kernel_size = 2)\n",
        "    self.conv3 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding='same')\n",
        "    self.bn3 = nn.BatchNorm2d(64)\n",
        "    self.pool3 = nn.MaxPool2d(kernel_size = 2)\n",
        "    self.conv4 = nn.Conv2d(in_channels=64, out_channels=16, kernel_size=3, stride=1, padding='same')\n",
        "    self.bn4 = nn.BatchNorm2d(16)\n",
        "    self.pool4 = nn.MaxPool2d(kernel_size = 2)\n",
        "\n",
        "    self.downsample1 = nn.Conv2d(ch, 64, kernel_size=1, stride=2, bias=False)\n",
        "\n",
        "    self.downsample2 = nn.Conv2d(64, 16, kernel_size=1, stride=2, bias=False)\n",
        "    self.downsample3 = nn.Conv2d(ch, 16, kernel_size=1, stride=4, bias=False)\n",
        "\n",
        "    self.fc1 = nn.Linear(in_features=c*h1*w1, out_features=128)\n",
        "    self.drop = nn.Dropout(p=0.25)\n",
        "    self.fc2 = nn.Linear(in_features=128, out_features=10)\n",
        "    self.relu = nn.ReLU()\n",
        "\n",
        "    nn.init.xavier_uniform_(self.conv1.weight)\n",
        "    nn.init.xavier_uniform_(self.conv2.weight)\n",
        "    nn.init.xavier_uniform_(self.conv3.weight)\n",
        "    nn.init.xavier_uniform_(self.conv4.weight)\n",
        "    nn.init.xavier_uniform_(self.fc1.weight)\n",
        "    nn.init.xavier_uniform_(self.fc2.weight)\n",
        "    nn.init.xavier_uniform_(self.downsample1.weight)\n",
        "    nn.init.xavier_uniform_(self.downsample2.weight)\n",
        "    nn.init.xavier_uniform_(self.downsample3.weight)\n",
        "    nn.init.zeros_(self.conv1.bias)\n",
        "    nn.init.zeros_(self.conv2.bias)\n",
        "    nn.init.zeros_(self.conv3.bias)\n",
        "    nn.init.zeros_(self.conv4.bias)\n",
        "    nn.init.zeros_(self.fc1.bias)\n",
        "    nn.init.zeros_(self.fc2.bias)\n",
        "\n",
        "    # Xavier initialization\n",
        "  def initialize_weight(self):\n",
        "    nn.init.xavier_uniform_(self.conv1.weight)\n",
        "    nn.init.xavier_uniform_(self.conv2.weight)\n",
        "    nn.init.xavier_uniform_(self.conv3.weight)\n",
        "    nn.init.xavier_uniform_(self.fc1.weight)\n",
        "    nn.init.xavier_uniform_(self.fc2.weight)\n",
        "\n",
        "  def forward(self, x):\n",
        "    residual = x\n",
        "    x = self.conv1(x)\n",
        "    x = self.bn1(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.pool1(x)\n",
        "    x = self.conv2(x)\n",
        "    x = self.bn2(x)\n",
        "    residual1 = self.downsample1(residual)\n",
        "    x += residual1\n",
        "    residual1 = x\n",
        "    residual1 = x\n",
        "    x = self.relu(x)\n",
        "    x = self.pool2(x)\n",
        "    x = self.conv3(x)\n",
        "    x = self.bn3(x)\n",
        "    x = self.relu(x)\n",
        "    #x = self.pool3(x)\n",
        "    x = self.conv4(x)\n",
        "    x = self.bn4(x)\n",
        "    residual1 = self.downsample2(residual1)\n",
        "    residual2 = self.downsample3(residual)\n",
        "    x += residual1+residual2\n",
        "    x = self.relu(x)\n",
        "    x = self.pool4(x)\n",
        "    x = x.view(-1, c*h1*w1)\n",
        "    x = self.fc1(x)\n",
        "    x = self.drop(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.fc2(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "class Net3(torch.nn.Module):\n",
        "  def __init__ (self):\n",
        "    super(Net3,self).__init__()\n",
        "    self.conv1 = nn.Conv2d(in_channels=ch, out_channels=32, kernel_size=3, stride=1, padding='same')\n",
        "    self.bn1 = nn.BatchNorm2d(32)\n",
        "    self.pool1 = nn.MaxPool2d(kernel_size = 2)\n",
        "    self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding='same')\n",
        "    self.bn2 = nn.BatchNorm2d(64)\n",
        "    self.pool2 = nn.MaxPool2d(kernel_size = 2)\n",
        "    self.conv3 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding='same')\n",
        "    self.bn3 = nn.BatchNorm2d(64)\n",
        "    self.pool3 = nn.MaxPool2d(kernel_size = 2)\n",
        "    self.conv4 = nn.Conv2d(in_channels=64, out_channels=16, kernel_size=3, stride=1, padding='same')\n",
        "    self.bn4 = nn.BatchNorm2d(16)\n",
        "    self.pool4 = nn.MaxPool2d(kernel_size = 2)\n",
        "\n",
        "    self.downsample1 = nn.Conv2d(ch, 64, kernel_size=1, stride=2, bias=False)\n",
        "\n",
        "    self.downsample2 = nn.Conv2d(64, 16, kernel_size=1, stride=2, bias=False)\n",
        "    self.downsample3 = nn.Conv2d(ch, 16, kernel_size=1, stride=4, bias=False)\n",
        "\n",
        "    self.fc1 = nn.Linear(in_features=c*h1*w1, out_features=128)\n",
        "    self.drop = nn.Dropout(p=0.25)\n",
        "    self.fc2 = nn.Linear(in_features=128, out_features=10)\n",
        "    self.relu = nn.ReLU()\n",
        "\n",
        "    nn.init.kaiming_normal_(self.conv1.weight, mode='fan_in', nonlinearity='relu')\n",
        "    nn.init.kaiming_normal_(self.conv2.weight, mode='fan_in', nonlinearity='relu')\n",
        "    nn.init.kaiming_normal_(self.conv3.weight, mode='fan_in', nonlinearity='relu')\n",
        "    nn.init.kaiming_normal_(self.conv4.weight, mode='fan_in', nonlinearity='relu')\n",
        "    nn.init.kaiming_normal_(self.fc1.weight, mode='fan_in', nonlinearity='relu')\n",
        "    nn.init.kaiming_normal_(self.fc2.weight, mode='fan_in', nonlinearity='relu')\n",
        "    nn.init.kaiming_normal_(self.downsample1.weight, mode='fan_in', nonlinearity='relu')\n",
        "    nn.init.kaiming_normal_(self.downsample2.weight, mode='fan_in', nonlinearity='relu')\n",
        "    nn.init.kaiming_normal_(self.downsample3.weight, mode='fan_in', nonlinearity='relu')\n",
        "    nn.init.zeros_(self.conv1.bias)\n",
        "    nn.init.zeros_(self.conv2.bias)\n",
        "    nn.init.zeros_(self.conv3.bias)\n",
        "    nn.init.zeros_(self.conv4.bias)\n",
        "    nn.init.zeros_(self.fc1.bias)\n",
        "    nn.init.zeros_(self.fc2.bias)\n",
        "\n",
        "    # He initialization\n",
        "  def initialize_weight(self):\n",
        "    nn.init.kaiming_normal_(self.conv1.weight, mode='fan_in', nonlinearity='relu')\n",
        "    nn.init.kaiming_normal_(self.conv2.weight, mode='fan_in', nonlinearity='relu')\n",
        "    nn.init.kaiming_normal_(self.conv3.weight, mode='fan_in', nonlinearity='relu')\n",
        "    nn.init.kaiming_normal_(self.fc1.weight, mode='fan_in', nonlinearity='relu')\n",
        "    nn.init.kaiming_normal_(self.fc2.weight, mode='fan_in', nonlinearity='relu')\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    residual = x\n",
        "    x = self.conv1(x)\n",
        "    x = self.bn1(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.pool1(x)\n",
        "    x = self.conv2(x)\n",
        "    x = self.bn2(x)\n",
        "    residual1 = self.downsample1(residual)\n",
        "    x += residual1\n",
        "    residual1 = x\n",
        "    x = self.relu(x)\n",
        "    x = self.pool2(x)\n",
        "    x = self.conv3(x)\n",
        "    x = self.bn3(x)\n",
        "    x = self.relu(x)\n",
        "   # x = self.pool3(x)\n",
        "    x = self.conv4(x)\n",
        "    x = self.bn4(x)\n",
        "    residual1 = self.downsample2(residual1)\n",
        "    residual2 = self.downsample3(residual)\n",
        "    x += residual1+residual2\n",
        "    x = self.relu(x)\n",
        "    x = self.pool4(x)\n",
        "    x = x.view(-1, c*h1*w1)\n",
        "    x = self.fc1(x)\n",
        "    x = self.drop(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.fc2(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "class Net4(torch.nn.Module):\n",
        "  def __init__ (self):\n",
        "    super(Net4,self).__init__()\n",
        "    self.conv1 = nn.Conv2d(in_channels=ch, out_channels=32, kernel_size=3, stride=1, padding='same')\n",
        "    self.bn1 = nn.BatchNorm2d(32)\n",
        "    self.pool1 = nn.MaxPool2d(kernel_size = 2)\n",
        "    self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding='same')\n",
        "    self.bn2 = nn.BatchNorm2d(64)\n",
        "    self.pool2 = nn.MaxPool2d(kernel_size = 2)\n",
        "    self.conv3 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding='same')\n",
        "    self.bn3 = nn.BatchNorm2d(64)\n",
        "    self.pool3 = nn.MaxPool2d(kernel_size = 2)\n",
        "    self.conv4 = nn.Conv2d(in_channels=64, out_channels=16, kernel_size=3, stride=1, padding='same')\n",
        "    self.bn4 = nn.BatchNorm2d(16)\n",
        "    self.pool4 = nn.MaxPool2d(kernel_size = 2)\n",
        "\n",
        "    self.downsample1 = nn.Conv2d(ch, 64, kernel_size=1, stride=2, bias=False)\n",
        "\n",
        "    self.downsample2 = nn.Conv2d(64, 16, kernel_size=1, stride=2, bias=False)\n",
        "    self.downsample3 = nn.Conv2d(ch, 16, kernel_size=1, stride=4, bias=False)\n",
        "\n",
        "    self.fc1 = nn.Linear(in_features=c*h*w, out_features=128)\n",
        "    self.drop = nn.Dropout(p=0.25)\n",
        "    self.fc2 = nn.Linear(in_features=128, out_features=10)\n",
        "    self.relu = nn.ReLU()\n",
        "\n",
        "    nn.init.kaiming_uniform_(self.conv1.weight, mode='fan_in', nonlinearity='relu')\n",
        "    nn.init.kaiming_uniform_(self.conv2.weight, mode='fan_in', nonlinearity='relu')\n",
        "    nn.init.kaiming_uniform_(self.conv3.weight, mode='fan_in', nonlinearity='relu')\n",
        "    nn.init.kaiming_uniform_(self.conv4.weight, mode='fan_in', nonlinearity='relu')\n",
        "    nn.init.kaiming_uniform_(self.fc1.weight, mode='fan_in', nonlinearity='relu')\n",
        "    nn.init.kaiming_uniform_(self.fc2.weight, mode='fan_in', nonlinearity='relu')\n",
        "    nn.init.kaiming_uniform_(self.downsample1.weight, mode='fan_in', nonlinearity='relu')\n",
        "    nn.init.kaiming_uniform_(self.downsample2.weight, mode='fan_in', nonlinearity='relu')\n",
        "    nn.init.kaiming_uniform_(self.downsample3.weight, mode='fan_in', nonlinearity='relu')\n",
        "    nn.init.zeros_(self.conv1.bias)\n",
        "    nn.init.zeros_(self.conv2.bias)\n",
        "    nn.init.zeros_(self.conv3.bias)\n",
        "    nn.init.zeros_(self.conv4.bias)\n",
        "    nn.init.zeros_(self.fc1.bias)\n",
        "    nn.init.zeros_(self.fc2.bias)\n",
        "\n",
        "    # Lecun Initialization\n",
        "  def initialize_weight(self):\n",
        "    nn.init.kaiming_uniform_(self.conv1.weight, mode='fan_in', nonlinearity='relu')\n",
        "    nn.init.kaiming_uniform_(self.conv2.weight, mode='fan_in', nonlinearity='relu')\n",
        "    nn.init.kaiming_uniform_(self.conv3.weight, mode='fan_in', nonlinearity='relu')\n",
        "    nn.init.kaiming_uniform_(self.fc1.weight, mode='fan_in', nonlinearity='relu')\n",
        "    nn.init.kaiming_uniform_(self.fc2.weight, mode='fan_in', nonlinearity='relu')\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    residual = x\n",
        "    x = self.conv1(x)\n",
        "    x = self.bn1(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.pool1(x)\n",
        "    x = self.conv2(x)\n",
        "    x = self.bn2(x)\n",
        "    residual1 = self.downsample1(residual)\n",
        "    x += residual1\n",
        "    residual1 = x\n",
        "    x = self.relu(x)\n",
        "    x = self.pool2(x)\n",
        "    x = self.conv3(x)\n",
        "    x = self.bn3(x)\n",
        "    x = self.relu(x)\n",
        "    #x = self.pool3(x)\n",
        "    x = self.conv4(x)\n",
        "    x = self.bn4(x)\n",
        "    residual1 = self.downsample2(residual1)\n",
        "    residual2 = self.downsample3(residual)\n",
        "    x += residual1+residual2\n",
        "    x = self.relu(x)\n",
        "    x = self.pool4(x)\n",
        "    x = x.view(-1, c*h*w)\n",
        "    x = self.fc1(x)\n",
        "    x = self.drop(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.fc2(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "class Net5(torch.nn.Module):\n",
        "  def __init__ (self):\n",
        "    super(Net5,self).__init__()\n",
        "    self.conv1 = nn.Conv2d(in_channels=ch, out_channels=32, kernel_size=3, stride=1, padding='same')\n",
        "    self.bn1 = nn.BatchNorm2d(32)\n",
        "    self.pool1 = nn.MaxPool2d(kernel_size = 2)\n",
        "    self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding='same')\n",
        "    self.bn2 = nn.BatchNorm2d(64)\n",
        "    self.pool2 = nn.MaxPool2d(kernel_size = 2)\n",
        "    self.conv3 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding='same')\n",
        "    self.bn3 = nn.BatchNorm2d(64)\n",
        "    self.pool3 = nn.MaxPool2d(kernel_size = 2)\n",
        "    self.conv4 = nn.Conv2d(in_channels=64, out_channels=16, kernel_size=3, stride=1, padding='same')\n",
        "    self.bn4 = nn.BatchNorm2d(16)\n",
        "    self.pool4 = nn.MaxPool2d(kernel_size = 2)\n",
        "\n",
        "    self.downsample1 = nn.Conv2d(ch, 64, kernel_size=1, stride=2, bias=False)\n",
        "\n",
        "    self.downsample2 = nn.Conv2d(64, 16, kernel_size=1, stride=2, bias=False)\n",
        "    self.downsample3 = nn.Conv2d(ch, 16, kernel_size=1, stride=4, bias=False)\n",
        "\n",
        "    self.fc1 = nn.Linear(in_features=c*h*w, out_features=128)\n",
        "    self.drop = nn.Dropout(p=0.25)\n",
        "    self.fc2 = nn.Linear(in_features=128, out_features=10)\n",
        "    self.relu = nn.ReLU()\n",
        "\n",
        "    nn.init.normal_(self.conv1.weight)\n",
        "    nn.init.normal_(self.conv2.weight)\n",
        "    nn.init.normal_(self.conv3.weight)\n",
        "    nn.init.normal_(self.conv4.weight)\n",
        "    nn.init.normal_(self.fc1.weight)\n",
        "    nn.init.normal_(self.fc2.weight)\n",
        "    nn.init.normal_(self.downsample1.weight)\n",
        "    nn.init.normal_(self.downsample2.weight)\n",
        "    nn.init.normal_(self.downsample3.weight)\n",
        "    nn.init.zeros_(self.conv1.bias)\n",
        "    nn.init.zeros_(self.conv2.bias)\n",
        "    nn.init.zeros_(self.conv3.bias)\n",
        "    nn.init.zeros_(self.conv4.bias)\n",
        "    nn.init.zeros_(self.fc1.bias)\n",
        "    nn.init.zeros_(self.fc2.bias)\n",
        "     # Random initialization\n",
        "  def initialize_weight(self):\n",
        "    nn.init.normal_(self.conv1.weight)\n",
        "    nn.init.normal_(self.conv2.weight)\n",
        "    nn.init.normal_(self.conv3.weight)\n",
        "    nn.init.normal_(self.fc1.weight)\n",
        "    nn.init.normal_(self.fc2.weight)\n",
        "\n",
        "  def forward(self, x):\n",
        "    residual = x\n",
        "    x = self.conv1(x)\n",
        "    x = self.bn1(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.pool1(x)\n",
        "    x = self.conv2(x)\n",
        "    x = self.bn2(x)\n",
        "    residual1 = self.downsample1(residual)\n",
        "    x += residual1\n",
        "    residual1 = x\n",
        "    x = self.relu(x)\n",
        "    x = self.pool2(x)\n",
        "    x = self.conv3(x)\n",
        "    x = self.bn3(x)\n",
        "    x = self.relu(x)\n",
        "    #x = self.pool3(x)\n",
        "    x = self.conv4(x)\n",
        "    x = self.bn4(x)\n",
        "    residual1 = self.downsample2(residual1)\n",
        "    residual2 = self.downsample3(residual)\n",
        "    x += residual1+residual2\n",
        "    x = self.relu(x)\n",
        "    x = self.pool4(x)\n",
        "    x = x.view(-1, c*h*w)\n",
        "    x = self.fc1(x)\n",
        "    x = self.drop(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.fc2(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "class Net6(torch.nn.Module):\n",
        "  def __init__ (self):\n",
        "    super(Net6,self).__init__()\n",
        "    self.conv1 = nn.Conv2d(in_channels=ch, out_channels=32, kernel_size=3, stride=1, padding='same')\n",
        "    self.bn1 = nn.BatchNorm2d(32)\n",
        "    self.pool1 = nn.MaxPool2d(kernel_size = 2)\n",
        "    self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding='same')\n",
        "    self.bn2 = nn.BatchNorm2d(64)\n",
        "    self.pool2 = nn.MaxPool2d(kernel_size = 2)\n",
        "    self.conv3 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding='same')\n",
        "    self.bn3 = nn.BatchNorm2d(64)\n",
        "    self.pool3 = nn.MaxPool2d(kernel_size = 2)\n",
        "    self.conv4 = nn.Conv2d(in_channels=64, out_channels=16, kernel_size=3, stride=1, padding='same')\n",
        "    self.bn4 = nn.BatchNorm2d(16)\n",
        "    self.pool4 = nn.MaxPool2d(kernel_size = 2)\n",
        "\n",
        "    self.downsample1 = nn.Conv2d(ch, 64, kernel_size=1, stride=2, bias=False)\n",
        "\n",
        "    self.downsample2 = nn.Conv2d(64, 16, kernel_size=1, stride=2, bias=False)\n",
        "    self.downsample3 = nn.Conv2d(ch, 16, kernel_size=1, stride=4, bias=False)\n",
        "\n",
        "    self.fc1 = nn.Linear(in_features=c*h*w, out_features=128)\n",
        "    self.drop = nn.Dropout(p=0.25)\n",
        "    self.fc2 = nn.Linear(in_features=128, out_features=10)\n",
        "    self.relu = nn.ReLU()\n",
        "\n",
        "    nn.init.orthogonal_(self.conv1.weight)\n",
        "    nn.init.orthogonal_(self.conv2.weight)\n",
        "    nn.init.orthogonal_(self.conv3.weight)\n",
        "    nn.init.orthogonal_(self.conv4.weight)\n",
        "    nn.init.orthogonal_(self.fc1.weight)\n",
        "    nn.init.orthogonal_(self.fc2.weight)\n",
        "    nn.init.orthogonal_(self.downsample1.weight)\n",
        "    nn.init.orthogonal_(self.downsample2.weight)\n",
        "    nn.init.orthogonal_(self.downsample3.weight)\n",
        "    nn.init.zeros_(self.conv1.bias)\n",
        "    nn.init.zeros_(self.conv2.bias)\n",
        "    nn.init.zeros_(self.conv3.bias)\n",
        "    nn.init.zeros_(self.conv4.bias)\n",
        "    nn.init.zeros_(self.fc1.bias)\n",
        "    nn.init.zeros_(self.fc2.bias)\n",
        "   # Orthogonal initialization\n",
        "  def initialize_weight(self):\n",
        "    nn.init.orthogonal_(self.conv1.weight)\n",
        "    nn.init.orthogonal_(self.conv2.weight)\n",
        "    nn.init.orthogonal_(self.conv3.weight)\n",
        "    nn.init.orthogonal_(self.fc1.weight)\n",
        "    nn.init.orthogonal_(self.fc2.weight)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    residual = x\n",
        "    x = self.conv1(x)\n",
        "    x = self.bn1(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.pool1(x)\n",
        "    x = self.conv2(x)\n",
        "    x = self.bn2(x)\n",
        "    residual1 = self.downsample1(residual)\n",
        "    x += residual1\n",
        "    residual1 = x\n",
        "    x = self.relu(x)\n",
        "    x = self.pool2(x)\n",
        "    x = self.conv3(x)\n",
        "    x = self.bn3(x)\n",
        "    x = self.relu(x)\n",
        "    #x = self.pool3(x)\n",
        "    x = self.conv4(x)\n",
        "    x = self.bn4(x)\n",
        "    residual1 = self.downsample2(residual1)\n",
        "    residual2 = self.downsample3(residual)\n",
        "    x += residual1+residual2\n",
        "    x = self.relu(x)\n",
        "    x = self.pool4(x)\n",
        "    x = x.view(-1, c*h*w)\n",
        "    x = self.fc1(x)\n",
        "    x = self.drop(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.fc2(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "class Net7(torch.nn.Module):\n",
        "  def __init__ (self):\n",
        "    super(Net7,self).__init__()\n",
        "    self.conv1 = nn.Conv2d(in_channels=ch, out_channels=32, kernel_size=3, stride=1, padding='same')\n",
        "    self.bn1 = nn.BatchNorm2d(32)\n",
        "    self.pool1 = nn.MaxPool2d(kernel_size = 2)\n",
        "    self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding='same')\n",
        "    self.bn2 = nn.BatchNorm2d(64)\n",
        "    self.pool2 = nn.MaxPool2d(kernel_size = 2)\n",
        "    self.conv3 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding='same')\n",
        "    self.bn3 = nn.BatchNorm2d(64)\n",
        "    self.pool3 = nn.MaxPool2d(kernel_size = 2)\n",
        "    self.conv4 = nn.Conv2d(in_channels=64, out_channels=16, kernel_size=3, stride=1, padding='same')\n",
        "    self.bn4 = nn.BatchNorm2d(16)\n",
        "    self.pool4 = nn.MaxPool2d(kernel_size = 2)\n",
        "\n",
        "    self.downsample1 = nn.Conv2d(ch, 64, kernel_size=1, stride=2, bias=False)\n",
        "\n",
        "    self.downsample2 = nn.Conv2d(64, 16, kernel_size=1, stride=2, bias=False)\n",
        "    self.downsample3 = nn.Conv2d(ch, 16, kernel_size=1, stride=4, bias=False)\n",
        "\n",
        "    self.fc1 = nn.Linear(in_features=c*h1*w1, out_features=128)\n",
        "    self.drop = nn.Dropout(p=0.25)\n",
        "    self.fc2 = nn.Linear(in_features=128, out_features=10)\n",
        "    self.relu = nn.ReLU()\n",
        "\n",
        "    nn.init.kaiming_normal_(self.conv1.weight, mode='fan_in', nonlinearity='relu')\n",
        "    nn.init.kaiming_normal_(self.conv2.weight, mode='fan_in', nonlinearity='relu')\n",
        "    nn.init.kaiming_normal_(self.conv3.weight, mode='fan_in', nonlinearity='relu')\n",
        "    nn.init.kaiming_normal_(self.conv4.weight, mode='fan_in', nonlinearity='relu')\n",
        "    nn.init.kaiming_normal_(self.fc1.weight, mode='fan_in', nonlinearity='relu')\n",
        "    nn.init.kaiming_normal_(self.fc2.weight, mode='fan_in', nonlinearity='relu')\n",
        "    nn.init.kaiming_normal_(self.downsample1.weight, mode='fan_in', nonlinearity='relu')\n",
        "    nn.init.kaiming_normal_(self.downsample2.weight, mode='fan_in', nonlinearity='relu')\n",
        "    nn.init.kaiming_normal_(self.downsample3.weight, mode='fan_in', nonlinearity='relu')\n",
        "    nn.init.zeros_(self.conv1.bias)\n",
        "    nn.init.zeros_(self.conv2.bias)\n",
        "    nn.init.zeros_(self.conv3.bias)\n",
        "    nn.init.zeros_(self.conv4.bias)\n",
        "    nn.init.zeros_(self.fc1.bias)\n",
        "    nn.init.zeros_(self.fc2.bias)\n",
        "      # He initialization\n",
        "  def initialize_weight(self):\n",
        "    nn.init.kaiming_normal_(self.conv1.weight, mode='fan_in', nonlinearity='relu')\n",
        "    nn.init.kaiming_normal_(self.conv2.weight, mode='fan_in', nonlinearity='relu')\n",
        "    nn.init.kaiming_normal_(self.conv3.weight, mode='fan_in', nonlinearity='relu')\n",
        "    nn.init.kaiming_normal_(self.fc1.weight, mode='fan_in', nonlinearity='relu')\n",
        "    nn.init.kaiming_normal_(self.fc2.weight, mode='fan_in', nonlinearity='relu')\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    residual = x\n",
        "    x = self.conv1(x)\n",
        "    x = self.bn1(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.pool1(x)\n",
        "    x = self.conv2(x)\n",
        "    x = self.bn2(x)\n",
        "    residual1 = self.downsample1(residual)\n",
        "    x += residual1\n",
        "    residual1 = x\n",
        "    x = self.relu(x)\n",
        "    x = self.pool2(x)\n",
        "    x = self.conv3(x)\n",
        "    x = self.bn3(x)\n",
        "    x = self.relu(x)\n",
        "    #x = self.pool3(x)\n",
        "    x = self.conv4(x)\n",
        "    x = self.bn4(x)\n",
        "    residual1 = self.downsample2(residual1)\n",
        "    residual2 = self.downsample3(residual)\n",
        "    x += residual1+residual2\n",
        "    x = self.relu(x)\n",
        "    x = self.pool4(x)\n",
        "    x = x.view(-1, c*h1*w1)\n",
        "    x = self.fc1(x)\n",
        "    x = self.drop(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.fc2(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "class Net8(torch.nn.Module):\n",
        "  def __init__ (self):\n",
        "    super(Net8,self).__init__()\n",
        "    self.conv1 = nn.Conv2d(in_channels=ch, out_channels=32, kernel_size=3, stride=1, padding='same')\n",
        "    self.bn1 = nn.BatchNorm2d(32)\n",
        "    self.pool1 = nn.MaxPool2d(kernel_size = 2)\n",
        "    self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding='same')\n",
        "    self.bn2 = nn.BatchNorm2d(64)\n",
        "    self.pool2 = nn.MaxPool2d(kernel_size = 2)\n",
        "    self.conv3 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding='same')\n",
        "    self.bn3 = nn.BatchNorm2d(64)\n",
        "    self.pool3 = nn.MaxPool2d(kernel_size = 2)\n",
        "    self.conv4 = nn.Conv2d(in_channels=64, out_channels=16, kernel_size=3, stride=1, padding='same')\n",
        "    self.bn4 = nn.BatchNorm2d(16)\n",
        "    self.pool4 = nn.MaxPool2d(kernel_size = 2)\n",
        "\n",
        "    self.downsample1 = nn.Conv2d(ch, 64, kernel_size=1, stride=2, bias=False)\n",
        "\n",
        "    self.downsample2 = nn.Conv2d(64, 16, kernel_size=1, stride=2, bias=False)\n",
        "    self.downsample3 = nn.Conv2d(ch, 16, kernel_size=1, stride=4, bias=False)\n",
        "\n",
        "    self.fc1 = nn.Linear(in_features=c*h*w, out_features=128)\n",
        "    self.drop = nn.Dropout(p=0.25)\n",
        "    self.fc2 = nn.Linear(in_features=128, out_features=10)\n",
        "    self.relu = nn.ReLU()\n",
        "\n",
        "    nn.init.kaiming_normal_(self.conv1.weight, mode='fan_in', nonlinearity='relu')\n",
        "    nn.init.kaiming_normal_(self.conv2.weight, mode='fan_in', nonlinearity='relu')\n",
        "    nn.init.kaiming_normal_(self.conv3.weight, mode='fan_in', nonlinearity='relu')\n",
        "    nn.init.kaiming_normal_(self.conv4.weight, mode='fan_in', nonlinearity='relu')\n",
        "    nn.init.kaiming_normal_(self.fc1.weight, mode='fan_in', nonlinearity='relu')\n",
        "    nn.init.kaiming_normal_(self.fc2.weight, mode='fan_in', nonlinearity='relu')\n",
        "    nn.init.kaiming_normal_(self.downsample1.weight, mode='fan_in', nonlinearity='relu')\n",
        "    nn.init.kaiming_normal_(self.downsample2.weight, mode='fan_in', nonlinearity='relu')\n",
        "    nn.init.kaiming_normal_(self.downsample3.weight, mode='fan_in', nonlinearity='relu')\n",
        "    nn.init.zeros_(self.conv1.bias)\n",
        "    nn.init.zeros_(self.conv2.bias)\n",
        "    nn.init.zeros_(self.conv3.bias)\n",
        "    nn.init.zeros_(self.conv4.bias)\n",
        "    nn.init.zeros_(self.fc1.bias)\n",
        "    nn.init.zeros_(self.fc2.bias)\n",
        "\n",
        "      # He initialization\n",
        "  def initialize_weight(self):\n",
        "    nn.init.kaiming_normal_(self.conv1.weight, mode='fan_in', nonlinearity='relu')\n",
        "    nn.init.kaiming_normal_(self.conv2.weight, mode='fan_in', nonlinearity='relu')\n",
        "    nn.init.kaiming_normal_(self.conv3.weight, mode='fan_in', nonlinearity='relu')\n",
        "    nn.init.kaiming_normal_(self.fc1.weight, mode='fan_in', nonlinearity='relu')\n",
        "    nn.init.kaiming_normal_(self.fc2.weight, mode='fan_in', nonlinearity='relu')\n",
        "\n",
        "  def forward(self, x):\n",
        "    residual = x\n",
        "    x = self.conv1(x)\n",
        "    x = self.bn1(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.pool1(x)\n",
        "    x = self.conv2(x)\n",
        "    x = self.bn2(x)\n",
        "    residual1 = self.downsample1(residual)\n",
        "    x += residual1\n",
        "    residual1 = x\n",
        "    x = self.relu(x)\n",
        "    x = self.pool2(x)\n",
        "    x = self.conv3(x)\n",
        "    x = self.bn3(x)\n",
        "    x = self.relu(x)\n",
        "    #x = self.pool3(x)\n",
        "    x = self.conv4(x)\n",
        "    x = self.bn4(x)\n",
        "    residual1 = self.downsample2(residual1)\n",
        "    residual2 = self.downsample3(residual)\n",
        "    x += residual1+residual2\n",
        "    x = self.relu(x)\n",
        "    x = self.pool4(x)\n",
        "    x = x.view(-1, c*h*w)\n",
        "    x = self.fc1(x)\n",
        "    x = self.drop(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.fc2(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "class Net9(torch.nn.Module):\n",
        "  def __init__ (self):\n",
        "    super(Net9,self).__init__()\n",
        "    self.conv1 = nn.Conv2d(in_channels=ch, out_channels=32, kernel_size=3, stride=1, padding='same')\n",
        "    self.bn1 = nn.BatchNorm2d(32)\n",
        "    self.pool1 = nn.MaxPool2d(kernel_size = 2)\n",
        "    self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding='same')\n",
        "    self.bn2 = nn.BatchNorm2d(64)\n",
        "    self.pool2 = nn.MaxPool2d(kernel_size = 2)\n",
        "    self.conv3 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding='same')\n",
        "    self.bn3 = nn.BatchNorm2d(64)\n",
        "    self.pool3 = nn.MaxPool2d(kernel_size = 2)\n",
        "    self.conv4 = nn.Conv2d(in_channels=64, out_channels=16, kernel_size=3, stride=1, padding='same')\n",
        "    self.bn4 = nn.BatchNorm2d(16)\n",
        "    self.pool4 = nn.MaxPool2d(kernel_size = 2)\n",
        "\n",
        "    self.downsample1 = nn.Conv2d(ch, 64, kernel_size=1, stride=2, bias=False)\n",
        "\n",
        "    self.downsample2 = nn.Conv2d(64, 16, kernel_size=1, stride=2, bias=False)\n",
        "    self.downsample3 = nn.Conv2d(ch, 16, kernel_size=1, stride=4, bias=False)\n",
        "\n",
        "    self.fc1 = nn.Linear(in_features=c*h*w, out_features=128)\n",
        "    self.drop = nn.Dropout(p=0.25)\n",
        "    self.fc2 = nn.Linear(in_features=128, out_features=10)\n",
        "    self.relu = nn.ReLU()\n",
        "\n",
        "    nn.init.constant_(self.conv1.weight, 0.01)\n",
        "    nn.init.constant_(self.conv2.weight, 0.01)\n",
        "    nn.init.constant_(self.conv3.weight, 0.01)\n",
        "    nn.init.constant_(self.conv4.weight, 0.01)\n",
        "    nn.init.constant_(self.fc1.weight, 0.01)\n",
        "    nn.init.constant_(self.fc2.weight, 0.01)\n",
        "    nn.init.constant_(self.downsample1.weight, 0.01)\n",
        "    nn.init.constant_(self.downsample2.weight, 0.01)\n",
        "    nn.init.constant_(self.downsample3.weight, 0.01)\n",
        "    nn.init.zeros_(self.conv1.bias)\n",
        "    nn.init.zeros_(self.conv2.bias)\n",
        "    nn.init.zeros_(self.conv3.bias)\n",
        "    nn.init.zeros_(self.conv4.bias)\n",
        "    nn.init.zeros_(self.fc1.bias)\n",
        "    nn.init.zeros_(self.fc2.bias)\n",
        "\n",
        "     # Constant Initialization\n",
        "  def initialize_weight(self):\n",
        "    nn.init.constant_(self.conv1.weight, 0.01)\n",
        "    nn.init.constant_(self.conv2.weight, 0.01)\n",
        "    nn.init.constant_(self.conv3.weight, 0.01)\n",
        "    nn.init.constant_(self.fc1.weight, 0.01)\n",
        "    nn.init.constant_(self.fc2.weight, 0.01)\n",
        "\n",
        "  def forward(self, x):\n",
        "    residual = x\n",
        "    x = self.conv1(x)\n",
        "    x = self.bn1(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.pool1(x)\n",
        "    x = self.conv2(x)\n",
        "    x = self.bn2(x)\n",
        "    residual1 = self.downsample1(residual)\n",
        "    x += residual1\n",
        "    residual1 = x\n",
        "    x = self.relu(x)\n",
        "    x = self.pool2(x)\n",
        "    x = self.conv3(x)\n",
        "    x = self.bn3(x)\n",
        "    x = self.relu(x)\n",
        "    #x = self.pool3(x)\n",
        "    x = self.conv4(x)\n",
        "    x = self.bn4(x)\n",
        "    residual1 = self.downsample2(residual1)\n",
        "    residual2 = self.downsample3(residual)\n",
        "    x += residual1+residual2\n",
        "    x = self.relu(x)\n",
        "    x = self.pool4(x)\n",
        "    x = x.view(-1, c*h*w)\n",
        "    x = self.fc1(x)\n",
        "    x = self.drop(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.fc2(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "class Net10(torch.nn.Module):\n",
        "  def __init__ (self):\n",
        "    super(Net10,self).__init__()\n",
        "    self.conv1 = nn.Conv2d(in_channels=ch, out_channels=32, kernel_size=3, stride=1, padding='same')\n",
        "    self.bn1 = nn.BatchNorm2d(32)\n",
        "    self.pool1 = nn.MaxPool2d(kernel_size = 2)\n",
        "    self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding='same')\n",
        "    self.bn2 = nn.BatchNorm2d(64)\n",
        "    self.pool2 = nn.MaxPool2d(kernel_size = 2)\n",
        "    self.conv3 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding='same')\n",
        "    self.bn3 = nn.BatchNorm2d(64)\n",
        "    self.pool3 = nn.MaxPool2d(kernel_size = 2)\n",
        "    self.conv4 = nn.Conv2d(in_channels=64, out_channels=16, kernel_size=3, stride=1, padding='same')\n",
        "    self.bn4 = nn.BatchNorm2d(16)\n",
        "    self.pool4 = nn.MaxPool2d(kernel_size = 2)\n",
        "\n",
        "    self.downsample1 = nn.Conv2d(ch, 64, kernel_size=1, stride=2, bias=False)\n",
        "\n",
        "    self.downsample2 = nn.Conv2d(64, 16, kernel_size=1, stride=2, bias=False)\n",
        "    self.downsample3 = nn.Conv2d(ch, 16, kernel_size=1, stride=4, bias=False)\n",
        "\n",
        "    self.fc1 = nn.Linear(in_features=c*h*w, out_features=128)\n",
        "    self.drop = nn.Dropout(p=0.25)\n",
        "    self.fc2 = nn.Linear(in_features=128, out_features=10)\n",
        "    self.relu = nn.ReLU()\n",
        "\n",
        "    nn.init.xavier_uniform_(self.conv1.weight, gain=nn.init.calculate_gain('relu'))\n",
        "    nn.init.xavier_uniform_(self.conv2.weight, gain=nn.init.calculate_gain('relu'))\n",
        "    nn.init.xavier_uniform_(self.conv3.weight, gain=nn.init.calculate_gain('relu'))\n",
        "    nn.init.xavier_uniform_(self.conv4.weight, gain=nn.init.calculate_gain('relu'))\n",
        "    nn.init.xavier_uniform_(self.fc1.weight, gain=nn.init.calculate_gain('relu'))\n",
        "    nn.init.xavier_uniform_(self.fc2.weight, gain=nn.init.calculate_gain('relu'))\n",
        "    nn.init.xavier_uniform_(self.downsample1.weight, gain=nn.init.calculate_gain('relu'))\n",
        "    nn.init.xavier_uniform_(self.downsample2.weight, gain=nn.init.calculate_gain('relu'))\n",
        "    nn.init.xavier_uniform_(self.downsample3.weight, gain=nn.init.calculate_gain('relu'))\n",
        "    nn.init.zeros_(self.conv1.bias)\n",
        "    nn.init.zeros_(self.conv2.bias)\n",
        "    nn.init.zeros_(self.conv3.bias)\n",
        "    nn.init.zeros_(self.conv4.bias)\n",
        "    nn.init.zeros_(self.fc1.bias)\n",
        "    nn.init.zeros_(self.fc2.bias)\n",
        "\n",
        "      # Glorot initialization\n",
        "  def initialize_weight(self):\n",
        "    nn.init.xavier_uniform_(self.conv1.weight, gain=nn.init.calculate_gain('relu'))\n",
        "    nn.init.xavier_uniform_(self.conv2.weight, gain=nn.init.calculate_gain('relu'))\n",
        "    nn.init.xavier_uniform_(self.conv3.weight, gain=nn.init.calculate_gain('relu'))\n",
        "    nn.init.xavier_uniform_(self.fc1.weight, gain=nn.init.calculate_gain('relu'))\n",
        "    nn.init.xavier_uniform_(self.fc2.weight, gain=nn.init.calculate_gain('relu'))\n",
        "\n",
        "  def forward(self, x):\n",
        "    residual = x\n",
        "    x = self.conv1(x)\n",
        "    x = self.bn1(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.pool1(x)\n",
        "    x = self.conv2(x)\n",
        "    x = self.bn2(x)\n",
        "    residual1 = self.downsample1(residual)\n",
        "    x += residual1\n",
        "    residual1 = x\n",
        "    x = self.relu(x)\n",
        "    x = self.pool2(x)\n",
        "    x = self.conv3(x)\n",
        "    x = self.bn3(x)\n",
        "    x = self.relu(x)\n",
        "\n",
        "    #x = self.pool3(x)\n",
        "    x = self.conv4(x)\n",
        "    x = self.bn4(x)\n",
        "\n",
        "    residual1 = self.downsample2(residual1)\n",
        "    residual2 = self.downsample3(residual)\n",
        "    x += residual1+residual2\n",
        "    x = self.relu(x)\n",
        "    x = self.pool4(x)\n",
        "    x = x.view(-1, c*h*w)\n",
        "    x = self.fc1(x)\n",
        "    x = self.drop(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.fc2(x)\n",
        "    return x\n",
        "\n",
        "class Classifier(torch.nn.Module):\n",
        "  def __init__ (self):\n",
        "    super(Classifier,self).__init__()\n",
        "    self.fc1 = nn.Linear(in_features=40, out_features=256)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.fc2 = nn.Linear(in_features=256, out_features=10)\n",
        "    self.relu = nn.ReLU()\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.fc1(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.fc2(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "\n",
        "class MyEnsemble(nn.Module):\n",
        "\n",
        "    def __init__(self, model1, model2, model3, model4,model5, model6, model7, model8, model9, model10):\n",
        "        super(MyEnsemble, self).__init__()\n",
        "        self.model1 = model1\n",
        "        self.model2 = model2\n",
        "        self.model3 = model3\n",
        "        self.model4 = model4\n",
        "        self.model5 = model5\n",
        "        self.model6 = model6\n",
        "        self.model7 = model7\n",
        "        self.model8 = model8\n",
        "        self.model9 = model9\n",
        "        self.model10 = model10\n",
        "        self.relu = nn.ReLU()\n",
        "        self.classifier = nn.Linear(100, 10)\n",
        "        self.classifier1 = nn.Linear(100, 10)\n",
        "\n",
        "        self.a = nn.Softmax(dim=1)\n",
        "    def init_weight(self):\n",
        "      self.model1 = self.model1.initialize_weight()\n",
        "      self.model2 = self.model2.initialize_weight()\n",
        "      self.model3 = self.model3.initialize_weight()\n",
        "      self.model4 = self.model4.initialize_weight()\n",
        "      self.model5 = self.model5.initialize_weight()\n",
        "      self.model6 = self.model6.initialize_weight()\n",
        "      self.model7 = self.model7.initialize_weight()\n",
        "      self.model8 = self.model8.initialize_weight()\n",
        "      self.model9 = self.model9.initialize_weight()\n",
        "      self.model10 = self.model10.initialize_weight()\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size,_,_,_ = x.shape\n",
        "        out1 = (self.model1(x.clone()))#.view(-1)\n",
        "        out2 = (self.model2(x.clone()))#.view(-1)\n",
        "        out3 = (self.model3(x.clone()))#.view(-1)\n",
        "        out4 = (self.model4(x.clone()))#.view(-1)\n",
        "        out5 = (self.model5(x.clone()))#.view(-1)\n",
        "        out6 = (self.model6(x.clone()))#.view(-1)\n",
        "        out7 = (self.model7(x.clone()))#.view(-1)\n",
        "        out8 = (self.model8(x.clone()))#.view(-1)\n",
        "        out9 = (self.model9(x.clone()))#.view(-1)\n",
        "        out10 = (self.model10(x.clone()))#.view(-1)\n",
        "\n",
        "        out = (0.1*out1+out2+0.8*out3+0.7*out4+0.1*out5+out6+0.8*out7+0.8*out8+0.1*out9+0.7*out10)\n",
        "\n",
        "        t = torch.cat((0.1*out1,out2,0.8*out3,0.7*out4,0.1*out5,out6,0.8*out7,0.8*out8,0.1*out9,0.7*out10),dim=1) #.view(batch_size,-1) #out6,out7,out8,out9,out10,,out4\n",
        "        #t = torch.tensor([out1,out2,out3,out4,out5,out6,out7,out8,out9,out10])\n",
        "        #out = t.mode(0).values.view(batch_size,-1)\n",
        "        out_f = self.classifier(self.relu(t)) #torch.mode(predictions, dim=0).values\n",
        "\n",
        "        return (out_f.squeeze())\n",
        "\n",
        "\n",
        "\n",
        "def count_parameters(model):\n",
        "  return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "\n",
        "\n",
        "def Offline_aug(x,y):\n",
        "  y1 = y.squeeze()\n",
        "  assert x.shape[0]==y1.shape[0]\n",
        "  tr_x = []\n",
        "  tr_y = []\n",
        "  for i in range(x.shape[0]):\n",
        "    x_ = transforms.ToPILImage()(x[i])\n",
        "    x1 = transforms.RandomRotation(40)(x_)\n",
        "    x2 = transforms.RandomHorizontalFlip()(x_)\n",
        "    x3 = transforms.RandomVerticalFlip()(x_)\n",
        "    x4 = transforms.ColorJitter()(x_)\n",
        "    x5 = transforms.AutoAugment()(x_)\n",
        "    tr_x.append(transforms.ToTensor()(x_))\n",
        "    tr_x.append(transforms.ToTensor()(x1))\n",
        "    tr_x.append(transforms.ToTensor()(x2))\n",
        "    tr_x.append(transforms.ToTensor()(x3))\n",
        "    tr_x.append(transforms.ToTensor()(x4))\n",
        "    tr_x.append(transforms.ToTensor()(x5))\n",
        "    tr_y.append(y1[i])\n",
        "    tr_y.append(y1[i])\n",
        "    tr_y.append(y1[i])\n",
        "    tr_y.append(y1[i])\n",
        "    tr_y.append(y1[i])\n",
        "    tr_y.append(y1[i])\n",
        "  tr_x = torch.stack(tr_x)\n",
        "  tr_y = torch.stack(tr_y)\n",
        "  return tr_x,tr_y\n",
        "\n",
        "class CustomTensorDataset(Dataset):\n",
        "    \"\"\"TensorDataset with support of transforms.\n",
        "    \"\"\"\n",
        "    def __init__(self, tensors, transform=None):\n",
        "        assert all(tensors[0].size(0) == tensor.size(0) for tensor in tensors)\n",
        "        self.tensors = tensors\n",
        "        self.transform = transform\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        x = self.tensors[0][index]\n",
        "\n",
        "        if self.transform:\n",
        "            x = self.transform(x)\n",
        "\n",
        "        y = self.tensors[1][index]\n",
        "\n",
        "        return x, y\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.tensors[0].size(0)"
      ],
      "metadata": {
        "id": "GHd9dKmn1bjv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_sdBhkUG8iqE",
        "outputId": "a12c2488-b0ef-4a56-8f16-9f4dbed625ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OMkh1LnuuqVv",
        "outputId": "7a977727-4499-4fb9-b808-a0a3b5d8d41a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Train Epoch: 0 [108/300 (90%)]\tLoss: 20.290308\n",
            "\n",
            "Test set: Average loss: 0.0366, Accuracy: 196/400 (49.00%)\n",
            "\n",
            "49.0\n",
            "\n",
            "Test set: Average loss: 0.0213, Accuracy: 259/400 (64.75%)\n",
            "\n",
            "64.75\n",
            "\n",
            "Test set: Average loss: 0.0116, Accuracy: 280/400 (70.00%)\n",
            "\n",
            "70.0\n",
            "\n",
            "Test set: Average loss: 0.0100, Accuracy: 299/400 (74.75%)\n",
            "\n",
            "74.75\n",
            "\n",
            "Test set: Average loss: 0.0082, Accuracy: 327/400 (81.75%)\n",
            "\n",
            "81.75\n",
            "\n",
            "Test set: Average loss: 0.0076, Accuracy: 328/400 (82.00%)\n",
            "\n",
            "82.0\n",
            "\n",
            "Test set: Average loss: 0.0066, Accuracy: 343/400 (85.75%)\n",
            "\n",
            "85.75\n",
            "Train Epoch: 7 [108/300 (90%)]\tLoss: 3.699418\n",
            "\n",
            "Test set: Average loss: 0.0065, Accuracy: 338/400 (84.50%)\n",
            "\n",
            "84.5\n",
            "\n",
            "Test set: Average loss: 0.0067, Accuracy: 340/400 (85.00%)\n",
            "\n",
            "85.0\n",
            "\n",
            "Test set: Average loss: 0.0068, Accuracy: 340/400 (85.00%)\n",
            "\n",
            "85.0\n",
            "\n",
            "Test set: Average loss: 0.0056, Accuracy: 351/400 (87.75%)\n",
            "\n",
            "87.75\n",
            "\n",
            "Test set: Average loss: 0.0078, Accuracy: 341/400 (85.25%)\n",
            "\n",
            "85.25\n",
            "\n",
            "Test set: Average loss: 0.0100, Accuracy: 331/400 (82.75%)\n",
            "\n",
            "82.75\n",
            "\n",
            "Test set: Average loss: 0.0095, Accuracy: 344/400 (86.00%)\n",
            "\n",
            "86.0\n",
            "Train Epoch: 14 [108/300 (90%)]\tLoss: 0.542698\n",
            "\n",
            "Test set: Average loss: 0.0084, Accuracy: 343/400 (85.75%)\n",
            "\n",
            "85.75\n",
            "\n",
            "Test set: Average loss: 0.0088, Accuracy: 343/400 (85.75%)\n",
            "\n",
            "85.75\n",
            "\n",
            "Test set: Average loss: 0.0148, Accuracy: 321/400 (80.25%)\n",
            "\n",
            "80.25\n",
            "\n",
            "Test set: Average loss: 0.0198, Accuracy: 297/400 (74.25%)\n",
            "\n",
            "74.25\n",
            "\n",
            "Test set: Average loss: 0.0073, Accuracy: 352/400 (88.00%)\n",
            "\n",
            "88.0\n",
            "\n",
            "Test set: Average loss: 0.0078, Accuracy: 345/400 (86.25%)\n",
            "\n",
            "86.25\n",
            "\n",
            "Test set: Average loss: 0.0142, Accuracy: 308/400 (77.00%)\n",
            "\n",
            "77.0\n",
            "Train Epoch: 21 [108/300 (90%)]\tLoss: 4.898823\n",
            "\n",
            "Test set: Average loss: 0.0069, Accuracy: 352/400 (88.00%)\n",
            "\n",
            "88.0\n",
            "\n",
            "Test set: Average loss: 0.0082, Accuracy: 348/400 (87.00%)\n",
            "\n",
            "87.0\n",
            "\n",
            "Test set: Average loss: 0.0099, Accuracy: 343/400 (85.75%)\n",
            "\n",
            "85.75\n",
            "\n",
            "Test set: Average loss: 0.0090, Accuracy: 338/400 (84.50%)\n",
            "\n",
            "84.5\n",
            "\n",
            "Test set: Average loss: 0.0083, Accuracy: 353/400 (88.25%)\n",
            "\n",
            "88.25\n",
            "\n",
            "Test set: Average loss: 0.0110, Accuracy: 329/400 (82.25%)\n",
            "\n",
            "82.25\n",
            "\n",
            "Test set: Average loss: 0.0086, Accuracy: 354/400 (88.50%)\n",
            "\n",
            "88.5\n",
            "Train Epoch: 28 [108/300 (90%)]\tLoss: 1.430696\n",
            "\n",
            "Test set: Average loss: 0.0101, Accuracy: 346/400 (86.50%)\n",
            "\n",
            "86.5\n",
            "\n",
            "Test set: Average loss: 0.0086, Accuracy: 351/400 (87.75%)\n",
            "\n",
            "87.75\n",
            "\n",
            "Test set: Average loss: 0.0087, Accuracy: 350/400 (87.50%)\n",
            "\n",
            "87.5\n",
            "\n",
            "Test set: Average loss: 0.0155, Accuracy: 335/400 (83.75%)\n",
            "\n",
            "83.75\n",
            "\n",
            "Test set: Average loss: 0.0102, Accuracy: 347/400 (86.75%)\n",
            "\n",
            "86.75\n",
            "\n",
            "Test set: Average loss: 0.0088, Accuracy: 345/400 (86.25%)\n",
            "\n",
            "86.25\n",
            "\n",
            "Test set: Average loss: 0.0103, Accuracy: 346/400 (86.50%)\n",
            "\n",
            "86.5\n",
            "Train Epoch: 35 [108/300 (90%)]\tLoss: 0.388461\n",
            "\n",
            "Test set: Average loss: 0.0140, Accuracy: 337/400 (84.25%)\n",
            "\n",
            "84.25\n",
            "\n",
            "Test set: Average loss: 0.0145, Accuracy: 337/400 (84.25%)\n",
            "\n",
            "84.25\n",
            "\n",
            "Test set: Average loss: 0.0139, Accuracy: 340/400 (85.00%)\n",
            "\n",
            "85.0\n",
            "\n",
            "Test set: Average loss: 0.0086, Accuracy: 348/400 (87.00%)\n",
            "\n",
            "87.0\n",
            "\n",
            "Test set: Average loss: 0.0089, Accuracy: 353/400 (88.25%)\n",
            "\n",
            "88.25\n",
            "\n",
            "Test set: Average loss: 0.0107, Accuracy: 337/400 (84.25%)\n",
            "\n",
            "84.25\n",
            "\n",
            "Test set: Average loss: 0.0114, Accuracy: 340/400 (85.00%)\n",
            "\n",
            "85.0\n",
            "Train Epoch: 42 [108/300 (90%)]\tLoss: 0.906653\n",
            "\n",
            "Test set: Average loss: 0.0093, Accuracy: 352/400 (88.00%)\n",
            "\n",
            "88.0\n",
            "\n",
            "Test set: Average loss: 0.0139, Accuracy: 337/400 (84.25%)\n",
            "\n",
            "84.25\n",
            "\n",
            "Test set: Average loss: 0.0173, Accuracy: 329/400 (82.25%)\n",
            "\n",
            "82.25\n",
            "\n",
            "Test set: Average loss: 0.0079, Accuracy: 353/400 (88.25%)\n",
            "\n",
            "88.25\n",
            "\n",
            "Test set: Average loss: 0.0090, Accuracy: 333/400 (83.25%)\n",
            "\n",
            "83.25\n",
            "\n",
            "Test set: Average loss: 0.0130, Accuracy: 328/400 (82.00%)\n",
            "\n",
            "82.0\n",
            "\n",
            "Test set: Average loss: 0.0085, Accuracy: 351/400 (87.75%)\n",
            "\n",
            "87.75\n",
            "Train Epoch: 49 [108/300 (90%)]\tLoss: 0.686851\n",
            "\n",
            "Test set: Average loss: 0.0177, Accuracy: 339/400 (84.75%)\n",
            "\n",
            "84.75\n",
            "\n",
            "Test set: Average loss: 0.0118, Accuracy: 345/400 (86.25%)\n",
            "\n",
            "86.25\n",
            "\n",
            "Test set: Average loss: 0.0113, Accuracy: 350/400 (87.50%)\n",
            "\n",
            "87.5\n",
            "\n",
            "Test set: Average loss: 0.0120, Accuracy: 350/400 (87.50%)\n",
            "\n",
            "87.5\n",
            "\n",
            "Test set: Average loss: 0.0104, Accuracy: 344/400 (86.00%)\n",
            "\n",
            "86.0\n",
            "\n",
            "Test set: Average loss: 0.0102, Accuracy: 349/400 (87.25%)\n",
            "\n",
            "87.25\n",
            "\n",
            "Test set: Average loss: 0.0179, Accuracy: 327/400 (81.75%)\n",
            "\n",
            "81.75\n",
            "Train Epoch: 56 [108/300 (90%)]\tLoss: 0.273585\n",
            "\n",
            "Test set: Average loss: 0.0114, Accuracy: 345/400 (86.25%)\n",
            "\n",
            "86.25\n",
            "\n",
            "Test set: Average loss: 0.0111, Accuracy: 352/400 (88.00%)\n",
            "\n",
            "88.0\n",
            "\n",
            "Test set: Average loss: 0.0130, Accuracy: 352/400 (88.00%)\n",
            "\n",
            "88.0\n",
            "\n",
            "Test set: Average loss: 0.0171, Accuracy: 335/400 (83.75%)\n",
            "\n",
            "83.75\n",
            "\n",
            "Test set: Average loss: 0.0114, Accuracy: 352/400 (88.00%)\n",
            "\n",
            "88.0\n",
            "\n",
            "Test set: Average loss: 0.0114, Accuracy: 344/400 (86.00%)\n",
            "\n",
            "86.0\n",
            "\n",
            "Test set: Average loss: 0.0095, Accuracy: 351/400 (87.75%)\n",
            "\n",
            "87.75\n",
            "Train Epoch: 63 [108/300 (90%)]\tLoss: 0.109343\n",
            "\n",
            "Test set: Average loss: 0.0114, Accuracy: 343/400 (85.75%)\n",
            "\n",
            "85.75\n",
            "\n",
            "Test set: Average loss: 0.0091, Accuracy: 357/400 (89.25%)\n",
            "\n",
            "89.25\n",
            "\n",
            "Test set: Average loss: 0.0116, Accuracy: 352/400 (88.00%)\n",
            "\n",
            "88.0\n",
            "\n",
            "Test set: Average loss: 0.0103, Accuracy: 354/400 (88.50%)\n",
            "\n",
            "88.5\n",
            "\n",
            "Test set: Average loss: 0.0094, Accuracy: 342/400 (85.50%)\n",
            "\n",
            "85.5\n",
            "\n",
            "Test set: Average loss: 0.0099, Accuracy: 340/400 (85.00%)\n",
            "\n",
            "85.0\n",
            "\n",
            "Test set: Average loss: 0.0109, Accuracy: 352/400 (88.00%)\n",
            "\n",
            "88.0\n",
            "Train Epoch: 70 [108/300 (90%)]\tLoss: 0.125551\n",
            "\n",
            "Test set: Average loss: 0.0131, Accuracy: 339/400 (84.75%)\n",
            "\n",
            "84.75\n",
            "\n",
            "Test set: Average loss: 0.0144, Accuracy: 348/400 (87.00%)\n",
            "\n",
            "87.0\n",
            "\n",
            "Test set: Average loss: 0.0130, Accuracy: 351/400 (87.75%)\n",
            "\n",
            "87.75\n",
            "\n",
            "Test set: Average loss: 0.0128, Accuracy: 343/400 (85.75%)\n",
            "\n",
            "85.75\n",
            "\n",
            "Test set: Average loss: 0.0122, Accuracy: 352/400 (88.00%)\n",
            "\n",
            "88.0\n",
            "\n",
            "Test set: Average loss: 0.0141, Accuracy: 317/400 (79.25%)\n",
            "\n",
            "79.25\n",
            "\n",
            "Test set: Average loss: 0.0094, Accuracy: 352/400 (88.00%)\n",
            "\n",
            "88.0\n",
            "Train Epoch: 77 [108/300 (90%)]\tLoss: 1.179552\n",
            "\n",
            "Test set: Average loss: 0.0112, Accuracy: 342/400 (85.50%)\n",
            "\n",
            "85.5\n",
            "\n",
            "Test set: Average loss: 0.0114, Accuracy: 342/400 (85.50%)\n",
            "\n",
            "85.5\n",
            "\n",
            "Test set: Average loss: 0.0108, Accuracy: 337/400 (84.25%)\n",
            "\n",
            "84.25\n",
            "\n",
            "Test set: Average loss: 0.0098, Accuracy: 351/400 (87.75%)\n",
            "\n",
            "87.75\n",
            "\n",
            "Test set: Average loss: 0.0128, Accuracy: 343/400 (85.75%)\n",
            "\n",
            "85.75\n",
            "\n",
            "Test set: Average loss: 0.0143, Accuracy: 343/400 (85.75%)\n",
            "\n",
            "85.75\n",
            "\n",
            "Test set: Average loss: 0.0161, Accuracy: 346/400 (86.50%)\n",
            "\n",
            "86.5\n",
            "Train Epoch: 84 [108/300 (90%)]\tLoss: 0.246602\n",
            "\n",
            "Test set: Average loss: 0.0212, Accuracy: 334/400 (83.50%)\n",
            "\n",
            "83.5\n",
            "\n",
            "Test set: Average loss: 0.0164, Accuracy: 341/400 (85.25%)\n",
            "\n",
            "85.25\n",
            "\n",
            "Test set: Average loss: 0.0140, Accuracy: 349/400 (87.25%)\n",
            "\n",
            "87.25\n",
            "\n",
            "Test set: Average loss: 0.0138, Accuracy: 345/400 (86.25%)\n",
            "\n",
            "86.25\n",
            "\n",
            "Test set: Average loss: 0.0132, Accuracy: 349/400 (87.25%)\n",
            "\n",
            "87.25\n",
            "\n",
            "Test set: Average loss: 0.0147, Accuracy: 343/400 (85.75%)\n",
            "\n",
            "85.75\n",
            "\n",
            "Test set: Average loss: 0.0154, Accuracy: 344/400 (86.00%)\n",
            "\n",
            "86.0\n",
            "Train Epoch: 91 [108/300 (90%)]\tLoss: 0.198394\n",
            "\n",
            "Test set: Average loss: 0.0154, Accuracy: 347/400 (86.75%)\n",
            "\n",
            "86.75\n",
            "\n",
            "Test set: Average loss: 0.0146, Accuracy: 348/400 (87.00%)\n",
            "\n",
            "87.0\n",
            "\n",
            "Test set: Average loss: 0.0145, Accuracy: 345/400 (86.25%)\n",
            "\n",
            "86.25\n",
            "\n",
            "Test set: Average loss: 0.0145, Accuracy: 346/400 (86.50%)\n",
            "\n",
            "86.5\n",
            "\n",
            "Test set: Average loss: 0.0142, Accuracy: 350/400 (87.50%)\n",
            "\n",
            "87.5\n",
            "\n",
            "Test set: Average loss: 0.0142, Accuracy: 348/400 (87.00%)\n",
            "\n",
            "87.0\n",
            "\n",
            "Test set: Average loss: 0.0147, Accuracy: 343/400 (85.75%)\n",
            "\n",
            "85.75\n",
            "Train Epoch: 98 [108/300 (90%)]\tLoss: 0.292525\n",
            "\n",
            "Test set: Average loss: 0.0149, Accuracy: 342/400 (85.50%)\n",
            "\n",
            "85.5\n",
            "\n",
            "Test set: Average loss: 0.0149, Accuracy: 341/400 (85.25%)\n",
            "\n",
            "85.25\n",
            "\n",
            "Test set: Average loss: 0.0151, Accuracy: 339/400 (84.75%)\n",
            "\n",
            "84.75\n",
            "\n",
            "Test set: Average loss: 0.0150, Accuracy: 341/400 (85.25%)\n",
            "\n",
            "85.25\n",
            "\n",
            "Test set: Average loss: 0.0150, Accuracy: 340/400 (85.00%)\n",
            "\n",
            "85.0\n",
            "\n",
            "Test set: Average loss: 0.0143, Accuracy: 349/400 (87.25%)\n",
            "\n",
            "87.25\n",
            "\n",
            "Test set: Average loss: 0.0144, Accuracy: 347/400 (86.75%)\n",
            "\n",
            "86.75\n",
            "Train Epoch: 105 [108/300 (90%)]\tLoss: 0.214725\n",
            "\n",
            "Test set: Average loss: 0.0145, Accuracy: 343/400 (85.75%)\n",
            "\n",
            "85.75\n",
            "\n",
            "Test set: Average loss: 0.0148, Accuracy: 344/400 (86.00%)\n",
            "\n",
            "86.0\n",
            "\n",
            "Test set: Average loss: 0.0145, Accuracy: 345/400 (86.25%)\n",
            "\n",
            "86.25\n",
            "\n",
            "Test set: Average loss: 0.0142, Accuracy: 347/400 (86.75%)\n",
            "\n",
            "86.75\n",
            "\n",
            "Test set: Average loss: 0.0146, Accuracy: 346/400 (86.50%)\n",
            "\n",
            "86.5\n",
            "\n",
            "Test set: Average loss: 0.0146, Accuracy: 343/400 (85.75%)\n",
            "\n",
            "85.75\n",
            "\n",
            "Test set: Average loss: 0.0143, Accuracy: 347/400 (86.75%)\n",
            "\n",
            "86.75\n",
            "Train Epoch: 112 [108/300 (90%)]\tLoss: 0.109508\n",
            "\n",
            "Test set: Average loss: 0.0142, Accuracy: 346/400 (86.50%)\n",
            "\n",
            "86.5\n",
            "\n",
            "Test set: Average loss: 0.0142, Accuracy: 347/400 (86.75%)\n",
            "\n",
            "86.75\n",
            "\n",
            "Test set: Average loss: 0.0146, Accuracy: 343/400 (85.75%)\n",
            "\n",
            "85.75\n",
            "\n",
            "Test set: Average loss: 0.0148, Accuracy: 342/400 (85.50%)\n",
            "\n",
            "85.5\n",
            "\n",
            "Test set: Average loss: 0.0150, Accuracy: 342/400 (85.50%)\n",
            "\n",
            "85.5\n",
            "\n",
            "Test set: Average loss: 0.0145, Accuracy: 344/400 (86.00%)\n",
            "\n",
            "86.0\n",
            "\n",
            "Test set: Average loss: 0.0146, Accuracy: 345/400 (86.25%)\n",
            "\n",
            "86.25\n",
            "Train Epoch: 119 [108/300 (90%)]\tLoss: 0.010184\n",
            "\n",
            "Test set: Average loss: 0.0149, Accuracy: 340/400 (85.00%)\n",
            "\n",
            "85.0\n",
            "Seed Number is 16\n",
            "Num Samples For Training 50 Num Samples For Val 400\n",
            "Train Epoch: 0 [108/300 (90%)]\tLoss: 20.978941\n",
            "\n",
            "Test set: Average loss: 0.0320, Accuracy: 189/400 (47.25%)\n",
            "\n",
            "47.25\n",
            "\n",
            "Test set: Average loss: 0.0183, Accuracy: 202/400 (50.50%)\n",
            "\n",
            "50.5\n",
            "\n",
            "Test set: Average loss: 0.0142, Accuracy: 246/400 (61.50%)\n",
            "\n",
            "61.5\n",
            "\n",
            "Test set: Average loss: 0.0120, Accuracy: 294/400 (73.50%)\n",
            "\n",
            "73.5\n",
            "\n",
            "Test set: Average loss: 0.0101, Accuracy: 309/400 (77.25%)\n",
            "\n",
            "77.25\n",
            "\n",
            "Test set: Average loss: 0.0100, Accuracy: 303/400 (75.75%)\n",
            "\n",
            "75.75\n",
            "\n",
            "Test set: Average loss: 0.0084, Accuracy: 328/400 (82.00%)\n",
            "\n",
            "82.0\n",
            "Train Epoch: 7 [108/300 (90%)]\tLoss: 1.939492\n",
            "\n",
            "Test set: Average loss: 0.0125, Accuracy: 304/400 (76.00%)\n",
            "\n",
            "76.0\n",
            "\n",
            "Test set: Average loss: 0.0210, Accuracy: 288/400 (72.00%)\n",
            "\n",
            "72.0\n",
            "\n",
            "Test set: Average loss: 0.0099, Accuracy: 323/400 (80.75%)\n",
            "\n",
            "80.75\n",
            "\n",
            "Test set: Average loss: 0.0106, Accuracy: 316/400 (79.00%)\n",
            "\n",
            "79.0\n",
            "\n",
            "Test set: Average loss: 0.0105, Accuracy: 311/400 (77.75%)\n",
            "\n",
            "77.75\n",
            "\n",
            "Test set: Average loss: 0.0087, Accuracy: 326/400 (81.50%)\n",
            "\n",
            "81.5\n",
            "\n",
            "Test set: Average loss: 0.0199, Accuracy: 296/400 (74.00%)\n",
            "\n",
            "74.0\n",
            "Train Epoch: 14 [108/300 (90%)]\tLoss: 0.461512\n",
            "\n",
            "Test set: Average loss: 0.0136, Accuracy: 313/400 (78.25%)\n",
            "\n",
            "78.25\n",
            "\n",
            "Test set: Average loss: 0.0089, Accuracy: 335/400 (83.75%)\n",
            "\n",
            "83.75\n",
            "\n",
            "Test set: Average loss: 0.0157, Accuracy: 308/400 (77.00%)\n",
            "\n",
            "77.0\n",
            "\n",
            "Test set: Average loss: 0.0120, Accuracy: 333/400 (83.25%)\n",
            "\n",
            "83.25\n",
            "\n",
            "Test set: Average loss: 0.0130, Accuracy: 325/400 (81.25%)\n",
            "\n",
            "81.25\n",
            "\n",
            "Test set: Average loss: 0.0101, Accuracy: 334/400 (83.50%)\n",
            "\n",
            "83.5\n",
            "\n",
            "Test set: Average loss: 0.0147, Accuracy: 313/400 (78.25%)\n",
            "\n",
            "78.25\n",
            "Train Epoch: 21 [108/300 (90%)]\tLoss: 1.377718\n",
            "\n",
            "Test set: Average loss: 0.0110, Accuracy: 335/400 (83.75%)\n",
            "\n",
            "83.75\n",
            "\n",
            "Test set: Average loss: 0.0115, Accuracy: 333/400 (83.25%)\n",
            "\n",
            "83.25\n",
            "\n",
            "Test set: Average loss: 0.0115, Accuracy: 325/400 (81.25%)\n",
            "\n",
            "81.25\n",
            "\n",
            "Test set: Average loss: 0.0128, Accuracy: 327/400 (81.75%)\n",
            "\n",
            "81.75\n",
            "\n",
            "Test set: Average loss: 0.0140, Accuracy: 323/400 (80.75%)\n",
            "\n",
            "80.75\n",
            "\n",
            "Test set: Average loss: 0.0167, Accuracy: 321/400 (80.25%)\n",
            "\n",
            "80.25\n",
            "\n",
            "Test set: Average loss: 0.0101, Accuracy: 334/400 (83.50%)\n",
            "\n",
            "83.5\n",
            "Train Epoch: 28 [108/300 (90%)]\tLoss: 2.908117\n",
            "\n",
            "Test set: Average loss: 0.0108, Accuracy: 340/400 (85.00%)\n",
            "\n",
            "85.0\n",
            "\n",
            "Test set: Average loss: 0.0105, Accuracy: 337/400 (84.25%)\n",
            "\n",
            "84.25\n",
            "\n",
            "Test set: Average loss: 0.0133, Accuracy: 316/400 (79.00%)\n",
            "\n",
            "79.0\n",
            "\n",
            "Test set: Average loss: 0.0113, Accuracy: 334/400 (83.50%)\n",
            "\n",
            "83.5\n",
            "\n",
            "Test set: Average loss: 0.0187, Accuracy: 327/400 (81.75%)\n",
            "\n",
            "81.75\n",
            "\n",
            "Test set: Average loss: 0.0224, Accuracy: 301/400 (75.25%)\n",
            "\n",
            "75.25\n",
            "\n",
            "Test set: Average loss: 0.0121, Accuracy: 329/400 (82.25%)\n",
            "\n",
            "82.25\n",
            "Train Epoch: 35 [108/300 (90%)]\tLoss: 0.203022\n",
            "\n",
            "Test set: Average loss: 0.0081, Accuracy: 349/400 (87.25%)\n",
            "\n",
            "87.25\n",
            "\n",
            "Test set: Average loss: 0.0124, Accuracy: 325/400 (81.25%)\n",
            "\n",
            "81.25\n",
            "\n",
            "Test set: Average loss: 0.0152, Accuracy: 326/400 (81.50%)\n",
            "\n",
            "81.5\n",
            "\n",
            "Test set: Average loss: 0.0132, Accuracy: 340/400 (85.00%)\n",
            "\n",
            "85.0\n",
            "\n",
            "Test set: Average loss: 0.0104, Accuracy: 347/400 (86.75%)\n",
            "\n",
            "86.75\n",
            "\n",
            "Test set: Average loss: 0.0112, Accuracy: 339/400 (84.75%)\n",
            "\n",
            "84.75\n",
            "\n",
            "Test set: Average loss: 0.0179, Accuracy: 325/400 (81.25%)\n",
            "\n",
            "81.25\n",
            "Train Epoch: 42 [108/300 (90%)]\tLoss: 0.509870\n",
            "\n",
            "Test set: Average loss: 0.0207, Accuracy: 324/400 (81.00%)\n",
            "\n",
            "81.0\n",
            "\n",
            "Test set: Average loss: 0.0191, Accuracy: 320/400 (80.00%)\n",
            "\n",
            "80.0\n",
            "\n",
            "Test set: Average loss: 0.0121, Accuracy: 334/400 (83.50%)\n",
            "\n",
            "83.5\n",
            "\n",
            "Test set: Average loss: 0.0161, Accuracy: 328/400 (82.00%)\n",
            "\n",
            "82.0\n",
            "\n",
            "Test set: Average loss: 0.0136, Accuracy: 338/400 (84.50%)\n",
            "\n",
            "84.5\n",
            "\n",
            "Test set: Average loss: 0.0163, Accuracy: 326/400 (81.50%)\n",
            "\n",
            "81.5\n",
            "\n",
            "Test set: Average loss: 0.0142, Accuracy: 334/400 (83.50%)\n",
            "\n",
            "83.5\n",
            "Train Epoch: 49 [108/300 (90%)]\tLoss: 0.332699\n",
            "\n",
            "Test set: Average loss: 0.0095, Accuracy: 337/400 (84.25%)\n",
            "\n",
            "84.25\n",
            "\n",
            "Test set: Average loss: 0.0140, Accuracy: 335/400 (83.75%)\n",
            "\n",
            "83.75\n",
            "\n",
            "Test set: Average loss: 0.0088, Accuracy: 345/400 (86.25%)\n",
            "\n",
            "86.25\n",
            "\n",
            "Test set: Average loss: 0.0093, Accuracy: 354/400 (88.50%)\n",
            "\n",
            "88.5\n",
            "\n",
            "Test set: Average loss: 0.0213, Accuracy: 317/400 (79.25%)\n",
            "\n",
            "79.25\n",
            "\n",
            "Test set: Average loss: 0.0120, Accuracy: 347/400 (86.75%)\n",
            "\n",
            "86.75\n",
            "\n",
            "Test set: Average loss: 0.0128, Accuracy: 343/400 (85.75%)\n",
            "\n",
            "85.75\n",
            "Train Epoch: 56 [108/300 (90%)]\tLoss: 2.042591\n",
            "\n",
            "Test set: Average loss: 0.0104, Accuracy: 334/400 (83.50%)\n",
            "\n",
            "83.5\n",
            "\n",
            "Test set: Average loss: 0.0120, Accuracy: 333/400 (83.25%)\n",
            "\n",
            "83.25\n",
            "\n",
            "Test set: Average loss: 0.0156, Accuracy: 323/400 (80.75%)\n",
            "\n",
            "80.75\n",
            "\n",
            "Test set: Average loss: 0.0107, Accuracy: 345/400 (86.25%)\n",
            "\n",
            "86.25\n",
            "\n",
            "Test set: Average loss: 0.0169, Accuracy: 340/400 (85.00%)\n",
            "\n",
            "85.0\n",
            "\n",
            "Test set: Average loss: 0.0222, Accuracy: 331/400 (82.75%)\n",
            "\n",
            "82.75\n",
            "\n",
            "Test set: Average loss: 0.0121, Accuracy: 349/400 (87.25%)\n",
            "\n",
            "87.25\n",
            "Train Epoch: 63 [108/300 (90%)]\tLoss: 0.131271\n",
            "\n",
            "Test set: Average loss: 0.0196, Accuracy: 328/400 (82.00%)\n",
            "\n",
            "82.0\n",
            "\n",
            "Test set: Average loss: 0.0142, Accuracy: 339/400 (84.75%)\n",
            "\n",
            "84.75\n",
            "\n",
            "Test set: Average loss: 0.0143, Accuracy: 340/400 (85.00%)\n",
            "\n",
            "85.0\n",
            "\n",
            "Test set: Average loss: 0.0156, Accuracy: 336/400 (84.00%)\n",
            "\n",
            "84.0\n",
            "\n",
            "Test set: Average loss: 0.0206, Accuracy: 329/400 (82.25%)\n",
            "\n",
            "82.25\n",
            "\n",
            "Test set: Average loss: 0.0287, Accuracy: 323/400 (80.75%)\n",
            "\n",
            "80.75\n",
            "\n",
            "Test set: Average loss: 0.0266, Accuracy: 330/400 (82.50%)\n",
            "\n",
            "82.5\n",
            "Train Epoch: 70 [108/300 (90%)]\tLoss: 0.116474\n",
            "\n",
            "Test set: Average loss: 0.0170, Accuracy: 339/400 (84.75%)\n",
            "\n",
            "84.75\n",
            "\n",
            "Test set: Average loss: 0.0153, Accuracy: 343/400 (85.75%)\n",
            "\n",
            "85.75\n",
            "\n",
            "Test set: Average loss: 0.0161, Accuracy: 344/400 (86.00%)\n",
            "\n",
            "86.0\n",
            "\n",
            "Test set: Average loss: 0.0208, Accuracy: 331/400 (82.75%)\n",
            "\n",
            "82.75\n",
            "\n",
            "Test set: Average loss: 0.0134, Accuracy: 346/400 (86.50%)\n",
            "\n",
            "86.5\n",
            "\n",
            "Test set: Average loss: 0.0146, Accuracy: 335/400 (83.75%)\n",
            "\n",
            "83.75\n",
            "\n",
            "Test set: Average loss: 0.0135, Accuracy: 335/400 (83.75%)\n",
            "\n",
            "83.75\n",
            "Train Epoch: 77 [108/300 (90%)]\tLoss: 2.302058\n",
            "\n",
            "Test set: Average loss: 0.0149, Accuracy: 339/400 (84.75%)\n",
            "\n",
            "84.75\n",
            "\n",
            "Test set: Average loss: 0.0185, Accuracy: 327/400 (81.75%)\n",
            "\n",
            "81.75\n",
            "\n",
            "Test set: Average loss: 0.0092, Accuracy: 350/400 (87.50%)\n",
            "\n",
            "87.5\n",
            "\n",
            "Test set: Average loss: 0.0091, Accuracy: 352/400 (88.00%)\n",
            "\n",
            "88.0\n",
            "\n",
            "Test set: Average loss: 0.0133, Accuracy: 335/400 (83.75%)\n",
            "\n",
            "83.75\n",
            "\n",
            "Test set: Average loss: 0.0238, Accuracy: 326/400 (81.50%)\n",
            "\n",
            "81.5\n",
            "\n",
            "Test set: Average loss: 0.0156, Accuracy: 340/400 (85.00%)\n",
            "\n",
            "85.0\n",
            "Train Epoch: 84 [108/300 (90%)]\tLoss: 0.010844\n",
            "\n",
            "Test set: Average loss: 0.0165, Accuracy: 341/400 (85.25%)\n",
            "\n",
            "85.25\n",
            "\n",
            "Test set: Average loss: 0.0192, Accuracy: 335/400 (83.75%)\n",
            "\n",
            "83.75\n",
            "\n",
            "Test set: Average loss: 0.0181, Accuracy: 337/400 (84.25%)\n",
            "\n",
            "84.25\n",
            "\n",
            "Test set: Average loss: 0.0222, Accuracy: 330/400 (82.50%)\n",
            "\n",
            "82.5\n",
            "\n",
            "Test set: Average loss: 0.0206, Accuracy: 331/400 (82.75%)\n",
            "\n",
            "82.75\n",
            "\n",
            "Test set: Average loss: 0.0156, Accuracy: 339/400 (84.75%)\n",
            "\n",
            "84.75\n",
            "\n",
            "Test set: Average loss: 0.0168, Accuracy: 339/400 (84.75%)\n",
            "\n",
            "84.75\n",
            "Train Epoch: 91 [108/300 (90%)]\tLoss: 0.107954\n",
            "\n",
            "Test set: Average loss: 0.0185, Accuracy: 334/400 (83.50%)\n",
            "\n",
            "83.5\n",
            "\n",
            "Test set: Average loss: 0.0187, Accuracy: 334/400 (83.50%)\n",
            "\n",
            "83.5\n",
            "\n",
            "Test set: Average loss: 0.0197, Accuracy: 333/400 (83.25%)\n",
            "\n",
            "83.25\n",
            "\n",
            "Test set: Average loss: 0.0206, Accuracy: 334/400 (83.50%)\n",
            "\n",
            "83.5\n",
            "\n",
            "Test set: Average loss: 0.0200, Accuracy: 334/400 (83.50%)\n",
            "\n",
            "83.5\n",
            "\n",
            "Test set: Average loss: 0.0197, Accuracy: 335/400 (83.75%)\n",
            "\n",
            "83.75\n",
            "\n",
            "Test set: Average loss: 0.0197, Accuracy: 335/400 (83.75%)\n",
            "\n",
            "83.75\n",
            "Train Epoch: 98 [108/300 (90%)]\tLoss: 0.037823\n",
            "\n",
            "Test set: Average loss: 0.0198, Accuracy: 332/400 (83.00%)\n",
            "\n",
            "83.0\n",
            "\n",
            "Test set: Average loss: 0.0198, Accuracy: 334/400 (83.50%)\n",
            "\n",
            "83.5\n",
            "\n",
            "Test set: Average loss: 0.0189, Accuracy: 336/400 (84.00%)\n",
            "\n",
            "84.0\n",
            "\n",
            "Test set: Average loss: 0.0196, Accuracy: 337/400 (84.25%)\n",
            "\n",
            "84.25\n",
            "\n",
            "Test set: Average loss: 0.0192, Accuracy: 336/400 (84.00%)\n",
            "\n",
            "84.0\n",
            "\n",
            "Test set: Average loss: 0.0184, Accuracy: 337/400 (84.25%)\n",
            "\n",
            "84.25\n",
            "\n",
            "Test set: Average loss: 0.0188, Accuracy: 336/400 (84.00%)\n",
            "\n",
            "84.0\n",
            "Train Epoch: 105 [108/300 (90%)]\tLoss: 0.001076\n",
            "\n",
            "Test set: Average loss: 0.0185, Accuracy: 337/400 (84.25%)\n",
            "\n",
            "84.25\n",
            "\n",
            "Test set: Average loss: 0.0178, Accuracy: 339/400 (84.75%)\n",
            "\n",
            "84.75\n",
            "\n",
            "Test set: Average loss: 0.0178, Accuracy: 338/400 (84.50%)\n",
            "\n",
            "84.5\n",
            "\n",
            "Test set: Average loss: 0.0181, Accuracy: 339/400 (84.75%)\n",
            "\n",
            "84.75\n",
            "\n",
            "Test set: Average loss: 0.0187, Accuracy: 338/400 (84.50%)\n",
            "\n",
            "84.5\n",
            "\n",
            "Test set: Average loss: 0.0179, Accuracy: 337/400 (84.25%)\n",
            "\n",
            "84.25\n",
            "\n",
            "Test set: Average loss: 0.0186, Accuracy: 338/400 (84.50%)\n",
            "\n",
            "84.5\n",
            "Train Epoch: 112 [108/300 (90%)]\tLoss: 0.089832\n",
            "\n",
            "Test set: Average loss: 0.0183, Accuracy: 338/400 (84.50%)\n",
            "\n",
            "84.5\n",
            "\n",
            "Test set: Average loss: 0.0187, Accuracy: 338/400 (84.50%)\n",
            "\n",
            "84.5\n",
            "\n",
            "Test set: Average loss: 0.0189, Accuracy: 338/400 (84.50%)\n",
            "\n",
            "84.5\n",
            "\n",
            "Test set: Average loss: 0.0191, Accuracy: 339/400 (84.75%)\n",
            "\n",
            "84.75\n",
            "\n",
            "Test set: Average loss: 0.0183, Accuracy: 339/400 (84.75%)\n",
            "\n",
            "84.75\n",
            "\n",
            "Test set: Average loss: 0.0190, Accuracy: 338/400 (84.50%)\n",
            "\n",
            "84.5\n",
            "\n",
            "Test set: Average loss: 0.0187, Accuracy: 339/400 (84.75%)\n",
            "\n",
            "84.75\n",
            "Train Epoch: 119 [108/300 (90%)]\tLoss: 0.120834\n",
            "\n",
            "Test set: Average loss: 0.0188, Accuracy: 338/400 (84.50%)\n",
            "\n",
            "84.5\n",
            "Seed Number is 17\n",
            "Num Samples For Training 50 Num Samples For Val 400\n",
            "Train Epoch: 0 [108/300 (90%)]\tLoss: 19.538174\n",
            "\n",
            "Test set: Average loss: 0.0329, Accuracy: 184/400 (46.00%)\n",
            "\n",
            "46.0\n",
            "\n",
            "Test set: Average loss: 0.0177, Accuracy: 217/400 (54.25%)\n",
            "\n",
            "54.25\n",
            "\n",
            "Test set: Average loss: 0.0134, Accuracy: 241/400 (60.25%)\n",
            "\n",
            "60.25\n",
            "\n",
            "Test set: Average loss: 0.0106, Accuracy: 307/400 (76.75%)\n",
            "\n",
            "76.75\n",
            "\n",
            "Test set: Average loss: 0.0090, Accuracy: 325/400 (81.25%)\n",
            "\n",
            "81.25\n",
            "\n",
            "Test set: Average loss: 0.0086, Accuracy: 324/400 (81.00%)\n",
            "\n",
            "81.0\n",
            "\n",
            "Test set: Average loss: 0.0082, Accuracy: 324/400 (81.00%)\n",
            "\n",
            "81.0\n",
            "Train Epoch: 7 [108/300 (90%)]\tLoss: 1.753638\n",
            "\n",
            "Test set: Average loss: 0.0079, Accuracy: 336/400 (84.00%)\n",
            "\n",
            "84.0\n",
            "\n",
            "Test set: Average loss: 0.0091, Accuracy: 333/400 (83.25%)\n",
            "\n",
            "83.25\n",
            "\n",
            "Test set: Average loss: 0.0164, Accuracy: 289/400 (72.25%)\n",
            "\n",
            "72.25\n",
            "\n",
            "Test set: Average loss: 0.0100, Accuracy: 325/400 (81.25%)\n",
            "\n",
            "81.25\n",
            "\n",
            "Test set: Average loss: 0.0149, Accuracy: 301/400 (75.25%)\n",
            "\n",
            "75.25\n",
            "\n",
            "Test set: Average loss: 0.0115, Accuracy: 321/400 (80.25%)\n",
            "\n",
            "80.25\n",
            "\n",
            "Test set: Average loss: 0.0118, Accuracy: 315/400 (78.75%)\n",
            "\n",
            "78.75\n",
            "Train Epoch: 14 [108/300 (90%)]\tLoss: 2.174973\n",
            "\n",
            "Test set: Average loss: 0.0112, Accuracy: 324/400 (81.00%)\n",
            "\n",
            "81.0\n",
            "\n",
            "Test set: Average loss: 0.0109, Accuracy: 334/400 (83.50%)\n",
            "\n",
            "83.5\n",
            "\n",
            "Test set: Average loss: 0.0145, Accuracy: 321/400 (80.25%)\n",
            "\n",
            "80.25\n",
            "\n",
            "Test set: Average loss: 0.0123, Accuracy: 323/400 (80.75%)\n",
            "\n",
            "80.75\n",
            "\n",
            "Test set: Average loss: 0.0123, Accuracy: 316/400 (79.00%)\n",
            "\n",
            "79.0\n",
            "\n",
            "Test set: Average loss: 0.0141, Accuracy: 318/400 (79.50%)\n",
            "\n",
            "79.5\n",
            "\n",
            "Test set: Average loss: 0.0134, Accuracy: 319/400 (79.75%)\n",
            "\n",
            "79.75\n",
            "Train Epoch: 21 [108/300 (90%)]\tLoss: 0.240436\n",
            "\n",
            "Test set: Average loss: 0.0147, Accuracy: 316/400 (79.00%)\n",
            "\n",
            "79.0\n",
            "\n",
            "Test set: Average loss: 0.0200, Accuracy: 304/400 (76.00%)\n",
            "\n",
            "76.0\n",
            "\n",
            "Test set: Average loss: 0.0177, Accuracy: 303/400 (75.75%)\n",
            "\n",
            "75.75\n",
            "\n",
            "Test set: Average loss: 0.0168, Accuracy: 315/400 (78.75%)\n",
            "\n",
            "78.75\n",
            "\n",
            "Test set: Average loss: 0.0138, Accuracy: 323/400 (80.75%)\n",
            "\n",
            "80.75\n",
            "\n",
            "Test set: Average loss: 0.0153, Accuracy: 307/400 (76.75%)\n",
            "\n",
            "76.75\n",
            "\n",
            "Test set: Average loss: 0.0152, Accuracy: 311/400 (77.75%)\n",
            "\n",
            "77.75\n",
            "Train Epoch: 28 [108/300 (90%)]\tLoss: 0.780198\n",
            "\n",
            "Test set: Average loss: 0.0166, Accuracy: 326/400 (81.50%)\n",
            "\n",
            "81.5\n",
            "\n",
            "Test set: Average loss: 0.0162, Accuracy: 331/400 (82.75%)\n",
            "\n",
            "82.75\n",
            "\n",
            "Test set: Average loss: 0.0194, Accuracy: 301/400 (75.25%)\n",
            "\n",
            "75.25\n",
            "\n",
            "Test set: Average loss: 0.0150, Accuracy: 315/400 (78.75%)\n",
            "\n",
            "78.75\n",
            "\n",
            "Test set: Average loss: 0.0122, Accuracy: 318/400 (79.50%)\n",
            "\n",
            "79.5\n",
            "\n",
            "Test set: Average loss: 0.0151, Accuracy: 311/400 (77.75%)\n",
            "\n",
            "77.75\n",
            "\n",
            "Test set: Average loss: 0.0208, Accuracy: 309/400 (77.25%)\n",
            "\n",
            "77.25\n",
            "Train Epoch: 35 [108/300 (90%)]\tLoss: 0.918760\n",
            "\n",
            "Test set: Average loss: 0.0123, Accuracy: 310/400 (77.50%)\n",
            "\n",
            "77.5\n",
            "\n",
            "Test set: Average loss: 0.0124, Accuracy: 319/400 (79.75%)\n",
            "\n",
            "79.75\n",
            "\n",
            "Test set: Average loss: 0.0126, Accuracy: 325/400 (81.25%)\n",
            "\n",
            "81.25\n",
            "\n",
            "Test set: Average loss: 0.0177, Accuracy: 308/400 (77.00%)\n",
            "\n",
            "77.0\n",
            "\n",
            "Test set: Average loss: 0.0159, Accuracy: 323/400 (80.75%)\n",
            "\n",
            "80.75\n",
            "\n",
            "Test set: Average loss: 0.0148, Accuracy: 329/400 (82.25%)\n",
            "\n",
            "82.25\n",
            "\n",
            "Test set: Average loss: 0.0172, Accuracy: 313/400 (78.25%)\n",
            "\n",
            "78.25\n",
            "Train Epoch: 42 [108/300 (90%)]\tLoss: 0.282064\n",
            "\n",
            "Test set: Average loss: 0.0137, Accuracy: 307/400 (76.75%)\n",
            "\n",
            "76.75\n",
            "\n",
            "Test set: Average loss: 0.0111, Accuracy: 322/400 (80.50%)\n",
            "\n",
            "80.5\n",
            "\n",
            "Test set: Average loss: 0.0158, Accuracy: 323/400 (80.75%)\n",
            "\n",
            "80.75\n",
            "\n",
            "Test set: Average loss: 0.0152, Accuracy: 333/400 (83.25%)\n",
            "\n",
            "83.25\n",
            "\n",
            "Test set: Average loss: 0.0191, Accuracy: 322/400 (80.50%)\n",
            "\n",
            "80.5\n",
            "\n",
            "Test set: Average loss: 0.0135, Accuracy: 317/400 (79.25%)\n",
            "\n",
            "79.25\n",
            "\n",
            "Test set: Average loss: 0.0164, Accuracy: 323/400 (80.75%)\n",
            "\n",
            "80.75\n",
            "Train Epoch: 49 [108/300 (90%)]\tLoss: 0.132096\n",
            "\n",
            "Test set: Average loss: 0.0210, Accuracy: 337/400 (84.25%)\n",
            "\n",
            "84.25\n",
            "\n",
            "Test set: Average loss: 0.0159, Accuracy: 326/400 (81.50%)\n",
            "\n",
            "81.5\n",
            "\n",
            "Test set: Average loss: 0.0141, Accuracy: 317/400 (79.25%)\n",
            "\n",
            "79.25\n",
            "\n",
            "Test set: Average loss: 0.0136, Accuracy: 311/400 (77.75%)\n",
            "\n",
            "77.75\n",
            "\n",
            "Test set: Average loss: 0.0158, Accuracy: 325/400 (81.25%)\n",
            "\n",
            "81.25\n",
            "\n",
            "Test set: Average loss: 0.0157, Accuracy: 320/400 (80.00%)\n",
            "\n",
            "80.0\n",
            "\n",
            "Test set: Average loss: 0.0143, Accuracy: 316/400 (79.00%)\n",
            "\n",
            "79.0\n",
            "Train Epoch: 56 [108/300 (90%)]\tLoss: 0.048233\n",
            "\n",
            "Test set: Average loss: 0.0246, Accuracy: 313/400 (78.25%)\n",
            "\n",
            "78.25\n",
            "\n",
            "Test set: Average loss: 0.0203, Accuracy: 322/400 (80.50%)\n",
            "\n",
            "80.5\n",
            "\n",
            "Test set: Average loss: 0.0235, Accuracy: 329/400 (82.25%)\n",
            "\n",
            "82.25\n",
            "\n",
            "Test set: Average loss: 0.0230, Accuracy: 325/400 (81.25%)\n",
            "\n",
            "81.25\n",
            "\n",
            "Test set: Average loss: 0.0143, Accuracy: 336/400 (84.00%)\n",
            "\n",
            "84.0\n",
            "\n",
            "Test set: Average loss: 0.0162, Accuracy: 320/400 (80.00%)\n",
            "\n",
            "80.0\n",
            "\n",
            "Test set: Average loss: 0.0186, Accuracy: 323/400 (80.75%)\n",
            "\n",
            "80.75\n",
            "Train Epoch: 63 [108/300 (90%)]\tLoss: 0.907820\n",
            "\n",
            "Test set: Average loss: 0.0222, Accuracy: 323/400 (80.75%)\n",
            "\n",
            "80.75\n",
            "\n",
            "Test set: Average loss: 0.0130, Accuracy: 327/400 (81.75%)\n",
            "\n",
            "81.75\n",
            "\n",
            "Test set: Average loss: 0.0115, Accuracy: 328/400 (82.00%)\n",
            "\n",
            "82.0\n",
            "\n",
            "Test set: Average loss: 0.0101, Accuracy: 328/400 (82.00%)\n",
            "\n",
            "82.0\n",
            "\n",
            "Test set: Average loss: 0.0128, Accuracy: 330/400 (82.50%)\n",
            "\n",
            "82.5\n",
            "\n",
            "Test set: Average loss: 0.0169, Accuracy: 330/400 (82.50%)\n",
            "\n",
            "82.5\n",
            "\n",
            "Test set: Average loss: 0.0198, Accuracy: 326/400 (81.50%)\n",
            "\n",
            "81.5\n",
            "Train Epoch: 70 [108/300 (90%)]\tLoss: 0.020440\n",
            "\n",
            "Test set: Average loss: 0.0177, Accuracy: 327/400 (81.75%)\n",
            "\n",
            "81.75\n",
            "\n",
            "Test set: Average loss: 0.0176, Accuracy: 325/400 (81.25%)\n",
            "\n",
            "81.25\n",
            "\n",
            "Test set: Average loss: 0.0159, Accuracy: 330/400 (82.50%)\n",
            "\n",
            "82.5\n",
            "\n",
            "Test set: Average loss: 0.0162, Accuracy: 335/400 (83.75%)\n",
            "\n",
            "83.75\n",
            "\n",
            "Test set: Average loss: 0.0149, Accuracy: 334/400 (83.50%)\n",
            "\n",
            "83.5\n",
            "\n",
            "Test set: Average loss: 0.0130, Accuracy: 332/400 (83.00%)\n",
            "\n",
            "83.0\n",
            "\n",
            "Test set: Average loss: 0.0144, Accuracy: 327/400 (81.75%)\n",
            "\n",
            "81.75\n",
            "Train Epoch: 77 [108/300 (90%)]\tLoss: 0.060686\n",
            "\n",
            "Test set: Average loss: 0.0137, Accuracy: 327/400 (81.75%)\n",
            "\n",
            "81.75\n",
            "\n",
            "Test set: Average loss: 0.0145, Accuracy: 327/400 (81.75%)\n",
            "\n",
            "81.75\n",
            "\n",
            "Test set: Average loss: 0.0151, Accuracy: 325/400 (81.25%)\n",
            "\n",
            "81.25\n",
            "\n",
            "Test set: Average loss: 0.0144, Accuracy: 325/400 (81.25%)\n",
            "\n",
            "81.25\n",
            "\n",
            "Test set: Average loss: 0.0143, Accuracy: 329/400 (82.25%)\n",
            "\n",
            "82.25\n",
            "\n",
            "Test set: Average loss: 0.0155, Accuracy: 328/400 (82.00%)\n",
            "\n",
            "82.0\n",
            "\n",
            "Test set: Average loss: 0.0171, Accuracy: 329/400 (82.25%)\n",
            "\n",
            "82.25\n",
            "Train Epoch: 84 [108/300 (90%)]\tLoss: 0.393417\n",
            "\n",
            "Test set: Average loss: 0.0172, Accuracy: 325/400 (81.25%)\n",
            "\n",
            "81.25\n",
            "\n",
            "Test set: Average loss: 0.0168, Accuracy: 329/400 (82.25%)\n",
            "\n",
            "82.25\n",
            "\n",
            "Test set: Average loss: 0.0164, Accuracy: 331/400 (82.75%)\n",
            "\n",
            "82.75\n",
            "\n",
            "Test set: Average loss: 0.0161, Accuracy: 328/400 (82.00%)\n",
            "\n",
            "82.0\n",
            "\n",
            "Test set: Average loss: 0.0173, Accuracy: 327/400 (81.75%)\n",
            "\n",
            "81.75\n",
            "\n",
            "Test set: Average loss: 0.0179, Accuracy: 328/400 (82.00%)\n",
            "\n",
            "82.0\n",
            "\n",
            "Test set: Average loss: 0.0182, Accuracy: 331/400 (82.75%)\n",
            "\n",
            "82.75\n",
            "Train Epoch: 91 [108/300 (90%)]\tLoss: 0.125105\n",
            "\n",
            "Test set: Average loss: 0.0182, Accuracy: 332/400 (83.00%)\n",
            "\n",
            "83.0\n",
            "\n",
            "Test set: Average loss: 0.0180, Accuracy: 331/400 (82.75%)\n",
            "\n",
            "82.75\n",
            "\n",
            "Test set: Average loss: 0.0180, Accuracy: 331/400 (82.75%)\n",
            "\n",
            "82.75\n",
            "\n",
            "Test set: Average loss: 0.0178, Accuracy: 330/400 (82.50%)\n",
            "\n",
            "82.5\n",
            "\n",
            "Test set: Average loss: 0.0176, Accuracy: 332/400 (83.00%)\n",
            "\n",
            "83.0\n",
            "\n",
            "Test set: Average loss: 0.0177, Accuracy: 331/400 (82.75%)\n",
            "\n",
            "82.75\n",
            "\n",
            "Test set: Average loss: 0.0176, Accuracy: 335/400 (83.75%)\n",
            "\n",
            "83.75\n",
            "Train Epoch: 98 [108/300 (90%)]\tLoss: 0.027032\n",
            "\n",
            "Test set: Average loss: 0.0177, Accuracy: 332/400 (83.00%)\n",
            "\n",
            "83.0\n",
            "\n",
            "Test set: Average loss: 0.0178, Accuracy: 333/400 (83.25%)\n",
            "\n",
            "83.25\n",
            "\n",
            "Test set: Average loss: 0.0176, Accuracy: 333/400 (83.25%)\n",
            "\n",
            "83.25\n",
            "\n",
            "Test set: Average loss: 0.0172, Accuracy: 333/400 (83.25%)\n",
            "\n",
            "83.25\n",
            "\n",
            "Test set: Average loss: 0.0171, Accuracy: 332/400 (83.00%)\n",
            "\n",
            "83.0\n",
            "\n",
            "Test set: Average loss: 0.0172, Accuracy: 331/400 (82.75%)\n",
            "\n",
            "82.75\n",
            "\n",
            "Test set: Average loss: 0.0169, Accuracy: 332/400 (83.00%)\n",
            "\n",
            "83.0\n",
            "Train Epoch: 105 [108/300 (90%)]\tLoss: 0.955608\n",
            "\n",
            "Test set: Average loss: 0.0168, Accuracy: 334/400 (83.50%)\n",
            "\n",
            "83.5\n",
            "\n",
            "Test set: Average loss: 0.0173, Accuracy: 333/400 (83.25%)\n",
            "\n",
            "83.25\n",
            "\n",
            "Test set: Average loss: 0.0175, Accuracy: 331/400 (82.75%)\n",
            "\n",
            "82.75\n",
            "\n",
            "Test set: Average loss: 0.0175, Accuracy: 332/400 (83.00%)\n",
            "\n",
            "83.0\n",
            "\n",
            "Test set: Average loss: 0.0174, Accuracy: 333/400 (83.25%)\n",
            "\n",
            "83.25\n",
            "\n",
            "Test set: Average loss: 0.0171, Accuracy: 332/400 (83.00%)\n",
            "\n",
            "83.0\n",
            "\n",
            "Test set: Average loss: 0.0172, Accuracy: 332/400 (83.00%)\n",
            "\n",
            "83.0\n",
            "Train Epoch: 112 [108/300 (90%)]\tLoss: 0.019453\n",
            "\n",
            "Test set: Average loss: 0.0171, Accuracy: 332/400 (83.00%)\n",
            "\n",
            "83.0\n",
            "\n",
            "Test set: Average loss: 0.0171, Accuracy: 331/400 (82.75%)\n",
            "\n",
            "82.75\n",
            "\n",
            "Test set: Average loss: 0.0171, Accuracy: 333/400 (83.25%)\n",
            "\n",
            "83.25\n",
            "\n",
            "Test set: Average loss: 0.0171, Accuracy: 334/400 (83.50%)\n",
            "\n",
            "83.5\n",
            "\n",
            "Test set: Average loss: 0.0171, Accuracy: 333/400 (83.25%)\n",
            "\n",
            "83.25\n",
            "\n",
            "Test set: Average loss: 0.0169, Accuracy: 331/400 (82.75%)\n",
            "\n",
            "82.75\n",
            "\n",
            "Test set: Average loss: 0.0168, Accuracy: 334/400 (83.50%)\n",
            "\n",
            "83.5\n",
            "Train Epoch: 119 [108/300 (90%)]\tLoss: 0.019072\n",
            "\n",
            "Test set: Average loss: 0.0169, Accuracy: 334/400 (83.50%)\n",
            "\n",
            "83.5\n",
            "Seed Number is 18\n",
            "Num Samples For Training 50 Num Samples For Val 400\n",
            "Train Epoch: 0 [108/300 (90%)]\tLoss: 22.499207\n",
            "\n",
            "Test set: Average loss: 0.0470, Accuracy: 45/400 (11.25%)\n",
            "\n",
            "11.25\n",
            "\n",
            "Test set: Average loss: 0.0259, Accuracy: 207/400 (51.75%)\n",
            "\n",
            "51.75\n",
            "\n",
            "Test set: Average loss: 0.0145, Accuracy: 221/400 (55.25%)\n",
            "\n",
            "55.25\n",
            "\n",
            "Test set: Average loss: 0.0147, Accuracy: 217/400 (54.25%)\n",
            "\n",
            "54.25\n",
            "\n",
            "Test set: Average loss: 0.0153, Accuracy: 233/400 (58.25%)\n",
            "\n",
            "58.25\n",
            "\n",
            "Test set: Average loss: 0.0130, Accuracy: 274/400 (68.50%)\n",
            "\n",
            "68.5\n",
            "\n",
            "Test set: Average loss: 0.0130, Accuracy: 276/400 (69.00%)\n",
            "\n",
            "69.0\n",
            "Train Epoch: 7 [108/300 (90%)]\tLoss: 2.885458\n",
            "\n",
            "Test set: Average loss: 0.0134, Accuracy: 274/400 (68.50%)\n",
            "\n",
            "68.5\n",
            "\n",
            "Test set: Average loss: 0.0143, Accuracy: 273/400 (68.25%)\n",
            "\n",
            "68.25\n",
            "\n",
            "Test set: Average loss: 0.0166, Accuracy: 264/400 (66.00%)\n",
            "\n",
            "66.0\n",
            "\n",
            "Test set: Average loss: 0.0152, Accuracy: 274/400 (68.50%)\n",
            "\n",
            "68.5\n",
            "\n",
            "Test set: Average loss: 0.0208, Accuracy: 259/400 (64.75%)\n",
            "\n",
            "64.75\n",
            "\n",
            "Test set: Average loss: 0.0189, Accuracy: 261/400 (65.25%)\n",
            "\n",
            "65.25\n",
            "\n",
            "Test set: Average loss: 0.0215, Accuracy: 247/400 (61.75%)\n",
            "\n",
            "61.75\n",
            "Train Epoch: 14 [108/300 (90%)]\tLoss: 1.498683\n",
            "\n",
            "Test set: Average loss: 0.0198, Accuracy: 251/400 (62.75%)\n",
            "\n",
            "62.75\n",
            "\n",
            "Test set: Average loss: 0.0187, Accuracy: 249/400 (62.25%)\n",
            "\n",
            "62.25\n",
            "\n",
            "Test set: Average loss: 0.0188, Accuracy: 265/400 (66.25%)\n",
            "\n",
            "66.25\n",
            "\n",
            "Test set: Average loss: 0.0181, Accuracy: 258/400 (64.50%)\n",
            "\n",
            "64.5\n",
            "\n",
            "Test set: Average loss: 0.0181, Accuracy: 268/400 (67.00%)\n",
            "\n",
            "67.0\n",
            "\n",
            "Test set: Average loss: 0.0219, Accuracy: 275/400 (68.75%)\n",
            "\n",
            "68.75\n",
            "\n",
            "Test set: Average loss: 0.0367, Accuracy: 258/400 (64.50%)\n",
            "\n",
            "64.5\n",
            "Train Epoch: 21 [108/300 (90%)]\tLoss: 1.721946\n",
            "\n",
            "Test set: Average loss: 0.0200, Accuracy: 253/400 (63.25%)\n",
            "\n",
            "63.25\n",
            "\n",
            "Test set: Average loss: 0.0186, Accuracy: 267/400 (66.75%)\n",
            "\n",
            "66.75\n",
            "\n",
            "Test set: Average loss: 0.0209, Accuracy: 251/400 (62.75%)\n",
            "\n",
            "62.75\n",
            "\n",
            "Test set: Average loss: 0.0249, Accuracy: 257/400 (64.25%)\n",
            "\n",
            "64.25\n",
            "\n",
            "Test set: Average loss: 0.0230, Accuracy: 258/400 (64.50%)\n",
            "\n",
            "64.5\n",
            "\n",
            "Test set: Average loss: 0.0293, Accuracy: 255/400 (63.75%)\n",
            "\n",
            "63.75\n",
            "\n",
            "Test set: Average loss: 0.0184, Accuracy: 263/400 (65.75%)\n",
            "\n",
            "65.75\n",
            "Train Epoch: 28 [108/300 (90%)]\tLoss: 1.590049\n",
            "\n",
            "Test set: Average loss: 0.0274, Accuracy: 245/400 (61.25%)\n",
            "\n",
            "61.25\n",
            "\n",
            "Test set: Average loss: 0.0188, Accuracy: 273/400 (68.25%)\n",
            "\n",
            "68.25\n",
            "\n",
            "Test set: Average loss: 0.0229, Accuracy: 262/400 (65.50%)\n",
            "\n",
            "65.5\n",
            "\n",
            "Test set: Average loss: 0.0248, Accuracy: 258/400 (64.50%)\n",
            "\n",
            "64.5\n",
            "\n",
            "Test set: Average loss: 0.0210, Accuracy: 272/400 (68.00%)\n",
            "\n",
            "68.0\n",
            "\n",
            "Test set: Average loss: 0.0201, Accuracy: 268/400 (67.00%)\n",
            "\n",
            "67.0\n",
            "\n",
            "Test set: Average loss: 0.0337, Accuracy: 252/400 (63.00%)\n",
            "\n",
            "63.0\n",
            "Train Epoch: 35 [108/300 (90%)]\tLoss: 0.416223\n",
            "\n",
            "Test set: Average loss: 0.0220, Accuracy: 272/400 (68.00%)\n",
            "\n",
            "68.0\n",
            "\n",
            "Test set: Average loss: 0.0331, Accuracy: 258/400 (64.50%)\n",
            "\n",
            "64.5\n",
            "\n",
            "Test set: Average loss: 0.0236, Accuracy: 256/400 (64.00%)\n",
            "\n",
            "64.0\n",
            "\n",
            "Test set: Average loss: 0.0198, Accuracy: 267/400 (66.75%)\n",
            "\n",
            "66.75\n",
            "\n",
            "Test set: Average loss: 0.0184, Accuracy: 263/400 (65.75%)\n",
            "\n",
            "65.75\n",
            "\n",
            "Test set: Average loss: 0.0353, Accuracy: 240/400 (60.00%)\n",
            "\n",
            "60.0\n",
            "\n",
            "Test set: Average loss: 0.0233, Accuracy: 273/400 (68.25%)\n",
            "\n",
            "68.25\n",
            "Train Epoch: 42 [108/300 (90%)]\tLoss: 0.638945\n",
            "\n",
            "Test set: Average loss: 0.0176, Accuracy: 264/400 (66.00%)\n",
            "\n",
            "66.0\n",
            "\n",
            "Test set: Average loss: 0.0192, Accuracy: 274/400 (68.50%)\n",
            "\n",
            "68.5\n",
            "\n",
            "Test set: Average loss: 0.0262, Accuracy: 268/400 (67.00%)\n",
            "\n",
            "67.0\n",
            "\n",
            "Test set: Average loss: 0.0252, Accuracy: 275/400 (68.75%)\n",
            "\n",
            "68.75\n",
            "\n",
            "Test set: Average loss: 0.0224, Accuracy: 278/400 (69.50%)\n",
            "\n",
            "69.5\n",
            "\n",
            "Test set: Average loss: 0.0307, Accuracy: 254/400 (63.50%)\n",
            "\n",
            "63.5\n",
            "\n",
            "Test set: Average loss: 0.0164, Accuracy: 269/400 (67.25%)\n",
            "\n",
            "67.25\n",
            "Train Epoch: 49 [108/300 (90%)]\tLoss: 0.280095\n",
            "\n",
            "Test set: Average loss: 0.0255, Accuracy: 282/400 (70.50%)\n",
            "\n",
            "70.5\n",
            "\n",
            "Test set: Average loss: 0.0326, Accuracy: 282/400 (70.50%)\n",
            "\n",
            "70.5\n",
            "\n",
            "Test set: Average loss: 0.0339, Accuracy: 280/400 (70.00%)\n",
            "\n",
            "70.0\n",
            "\n",
            "Test set: Average loss: 0.0266, Accuracy: 272/400 (68.00%)\n",
            "\n",
            "68.0\n",
            "\n",
            "Test set: Average loss: 0.0220, Accuracy: 282/400 (70.50%)\n",
            "\n",
            "70.5\n",
            "\n",
            "Test set: Average loss: 0.0287, Accuracy: 278/400 (69.50%)\n",
            "\n",
            "69.5\n",
            "\n",
            "Test set: Average loss: 0.0284, Accuracy: 265/400 (66.25%)\n",
            "\n",
            "66.25\n",
            "Train Epoch: 56 [108/300 (90%)]\tLoss: 0.882373\n",
            "\n",
            "Test set: Average loss: 0.0257, Accuracy: 273/400 (68.25%)\n",
            "\n",
            "68.25\n",
            "\n",
            "Test set: Average loss: 0.0343, Accuracy: 273/400 (68.25%)\n",
            "\n",
            "68.25\n",
            "\n",
            "Test set: Average loss: 0.0342, Accuracy: 262/400 (65.50%)\n",
            "\n",
            "65.5\n",
            "\n",
            "Test set: Average loss: 0.0277, Accuracy: 261/400 (65.25%)\n",
            "\n",
            "65.25\n",
            "\n",
            "Test set: Average loss: 0.0288, Accuracy: 261/400 (65.25%)\n",
            "\n",
            "65.25\n",
            "\n",
            "Test set: Average loss: 0.0244, Accuracy: 269/400 (67.25%)\n",
            "\n",
            "67.25\n",
            "\n",
            "Test set: Average loss: 0.0266, Accuracy: 274/400 (68.50%)\n",
            "\n",
            "68.5\n",
            "Train Epoch: 63 [108/300 (90%)]\tLoss: 0.345631\n",
            "\n",
            "Test set: Average loss: 0.0256, Accuracy: 275/400 (68.75%)\n",
            "\n",
            "68.75\n",
            "\n",
            "Test set: Average loss: 0.0261, Accuracy: 277/400 (69.25%)\n",
            "\n",
            "69.25\n",
            "\n",
            "Test set: Average loss: 0.0287, Accuracy: 269/400 (67.25%)\n",
            "\n",
            "67.25\n",
            "\n",
            "Test set: Average loss: 0.0313, Accuracy: 257/400 (64.25%)\n",
            "\n",
            "64.25\n",
            "\n",
            "Test set: Average loss: 0.0312, Accuracy: 270/400 (67.50%)\n",
            "\n",
            "67.5\n",
            "\n",
            "Test set: Average loss: 0.0305, Accuracy: 276/400 (69.00%)\n",
            "\n",
            "69.0\n",
            "\n",
            "Test set: Average loss: 0.0283, Accuracy: 261/400 (65.25%)\n",
            "\n",
            "65.25\n",
            "Train Epoch: 70 [108/300 (90%)]\tLoss: 0.342679\n",
            "\n",
            "Test set: Average loss: 0.0253, Accuracy: 252/400 (63.00%)\n",
            "\n",
            "63.0\n",
            "\n",
            "Test set: Average loss: 0.0225, Accuracy: 273/400 (68.25%)\n",
            "\n",
            "68.25\n",
            "\n",
            "Test set: Average loss: 0.0256, Accuracy: 271/400 (67.75%)\n",
            "\n",
            "67.75\n",
            "\n",
            "Test set: Average loss: 0.0261, Accuracy: 279/400 (69.75%)\n",
            "\n",
            "69.75\n",
            "\n",
            "Test set: Average loss: 0.0243, Accuracy: 273/400 (68.25%)\n",
            "\n",
            "68.25\n",
            "\n",
            "Test set: Average loss: 0.0255, Accuracy: 273/400 (68.25%)\n",
            "\n",
            "68.25\n",
            "\n",
            "Test set: Average loss: 0.0270, Accuracy: 264/400 (66.00%)\n",
            "\n",
            "66.0\n",
            "Train Epoch: 77 [108/300 (90%)]\tLoss: 0.410567\n",
            "\n",
            "Test set: Average loss: 0.0285, Accuracy: 277/400 (69.25%)\n",
            "\n",
            "69.25\n",
            "\n",
            "Test set: Average loss: 0.0302, Accuracy: 268/400 (67.00%)\n",
            "\n",
            "67.0\n",
            "\n",
            "Test set: Average loss: 0.0301, Accuracy: 269/400 (67.25%)\n",
            "\n",
            "67.25\n",
            "\n",
            "Test set: Average loss: 0.0337, Accuracy: 278/400 (69.50%)\n",
            "\n",
            "69.5\n",
            "\n",
            "Test set: Average loss: 0.0306, Accuracy: 276/400 (69.00%)\n",
            "\n",
            "69.0\n",
            "\n",
            "Test set: Average loss: 0.0285, Accuracy: 272/400 (68.00%)\n",
            "\n",
            "68.0\n",
            "\n",
            "Test set: Average loss: 0.0332, Accuracy: 264/400 (66.00%)\n",
            "\n",
            "66.0\n",
            "Train Epoch: 84 [108/300 (90%)]\tLoss: 0.018582\n",
            "\n",
            "Test set: Average loss: 0.0312, Accuracy: 273/400 (68.25%)\n",
            "\n",
            "68.25\n",
            "\n",
            "Test set: Average loss: 0.0324, Accuracy: 274/400 (68.50%)\n",
            "\n",
            "68.5\n",
            "\n",
            "Test set: Average loss: 0.0319, Accuracy: 283/400 (70.75%)\n",
            "\n",
            "70.75\n",
            "\n",
            "Test set: Average loss: 0.0342, Accuracy: 279/400 (69.75%)\n",
            "\n",
            "69.75\n",
            "\n",
            "Test set: Average loss: 0.0334, Accuracy: 281/400 (70.25%)\n",
            "\n",
            "70.25\n",
            "\n",
            "Test set: Average loss: 0.0333, Accuracy: 279/400 (69.75%)\n",
            "\n",
            "69.75\n",
            "\n",
            "Test set: Average loss: 0.0333, Accuracy: 278/400 (69.50%)\n",
            "\n",
            "69.5\n",
            "Train Epoch: 91 [108/300 (90%)]\tLoss: 0.008453\n",
            "\n",
            "Test set: Average loss: 0.0342, Accuracy: 280/400 (70.00%)\n",
            "\n",
            "70.0\n",
            "\n",
            "Test set: Average loss: 0.0347, Accuracy: 276/400 (69.00%)\n",
            "\n",
            "69.0\n",
            "\n",
            "Test set: Average loss: 0.0335, Accuracy: 281/400 (70.25%)\n",
            "\n",
            "70.25\n",
            "\n",
            "Test set: Average loss: 0.0333, Accuracy: 279/400 (69.75%)\n",
            "\n",
            "69.75\n",
            "\n",
            "Test set: Average loss: 0.0335, Accuracy: 279/400 (69.75%)\n",
            "\n",
            "69.75\n",
            "\n",
            "Test set: Average loss: 0.0336, Accuracy: 280/400 (70.00%)\n",
            "\n",
            "70.0\n",
            "\n",
            "Test set: Average loss: 0.0342, Accuracy: 278/400 (69.50%)\n",
            "\n",
            "69.5\n",
            "Train Epoch: 98 [108/300 (90%)]\tLoss: 0.013871\n",
            "\n",
            "Test set: Average loss: 0.0344, Accuracy: 279/400 (69.75%)\n",
            "\n",
            "69.75\n",
            "\n",
            "Test set: Average loss: 0.0344, Accuracy: 278/400 (69.50%)\n",
            "\n",
            "69.5\n",
            "\n",
            "Test set: Average loss: 0.0347, Accuracy: 282/400 (70.50%)\n",
            "\n",
            "70.5\n",
            "\n",
            "Test set: Average loss: 0.0352, Accuracy: 276/400 (69.00%)\n",
            "\n",
            "69.0\n",
            "\n",
            "Test set: Average loss: 0.0351, Accuracy: 279/400 (69.75%)\n",
            "\n",
            "69.75\n",
            "\n",
            "Test set: Average loss: 0.0349, Accuracy: 276/400 (69.00%)\n",
            "\n",
            "69.0\n",
            "\n",
            "Test set: Average loss: 0.0347, Accuracy: 278/400 (69.50%)\n",
            "\n",
            "69.5\n",
            "Train Epoch: 105 [108/300 (90%)]\tLoss: 0.053858\n",
            "\n",
            "Test set: Average loss: 0.0344, Accuracy: 277/400 (69.25%)\n",
            "\n",
            "69.25\n",
            "\n",
            "Test set: Average loss: 0.0343, Accuracy: 276/400 (69.00%)\n",
            "\n",
            "69.0\n",
            "\n",
            "Test set: Average loss: 0.0340, Accuracy: 277/400 (69.25%)\n",
            "\n",
            "69.25\n",
            "\n",
            "Test set: Average loss: 0.0331, Accuracy: 280/400 (70.00%)\n",
            "\n",
            "70.0\n",
            "\n",
            "Test set: Average loss: 0.0335, Accuracy: 279/400 (69.75%)\n",
            "\n",
            "69.75\n",
            "\n",
            "Test set: Average loss: 0.0331, Accuracy: 277/400 (69.25%)\n",
            "\n",
            "69.25\n",
            "\n",
            "Test set: Average loss: 0.0330, Accuracy: 280/400 (70.00%)\n",
            "\n",
            "70.0\n",
            "Train Epoch: 112 [108/300 (90%)]\tLoss: 0.257761\n",
            "\n",
            "Test set: Average loss: 0.0330, Accuracy: 277/400 (69.25%)\n",
            "\n",
            "69.25\n",
            "\n",
            "Test set: Average loss: 0.0327, Accuracy: 279/400 (69.75%)\n",
            "\n",
            "69.75\n",
            "\n",
            "Test set: Average loss: 0.0327, Accuracy: 280/400 (70.00%)\n",
            "\n",
            "70.0\n",
            "\n",
            "Test set: Average loss: 0.0327, Accuracy: 279/400 (69.75%)\n",
            "\n",
            "69.75\n",
            "\n",
            "Test set: Average loss: 0.0329, Accuracy: 280/400 (70.00%)\n",
            "\n",
            "70.0\n",
            "\n",
            "Test set: Average loss: 0.0330, Accuracy: 280/400 (70.00%)\n",
            "\n",
            "70.0\n",
            "\n",
            "Test set: Average loss: 0.0334, Accuracy: 280/400 (70.00%)\n",
            "\n",
            "70.0\n",
            "Train Epoch: 119 [108/300 (90%)]\tLoss: 0.041955\n",
            "\n",
            "Test set: Average loss: 0.0332, Accuracy: 280/400 (70.00%)\n",
            "\n",
            "70.0\n",
            "Seed Number is 19\n",
            "Num Samples For Training 50 Num Samples For Val 400\n",
            "Train Epoch: 0 [108/300 (90%)]\tLoss: 17.706030\n",
            "\n",
            "Test set: Average loss: 0.0418, Accuracy: 62/400 (15.50%)\n",
            "\n",
            "15.5\n",
            "\n",
            "Test set: Average loss: 0.0195, Accuracy: 233/400 (58.25%)\n",
            "\n",
            "58.25\n",
            "\n",
            "Test set: Average loss: 0.0132, Accuracy: 246/400 (61.50%)\n",
            "\n",
            "61.5\n",
            "\n",
            "Test set: Average loss: 0.0139, Accuracy: 230/400 (57.50%)\n",
            "\n",
            "57.5\n",
            "\n",
            "Test set: Average loss: 0.0135, Accuracy: 231/400 (57.75%)\n",
            "\n",
            "57.75\n",
            "\n",
            "Test set: Average loss: 0.0149, Accuracy: 218/400 (54.50%)\n",
            "\n",
            "54.5\n",
            "\n",
            "Test set: Average loss: 0.0173, Accuracy: 221/400 (55.25%)\n",
            "\n",
            "55.25\n",
            "Train Epoch: 7 [108/300 (90%)]\tLoss: 3.511911\n",
            "\n",
            "Test set: Average loss: 0.0115, Accuracy: 282/400 (70.50%)\n",
            "\n",
            "70.5\n",
            "\n",
            "Test set: Average loss: 0.0141, Accuracy: 249/400 (62.25%)\n",
            "\n",
            "62.25\n",
            "\n",
            "Test set: Average loss: 0.0133, Accuracy: 287/400 (71.75%)\n",
            "\n",
            "71.75\n",
            "\n",
            "Test set: Average loss: 0.0123, Accuracy: 296/400 (74.00%)\n",
            "\n",
            "74.0\n",
            "\n",
            "Test set: Average loss: 0.0130, Accuracy: 272/400 (68.00%)\n",
            "\n",
            "68.0\n",
            "\n",
            "Test set: Average loss: 0.0150, Accuracy: 277/400 (69.25%)\n",
            "\n",
            "69.25\n",
            "\n",
            "Test set: Average loss: 0.0141, Accuracy: 275/400 (68.75%)\n",
            "\n",
            "68.75\n",
            "Train Epoch: 14 [108/300 (90%)]\tLoss: 1.328159\n",
            "\n",
            "Test set: Average loss: 0.0128, Accuracy: 290/400 (72.50%)\n",
            "\n",
            "72.5\n",
            "\n",
            "Test set: Average loss: 0.0168, Accuracy: 274/400 (68.50%)\n",
            "\n",
            "68.5\n",
            "\n",
            "Test set: Average loss: 0.0177, Accuracy: 264/400 (66.00%)\n",
            "\n",
            "66.0\n",
            "\n",
            "Test set: Average loss: 0.0142, Accuracy: 282/400 (70.50%)\n",
            "\n",
            "70.5\n",
            "\n",
            "Test set: Average loss: 0.0173, Accuracy: 268/400 (67.00%)\n",
            "\n",
            "67.0\n",
            "\n",
            "Test set: Average loss: 0.0216, Accuracy: 276/400 (69.00%)\n",
            "\n",
            "69.0\n",
            "\n",
            "Test set: Average loss: 0.0168, Accuracy: 291/400 (72.75%)\n",
            "\n",
            "72.75\n",
            "Train Epoch: 21 [108/300 (90%)]\tLoss: 1.515176\n",
            "\n",
            "Test set: Average loss: 0.0197, Accuracy: 254/400 (63.50%)\n",
            "\n",
            "63.5\n",
            "\n",
            "Test set: Average loss: 0.0156, Accuracy: 279/400 (69.75%)\n",
            "\n",
            "69.75\n",
            "\n",
            "Test set: Average loss: 0.0301, Accuracy: 258/400 (64.50%)\n",
            "\n",
            "64.5\n",
            "\n",
            "Test set: Average loss: 0.0275, Accuracy: 253/400 (63.25%)\n",
            "\n",
            "63.25\n",
            "\n",
            "Test set: Average loss: 0.0152, Accuracy: 271/400 (67.75%)\n",
            "\n",
            "67.75\n",
            "\n",
            "Test set: Average loss: 0.0152, Accuracy: 283/400 (70.75%)\n",
            "\n",
            "70.75\n",
            "\n",
            "Test set: Average loss: 0.0275, Accuracy: 247/400 (61.75%)\n",
            "\n",
            "61.75\n",
            "Train Epoch: 28 [108/300 (90%)]\tLoss: 0.704969\n",
            "\n",
            "Test set: Average loss: 0.0204, Accuracy: 271/400 (67.75%)\n",
            "\n",
            "67.75\n",
            "\n",
            "Test set: Average loss: 0.0188, Accuracy: 268/400 (67.00%)\n",
            "\n",
            "67.0\n",
            "\n",
            "Test set: Average loss: 0.0181, Accuracy: 265/400 (66.25%)\n",
            "\n",
            "66.25\n",
            "\n",
            "Test set: Average loss: 0.0214, Accuracy: 254/400 (63.50%)\n",
            "\n",
            "63.5\n",
            "\n",
            "Test set: Average loss: 0.0178, Accuracy: 273/400 (68.25%)\n",
            "\n",
            "68.25\n",
            "\n",
            "Test set: Average loss: 0.0206, Accuracy: 279/400 (69.75%)\n",
            "\n",
            "69.75\n",
            "\n",
            "Test set: Average loss: 0.0178, Accuracy: 279/400 (69.75%)\n",
            "\n",
            "69.75\n",
            "Train Epoch: 35 [108/300 (90%)]\tLoss: 1.112240\n",
            "\n",
            "Test set: Average loss: 0.0175, Accuracy: 276/400 (69.00%)\n",
            "\n",
            "69.0\n",
            "\n",
            "Test set: Average loss: 0.0236, Accuracy: 272/400 (68.00%)\n",
            "\n",
            "68.0\n",
            "\n",
            "Test set: Average loss: 0.0207, Accuracy: 274/400 (68.50%)\n",
            "\n",
            "68.5\n",
            "\n",
            "Test set: Average loss: 0.0158, Accuracy: 288/400 (72.00%)\n",
            "\n",
            "72.0\n",
            "\n",
            "Test set: Average loss: 0.0220, Accuracy: 271/400 (67.75%)\n",
            "\n",
            "67.75\n",
            "\n",
            "Test set: Average loss: 0.0226, Accuracy: 267/400 (66.75%)\n",
            "\n",
            "66.75\n",
            "\n",
            "Test set: Average loss: 0.0294, Accuracy: 239/400 (59.75%)\n",
            "\n",
            "59.75\n",
            "Train Epoch: 42 [108/300 (90%)]\tLoss: 0.523825\n",
            "\n",
            "Test set: Average loss: 0.0284, Accuracy: 263/400 (65.75%)\n",
            "\n",
            "65.75\n",
            "\n",
            "Test set: Average loss: 0.0178, Accuracy: 289/400 (72.25%)\n",
            "\n",
            "72.25\n",
            "\n",
            "Test set: Average loss: 0.0230, Accuracy: 251/400 (62.75%)\n",
            "\n",
            "62.75\n",
            "\n",
            "Test set: Average loss: 0.0201, Accuracy: 285/400 (71.25%)\n",
            "\n",
            "71.25\n",
            "\n",
            "Test set: Average loss: 0.0295, Accuracy: 264/400 (66.00%)\n",
            "\n",
            "66.0\n",
            "\n",
            "Test set: Average loss: 0.0297, Accuracy: 275/400 (68.75%)\n",
            "\n",
            "68.75\n",
            "\n",
            "Test set: Average loss: 0.0166, Accuracy: 285/400 (71.25%)\n",
            "\n",
            "71.25\n",
            "Train Epoch: 49 [108/300 (90%)]\tLoss: 3.743154\n",
            "\n",
            "Test set: Average loss: 0.0230, Accuracy: 275/400 (68.75%)\n",
            "\n",
            "68.75\n",
            "\n",
            "Test set: Average loss: 0.0156, Accuracy: 273/400 (68.25%)\n",
            "\n",
            "68.25\n",
            "\n",
            "Test set: Average loss: 0.0215, Accuracy: 281/400 (70.25%)\n",
            "\n",
            "70.25\n",
            "\n",
            "Test set: Average loss: 0.0338, Accuracy: 256/400 (64.00%)\n",
            "\n",
            "64.0\n",
            "\n",
            "Test set: Average loss: 0.0267, Accuracy: 268/400 (67.00%)\n",
            "\n",
            "67.0\n",
            "\n",
            "Test set: Average loss: 0.0233, Accuracy: 269/400 (67.25%)\n",
            "\n",
            "67.25\n",
            "\n",
            "Test set: Average loss: 0.0260, Accuracy: 268/400 (67.00%)\n",
            "\n",
            "67.0\n",
            "Train Epoch: 56 [108/300 (90%)]\tLoss: 0.226636\n",
            "\n",
            "Test set: Average loss: 0.0323, Accuracy: 263/400 (65.75%)\n",
            "\n",
            "65.75\n",
            "\n",
            "Test set: Average loss: 0.0261, Accuracy: 279/400 (69.75%)\n",
            "\n",
            "69.75\n",
            "\n",
            "Test set: Average loss: 0.0355, Accuracy: 264/400 (66.00%)\n",
            "\n",
            "66.0\n",
            "\n",
            "Test set: Average loss: 0.0250, Accuracy: 282/400 (70.50%)\n",
            "\n",
            "70.5\n",
            "\n",
            "Test set: Average loss: 0.0262, Accuracy: 266/400 (66.50%)\n",
            "\n",
            "66.5\n",
            "\n",
            "Test set: Average loss: 0.0213, Accuracy: 282/400 (70.50%)\n",
            "\n",
            "70.5\n",
            "\n",
            "Test set: Average loss: 0.0231, Accuracy: 271/400 (67.75%)\n",
            "\n",
            "67.75\n",
            "Train Epoch: 63 [108/300 (90%)]\tLoss: 0.282226\n",
            "\n",
            "Test set: Average loss: 0.0216, Accuracy: 258/400 (64.50%)\n",
            "\n",
            "64.5\n",
            "\n",
            "Test set: Average loss: 0.0292, Accuracy: 264/400 (66.00%)\n",
            "\n",
            "66.0\n",
            "\n",
            "Test set: Average loss: 0.0289, Accuracy: 272/400 (68.00%)\n",
            "\n",
            "68.0\n",
            "\n",
            "Test set: Average loss: 0.0261, Accuracy: 265/400 (66.25%)\n",
            "\n",
            "66.25\n",
            "\n",
            "Test set: Average loss: 0.0243, Accuracy: 274/400 (68.50%)\n",
            "\n",
            "68.5\n",
            "\n",
            "Test set: Average loss: 0.0219, Accuracy: 268/400 (67.00%)\n",
            "\n",
            "67.0\n",
            "\n",
            "Test set: Average loss: 0.0331, Accuracy: 249/400 (62.25%)\n",
            "\n",
            "62.25\n",
            "Train Epoch: 70 [108/300 (90%)]\tLoss: 1.430844\n",
            "\n",
            "Test set: Average loss: 0.0248, Accuracy: 268/400 (67.00%)\n",
            "\n",
            "67.0\n",
            "\n",
            "Test set: Average loss: 0.0251, Accuracy: 272/400 (68.00%)\n",
            "\n",
            "68.0\n",
            "\n",
            "Test set: Average loss: 0.0299, Accuracy: 276/400 (69.00%)\n",
            "\n",
            "69.0\n",
            "\n",
            "Test set: Average loss: 0.0301, Accuracy: 269/400 (67.25%)\n",
            "\n",
            "67.25\n",
            "\n",
            "Test set: Average loss: 0.0245, Accuracy: 264/400 (66.00%)\n",
            "\n",
            "66.0\n",
            "\n",
            "Test set: Average loss: 0.0231, Accuracy: 266/400 (66.50%)\n",
            "\n",
            "66.5\n",
            "\n",
            "Test set: Average loss: 0.0273, Accuracy: 273/400 (68.25%)\n",
            "\n",
            "68.25\n",
            "Train Epoch: 77 [108/300 (90%)]\tLoss: 0.311752\n",
            "\n",
            "Test set: Average loss: 0.0267, Accuracy: 286/400 (71.50%)\n",
            "\n",
            "71.5\n",
            "\n",
            "Test set: Average loss: 0.0270, Accuracy: 282/400 (70.50%)\n",
            "\n",
            "70.5\n",
            "\n",
            "Test set: Average loss: 0.0282, Accuracy: 286/400 (71.50%)\n",
            "\n",
            "71.5\n",
            "\n",
            "Test set: Average loss: 0.0284, Accuracy: 283/400 (70.75%)\n",
            "\n",
            "70.75\n",
            "\n",
            "Test set: Average loss: 0.0280, Accuracy: 269/400 (67.25%)\n",
            "\n",
            "67.25\n",
            "\n",
            "Test set: Average loss: 0.0256, Accuracy: 275/400 (68.75%)\n",
            "\n",
            "68.75\n",
            "\n",
            "Test set: Average loss: 0.0259, Accuracy: 274/400 (68.50%)\n",
            "\n",
            "68.5\n",
            "Train Epoch: 84 [108/300 (90%)]\tLoss: 0.209459\n",
            "\n",
            "Test set: Average loss: 0.0242, Accuracy: 278/400 (69.50%)\n",
            "\n",
            "69.5\n",
            "\n",
            "Test set: Average loss: 0.0250, Accuracy: 278/400 (69.50%)\n",
            "\n",
            "69.5\n",
            "\n",
            "Test set: Average loss: 0.0246, Accuracy: 280/400 (70.00%)\n",
            "\n",
            "70.0\n",
            "\n",
            "Test set: Average loss: 0.0244, Accuracy: 278/400 (69.50%)\n",
            "\n",
            "69.5\n",
            "\n",
            "Test set: Average loss: 0.0256, Accuracy: 276/400 (69.00%)\n",
            "\n",
            "69.0\n",
            "\n",
            "Test set: Average loss: 0.0263, Accuracy: 276/400 (69.00%)\n",
            "\n",
            "69.0\n",
            "\n",
            "Test set: Average loss: 0.0275, Accuracy: 275/400 (68.75%)\n",
            "\n",
            "68.75\n",
            "Train Epoch: 91 [108/300 (90%)]\tLoss: 0.496956\n",
            "\n",
            "Test set: Average loss: 0.0288, Accuracy: 277/400 (69.25%)\n",
            "\n",
            "69.25\n",
            "\n",
            "Test set: Average loss: 0.0290, Accuracy: 280/400 (70.00%)\n",
            "\n",
            "70.0\n",
            "\n",
            "Test set: Average loss: 0.0285, Accuracy: 282/400 (70.50%)\n",
            "\n",
            "70.5\n",
            "\n",
            "Test set: Average loss: 0.0285, Accuracy: 279/400 (69.75%)\n",
            "\n",
            "69.75\n",
            "\n",
            "Test set: Average loss: 0.0290, Accuracy: 280/400 (70.00%)\n",
            "\n",
            "70.0\n",
            "\n",
            "Test set: Average loss: 0.0285, Accuracy: 280/400 (70.00%)\n",
            "\n",
            "70.0\n",
            "\n",
            "Test set: Average loss: 0.0289, Accuracy: 283/400 (70.75%)\n",
            "\n",
            "70.75\n",
            "Train Epoch: 98 [108/300 (90%)]\tLoss: 0.165229\n",
            "\n",
            "Test set: Average loss: 0.0291, Accuracy: 282/400 (70.50%)\n",
            "\n",
            "70.5\n",
            "\n",
            "Test set: Average loss: 0.0295, Accuracy: 285/400 (71.25%)\n",
            "\n",
            "71.25\n",
            "\n",
            "Test set: Average loss: 0.0296, Accuracy: 283/400 (70.75%)\n",
            "\n",
            "70.75\n",
            "\n",
            "Test set: Average loss: 0.0296, Accuracy: 282/400 (70.50%)\n",
            "\n",
            "70.5\n",
            "\n",
            "Test set: Average loss: 0.0287, Accuracy: 282/400 (70.50%)\n",
            "\n",
            "70.5\n",
            "\n",
            "Test set: Average loss: 0.0287, Accuracy: 283/400 (70.75%)\n",
            "\n",
            "70.75\n",
            "\n",
            "Test set: Average loss: 0.0292, Accuracy: 286/400 (71.50%)\n",
            "\n",
            "71.5\n",
            "Train Epoch: 105 [108/300 (90%)]\tLoss: 0.022488\n",
            "\n",
            "Test set: Average loss: 0.0289, Accuracy: 285/400 (71.25%)\n",
            "\n",
            "71.25\n",
            "\n",
            "Test set: Average loss: 0.0292, Accuracy: 284/400 (71.00%)\n",
            "\n",
            "71.0\n",
            "\n",
            "Test set: Average loss: 0.0294, Accuracy: 281/400 (70.25%)\n",
            "\n",
            "70.25\n",
            "\n",
            "Test set: Average loss: 0.0294, Accuracy: 284/400 (71.00%)\n",
            "\n",
            "71.0\n",
            "\n",
            "Test set: Average loss: 0.0291, Accuracy: 281/400 (70.25%)\n",
            "\n",
            "70.25\n",
            "\n",
            "Test set: Average loss: 0.0294, Accuracy: 281/400 (70.25%)\n",
            "\n",
            "70.25\n",
            "\n",
            "Test set: Average loss: 0.0294, Accuracy: 281/400 (70.25%)\n",
            "\n",
            "70.25\n",
            "Train Epoch: 112 [108/300 (90%)]\tLoss: 0.189426\n",
            "\n",
            "Test set: Average loss: 0.0298, Accuracy: 282/400 (70.50%)\n",
            "\n",
            "70.5\n",
            "\n",
            "Test set: Average loss: 0.0297, Accuracy: 278/400 (69.50%)\n",
            "\n",
            "69.5\n",
            "\n",
            "Test set: Average loss: 0.0299, Accuracy: 281/400 (70.25%)\n",
            "\n",
            "70.25\n",
            "\n",
            "Test set: Average loss: 0.0301, Accuracy: 281/400 (70.25%)\n",
            "\n",
            "70.25\n",
            "\n",
            "Test set: Average loss: 0.0305, Accuracy: 282/400 (70.50%)\n",
            "\n",
            "70.5\n",
            "\n",
            "Test set: Average loss: 0.0304, Accuracy: 281/400 (70.25%)\n",
            "\n",
            "70.25\n",
            "\n",
            "Test set: Average loss: 0.0301, Accuracy: 280/400 (70.00%)\n",
            "\n",
            "70.0\n",
            "Train Epoch: 119 [108/300 (90%)]\tLoss: 0.138019\n",
            "\n",
            "Test set: Average loss: 0.0303, Accuracy: 281/400 (70.25%)\n",
            "\n",
            "70.25\n",
            "Seed Number is 20\n",
            "Num Samples For Training 50 Num Samples For Val 400\n",
            "Train Epoch: 0 [108/300 (90%)]\tLoss: 19.107821\n",
            "\n",
            "Test set: Average loss: 0.0410, Accuracy: 97/400 (24.25%)\n",
            "\n",
            "24.25\n",
            "\n",
            "Test set: Average loss: 0.0208, Accuracy: 152/400 (38.00%)\n",
            "\n",
            "38.0\n",
            "\n",
            "Test set: Average loss: 0.0122, Accuracy: 291/400 (72.75%)\n",
            "\n",
            "72.75\n",
            "\n",
            "Test set: Average loss: 0.0089, Accuracy: 310/400 (77.50%)\n",
            "\n",
            "77.5\n",
            "\n",
            "Test set: Average loss: 0.0065, Accuracy: 357/400 (89.25%)\n",
            "\n",
            "89.25\n",
            "\n",
            "Test set: Average loss: 0.0101, Accuracy: 309/400 (77.25%)\n",
            "\n",
            "77.25\n",
            "\n",
            "Test set: Average loss: 0.0060, Accuracy: 362/400 (90.50%)\n",
            "\n",
            "90.5\n",
            "Train Epoch: 7 [108/300 (90%)]\tLoss: 2.233611\n",
            "\n",
            "Test set: Average loss: 0.0106, Accuracy: 309/400 (77.25%)\n",
            "\n",
            "77.25\n",
            "\n",
            "Test set: Average loss: 0.0074, Accuracy: 352/400 (88.00%)\n",
            "\n",
            "88.0\n",
            "\n",
            "Test set: Average loss: 0.0066, Accuracy: 346/400 (86.50%)\n",
            "\n",
            "86.5\n",
            "\n",
            "Test set: Average loss: 0.0060, Accuracy: 356/400 (89.00%)\n",
            "\n",
            "89.0\n",
            "\n",
            "Test set: Average loss: 0.0117, Accuracy: 316/400 (79.00%)\n",
            "\n",
            "79.0\n",
            "\n",
            "Test set: Average loss: 0.0054, Accuracy: 362/400 (90.50%)\n",
            "\n",
            "90.5\n",
            "\n",
            "Test set: Average loss: 0.0102, Accuracy: 323/400 (80.75%)\n",
            "\n",
            "80.75\n",
            "Train Epoch: 14 [108/300 (90%)]\tLoss: 0.805565\n",
            "\n",
            "Test set: Average loss: 0.0054, Accuracy: 363/400 (90.75%)\n",
            "\n",
            "90.75\n",
            "\n",
            "Test set: Average loss: 0.0089, Accuracy: 351/400 (87.75%)\n",
            "\n",
            "87.75\n",
            "\n",
            "Test set: Average loss: 0.0076, Accuracy: 344/400 (86.00%)\n",
            "\n",
            "86.0\n",
            "\n",
            "Test set: Average loss: 0.0067, Accuracy: 350/400 (87.50%)\n",
            "\n",
            "87.5\n",
            "\n",
            "Test set: Average loss: 0.0055, Accuracy: 352/400 (88.00%)\n",
            "\n",
            "88.0\n",
            "\n",
            "Test set: Average loss: 0.0067, Accuracy: 352/400 (88.00%)\n",
            "\n",
            "88.0\n",
            "\n",
            "Test set: Average loss: 0.0054, Accuracy: 362/400 (90.50%)\n",
            "\n",
            "90.5\n",
            "Train Epoch: 21 [108/300 (90%)]\tLoss: 0.574787\n",
            "\n",
            "Test set: Average loss: 0.0082, Accuracy: 340/400 (85.00%)\n",
            "\n",
            "85.0\n",
            "\n",
            "Test set: Average loss: 0.0064, Accuracy: 363/400 (90.75%)\n",
            "\n",
            "90.75\n",
            "\n",
            "Test set: Average loss: 0.0064, Accuracy: 363/400 (90.75%)\n",
            "\n",
            "90.75\n",
            "\n",
            "Test set: Average loss: 0.0071, Accuracy: 350/400 (87.50%)\n",
            "\n",
            "87.5\n",
            "\n",
            "Test set: Average loss: 0.0067, Accuracy: 359/400 (89.75%)\n",
            "\n",
            "89.75\n",
            "\n",
            "Test set: Average loss: 0.0109, Accuracy: 319/400 (79.75%)\n",
            "\n",
            "79.75\n",
            "\n",
            "Test set: Average loss: 0.0115, Accuracy: 338/400 (84.50%)\n",
            "\n",
            "84.5\n",
            "Train Epoch: 28 [108/300 (90%)]\tLoss: 1.474486\n",
            "\n",
            "Test set: Average loss: 0.0070, Accuracy: 354/400 (88.50%)\n",
            "\n",
            "88.5\n",
            "\n",
            "Test set: Average loss: 0.0108, Accuracy: 335/400 (83.75%)\n",
            "\n",
            "83.75\n",
            "\n",
            "Test set: Average loss: 0.0057, Accuracy: 363/400 (90.75%)\n",
            "\n",
            "90.75\n",
            "\n",
            "Test set: Average loss: 0.0106, Accuracy: 334/400 (83.50%)\n",
            "\n",
            "83.5\n",
            "\n",
            "Test set: Average loss: 0.0088, Accuracy: 349/400 (87.25%)\n",
            "\n",
            "87.25\n",
            "\n",
            "Test set: Average loss: 0.0085, Accuracy: 345/400 (86.25%)\n",
            "\n",
            "86.25\n",
            "\n",
            "Test set: Average loss: 0.0073, Accuracy: 364/400 (91.00%)\n",
            "\n",
            "91.0\n",
            "Train Epoch: 35 [108/300 (90%)]\tLoss: 1.922693\n",
            "\n",
            "Test set: Average loss: 0.0113, Accuracy: 350/400 (87.50%)\n",
            "\n",
            "87.5\n",
            "\n",
            "Test set: Average loss: 0.0074, Accuracy: 363/400 (90.75%)\n",
            "\n",
            "90.75\n",
            "\n",
            "Test set: Average loss: 0.0064, Accuracy: 355/400 (88.75%)\n",
            "\n",
            "88.75\n",
            "\n",
            "Test set: Average loss: 0.0059, Accuracy: 360/400 (90.00%)\n",
            "\n",
            "90.0\n",
            "\n",
            "Test set: Average loss: 0.0066, Accuracy: 358/400 (89.50%)\n",
            "\n",
            "89.5\n",
            "\n",
            "Test set: Average loss: 0.0099, Accuracy: 341/400 (85.25%)\n",
            "\n",
            "85.25\n",
            "\n",
            "Test set: Average loss: 0.0107, Accuracy: 334/400 (83.50%)\n",
            "\n",
            "83.5\n",
            "Train Epoch: 42 [108/300 (90%)]\tLoss: 2.142739\n",
            "\n",
            "Test set: Average loss: 0.0104, Accuracy: 346/400 (86.50%)\n",
            "\n",
            "86.5\n",
            "\n",
            "Test set: Average loss: 0.0064, Accuracy: 360/400 (90.00%)\n",
            "\n",
            "90.0\n",
            "\n",
            "Test set: Average loss: 0.0110, Accuracy: 338/400 (84.50%)\n",
            "\n",
            "84.5\n",
            "\n",
            "Test set: Average loss: 0.0077, Accuracy: 361/400 (90.25%)\n",
            "\n",
            "90.25\n",
            "\n",
            "Test set: Average loss: 0.0105, Accuracy: 359/400 (89.75%)\n",
            "\n",
            "89.75\n",
            "\n",
            "Test set: Average loss: 0.0096, Accuracy: 350/400 (87.50%)\n",
            "\n",
            "87.5\n",
            "\n",
            "Test set: Average loss: 0.0102, Accuracy: 360/400 (90.00%)\n",
            "\n",
            "90.0\n",
            "Train Epoch: 49 [108/300 (90%)]\tLoss: 0.832571\n",
            "\n",
            "Test set: Average loss: 0.0092, Accuracy: 347/400 (86.75%)\n",
            "\n",
            "86.75\n",
            "\n",
            "Test set: Average loss: 0.0085, Accuracy: 355/400 (88.75%)\n",
            "\n",
            "88.75\n",
            "\n",
            "Test set: Average loss: 0.0075, Accuracy: 356/400 (89.00%)\n",
            "\n",
            "89.0\n",
            "\n",
            "Test set: Average loss: 0.0107, Accuracy: 350/400 (87.50%)\n",
            "\n",
            "87.5\n",
            "\n",
            "Test set: Average loss: 0.0120, Accuracy: 362/400 (90.50%)\n",
            "\n",
            "90.5\n",
            "\n",
            "Test set: Average loss: 0.0074, Accuracy: 363/400 (90.75%)\n",
            "\n",
            "90.75\n",
            "\n",
            "Test set: Average loss: 0.0048, Accuracy: 370/400 (92.50%)\n",
            "\n",
            "92.5\n",
            "Train Epoch: 56 [108/300 (90%)]\tLoss: 1.300882\n",
            "\n",
            "Test set: Average loss: 0.0068, Accuracy: 353/400 (88.25%)\n",
            "\n",
            "88.25\n",
            "\n",
            "Test set: Average loss: 0.0105, Accuracy: 363/400 (90.75%)\n",
            "\n",
            "90.75\n",
            "\n",
            "Test set: Average loss: 0.0079, Accuracy: 360/400 (90.00%)\n",
            "\n",
            "90.0\n",
            "\n",
            "Test set: Average loss: 0.0064, Accuracy: 369/400 (92.25%)\n",
            "\n",
            "92.25\n",
            "\n",
            "Test set: Average loss: 0.0098, Accuracy: 344/400 (86.00%)\n",
            "\n",
            "86.0\n",
            "\n",
            "Test set: Average loss: 0.0084, Accuracy: 358/400 (89.50%)\n",
            "\n",
            "89.5\n",
            "\n",
            "Test set: Average loss: 0.0098, Accuracy: 365/400 (91.25%)\n",
            "\n",
            "91.25\n",
            "Train Epoch: 63 [108/300 (90%)]\tLoss: 0.236028\n",
            "\n",
            "Test set: Average loss: 0.0147, Accuracy: 346/400 (86.50%)\n",
            "\n",
            "86.5\n",
            "\n",
            "Test set: Average loss: 0.0092, Accuracy: 367/400 (91.75%)\n",
            "\n",
            "91.75\n",
            "\n",
            "Test set: Average loss: 0.0049, Accuracy: 373/400 (93.25%)\n",
            "\n",
            "93.25\n",
            "\n",
            "Test set: Average loss: 0.0084, Accuracy: 363/400 (90.75%)\n",
            "\n",
            "90.75\n",
            "\n",
            "Test set: Average loss: 0.0116, Accuracy: 359/400 (89.75%)\n",
            "\n",
            "89.75\n",
            "\n",
            "Test set: Average loss: 0.0106, Accuracy: 365/400 (91.25%)\n",
            "\n",
            "91.25\n",
            "\n",
            "Test set: Average loss: 0.0130, Accuracy: 352/400 (88.00%)\n",
            "\n",
            "88.0\n",
            "Train Epoch: 70 [108/300 (90%)]\tLoss: 0.234698\n",
            "\n",
            "Test set: Average loss: 0.0114, Accuracy: 361/400 (90.25%)\n",
            "\n",
            "90.25\n",
            "\n",
            "Test set: Average loss: 0.0068, Accuracy: 365/400 (91.25%)\n",
            "\n",
            "91.25\n",
            "\n",
            "Test set: Average loss: 0.0082, Accuracy: 364/400 (91.00%)\n",
            "\n",
            "91.0\n",
            "\n",
            "Test set: Average loss: 0.0091, Accuracy: 364/400 (91.00%)\n",
            "\n",
            "91.0\n",
            "\n",
            "Test set: Average loss: 0.0109, Accuracy: 364/400 (91.00%)\n",
            "\n",
            "91.0\n",
            "\n",
            "Test set: Average loss: 0.0127, Accuracy: 360/400 (90.00%)\n",
            "\n",
            "90.0\n",
            "\n",
            "Test set: Average loss: 0.0083, Accuracy: 363/400 (90.75%)\n",
            "\n",
            "90.75\n",
            "Train Epoch: 77 [108/300 (90%)]\tLoss: 0.738251\n",
            "\n",
            "Test set: Average loss: 0.0078, Accuracy: 355/400 (88.75%)\n",
            "\n",
            "88.75\n",
            "\n",
            "Test set: Average loss: 0.0075, Accuracy: 363/400 (90.75%)\n",
            "\n",
            "90.75\n",
            "\n",
            "Test set: Average loss: 0.0082, Accuracy: 364/400 (91.00%)\n",
            "\n",
            "91.0\n",
            "\n",
            "Test set: Average loss: 0.0083, Accuracy: 358/400 (89.50%)\n",
            "\n",
            "89.5\n",
            "\n",
            "Test set: Average loss: 0.0084, Accuracy: 361/400 (90.25%)\n",
            "\n",
            "90.25\n",
            "\n",
            "Test set: Average loss: 0.0084, Accuracy: 364/400 (91.00%)\n",
            "\n",
            "91.0\n",
            "\n",
            "Test set: Average loss: 0.0090, Accuracy: 367/400 (91.75%)\n",
            "\n",
            "91.75\n",
            "Train Epoch: 84 [108/300 (90%)]\tLoss: 0.241499\n",
            "\n",
            "Test set: Average loss: 0.0099, Accuracy: 367/400 (91.75%)\n",
            "\n",
            "91.75\n",
            "\n",
            "Test set: Average loss: 0.0103, Accuracy: 364/400 (91.00%)\n",
            "\n",
            "91.0\n",
            "\n",
            "Test set: Average loss: 0.0107, Accuracy: 362/400 (90.50%)\n",
            "\n",
            "90.5\n",
            "\n",
            "Test set: Average loss: 0.0111, Accuracy: 361/400 (90.25%)\n",
            "\n",
            "90.25\n",
            "\n",
            "Test set: Average loss: 0.0112, Accuracy: 362/400 (90.50%)\n",
            "\n",
            "90.5\n",
            "\n",
            "Test set: Average loss: 0.0114, Accuracy: 363/400 (90.75%)\n",
            "\n",
            "90.75\n",
            "\n",
            "Test set: Average loss: 0.0119, Accuracy: 363/400 (90.75%)\n",
            "\n",
            "90.75\n",
            "Train Epoch: 91 [108/300 (90%)]\tLoss: 0.010097\n",
            "\n",
            "Test set: Average loss: 0.0118, Accuracy: 359/400 (89.75%)\n",
            "\n",
            "89.75\n",
            "\n",
            "Test set: Average loss: 0.0113, Accuracy: 358/400 (89.50%)\n",
            "\n",
            "89.5\n",
            "\n",
            "Test set: Average loss: 0.0109, Accuracy: 358/400 (89.50%)\n",
            "\n",
            "89.5\n",
            "\n",
            "Test set: Average loss: 0.0104, Accuracy: 358/400 (89.50%)\n",
            "\n",
            "89.5\n",
            "\n",
            "Test set: Average loss: 0.0102, Accuracy: 363/400 (90.75%)\n",
            "\n",
            "90.75\n",
            "\n",
            "Test set: Average loss: 0.0104, Accuracy: 363/400 (90.75%)\n",
            "\n",
            "90.75\n",
            "\n",
            "Test set: Average loss: 0.0108, Accuracy: 363/400 (90.75%)\n",
            "\n",
            "90.75\n",
            "Train Epoch: 98 [108/300 (90%)]\tLoss: 0.003968\n",
            "\n",
            "Test set: Average loss: 0.0110, Accuracy: 361/400 (90.25%)\n",
            "\n",
            "90.25\n",
            "\n",
            "Test set: Average loss: 0.0110, Accuracy: 363/400 (90.75%)\n",
            "\n",
            "90.75\n",
            "\n",
            "Test set: Average loss: 0.0111, Accuracy: 362/400 (90.50%)\n",
            "\n",
            "90.5\n",
            "\n",
            "Test set: Average loss: 0.0110, Accuracy: 362/400 (90.50%)\n",
            "\n",
            "90.5\n",
            "\n",
            "Test set: Average loss: 0.0113, Accuracy: 364/400 (91.00%)\n",
            "\n",
            "91.0\n",
            "\n",
            "Test set: Average loss: 0.0113, Accuracy: 364/400 (91.00%)\n",
            "\n",
            "91.0\n",
            "\n",
            "Test set: Average loss: 0.0111, Accuracy: 363/400 (90.75%)\n",
            "\n",
            "90.75\n",
            "Train Epoch: 105 [108/300 (90%)]\tLoss: 0.010434\n",
            "\n",
            "Test set: Average loss: 0.0113, Accuracy: 362/400 (90.50%)\n",
            "\n",
            "90.5\n",
            "\n",
            "Test set: Average loss: 0.0113, Accuracy: 362/400 (90.50%)\n",
            "\n",
            "90.5\n",
            "\n",
            "Test set: Average loss: 0.0112, Accuracy: 360/400 (90.00%)\n",
            "\n",
            "90.0\n",
            "\n",
            "Test set: Average loss: 0.0113, Accuracy: 361/400 (90.25%)\n",
            "\n",
            "90.25\n",
            "\n",
            "Test set: Average loss: 0.0112, Accuracy: 361/400 (90.25%)\n",
            "\n",
            "90.25\n",
            "\n",
            "Test set: Average loss: 0.0112, Accuracy: 361/400 (90.25%)\n",
            "\n",
            "90.25\n",
            "\n",
            "Test set: Average loss: 0.0112, Accuracy: 361/400 (90.25%)\n",
            "\n",
            "90.25\n",
            "Train Epoch: 112 [108/300 (90%)]\tLoss: 0.015297\n",
            "\n",
            "Test set: Average loss: 0.0111, Accuracy: 361/400 (90.25%)\n",
            "\n",
            "90.25\n",
            "\n",
            "Test set: Average loss: 0.0112, Accuracy: 361/400 (90.25%)\n",
            "\n",
            "90.25\n",
            "\n",
            "Test set: Average loss: 0.0112, Accuracy: 362/400 (90.50%)\n",
            "\n",
            "90.5\n",
            "\n",
            "Test set: Average loss: 0.0112, Accuracy: 362/400 (90.50%)\n",
            "\n",
            "90.5\n",
            "\n",
            "Test set: Average loss: 0.0113, Accuracy: 362/400 (90.50%)\n",
            "\n",
            "90.5\n",
            "\n",
            "Test set: Average loss: 0.0112, Accuracy: 362/400 (90.50%)\n",
            "\n",
            "90.5\n",
            "\n",
            "Test set: Average loss: 0.0112, Accuracy: 361/400 (90.25%)\n",
            "\n",
            "90.25\n",
            "Train Epoch: 119 [108/300 (90%)]\tLoss: 0.066821\n",
            "\n",
            "Test set: Average loss: 0.0111, Accuracy: 362/400 (90.50%)\n",
            "\n",
            "90.5\n",
            "Seed Number is 21\n",
            "Num Samples For Training 50 Num Samples For Val 400\n",
            "Train Epoch: 0 [108/300 (90%)]\tLoss: 22.639816\n",
            "\n",
            "Test set: Average loss: 0.0355, Accuracy: 211/400 (52.75%)\n",
            "\n",
            "52.75\n",
            "\n",
            "Test set: Average loss: 0.0212, Accuracy: 253/400 (63.25%)\n",
            "\n",
            "63.25\n",
            "\n",
            "Test set: Average loss: 0.0145, Accuracy: 226/400 (56.50%)\n",
            "\n",
            "56.5\n",
            "\n",
            "Test set: Average loss: 0.0115, Accuracy: 280/400 (70.00%)\n",
            "\n",
            "70.0\n",
            "\n",
            "Test set: Average loss: 0.0105, Accuracy: 290/400 (72.50%)\n",
            "\n",
            "72.5\n",
            "\n",
            "Test set: Average loss: 0.0096, Accuracy: 309/400 (77.25%)\n",
            "\n",
            "77.25\n",
            "\n",
            "Test set: Average loss: 0.0097, Accuracy: 303/400 (75.75%)\n",
            "\n",
            "75.75\n",
            "Train Epoch: 7 [108/300 (90%)]\tLoss: 4.160328\n",
            "\n",
            "Test set: Average loss: 0.0103, Accuracy: 305/400 (76.25%)\n",
            "\n",
            "76.25\n",
            "\n",
            "Test set: Average loss: 0.0117, Accuracy: 295/400 (73.75%)\n",
            "\n",
            "73.75\n",
            "\n",
            "Test set: Average loss: 0.0125, Accuracy: 289/400 (72.25%)\n",
            "\n",
            "72.25\n",
            "\n",
            "Test set: Average loss: 0.0144, Accuracy: 290/400 (72.50%)\n",
            "\n",
            "72.5\n",
            "\n",
            "Test set: Average loss: 0.0107, Accuracy: 313/400 (78.25%)\n",
            "\n",
            "78.25\n",
            "\n",
            "Test set: Average loss: 0.0108, Accuracy: 313/400 (78.25%)\n",
            "\n",
            "78.25\n",
            "\n",
            "Test set: Average loss: 0.0105, Accuracy: 312/400 (78.00%)\n",
            "\n",
            "78.0\n",
            "Train Epoch: 14 [108/300 (90%)]\tLoss: 1.813326\n",
            "\n",
            "Test set: Average loss: 0.0120, Accuracy: 307/400 (76.75%)\n",
            "\n",
            "76.75\n",
            "\n",
            "Test set: Average loss: 0.0113, Accuracy: 314/400 (78.50%)\n",
            "\n",
            "78.5\n",
            "\n",
            "Test set: Average loss: 0.0110, Accuracy: 309/400 (77.25%)\n",
            "\n",
            "77.25\n",
            "\n",
            "Test set: Average loss: 0.0141, Accuracy: 316/400 (79.00%)\n",
            "\n",
            "79.0\n",
            "\n",
            "Test set: Average loss: 0.0123, Accuracy: 313/400 (78.25%)\n",
            "\n",
            "78.25\n",
            "\n",
            "Test set: Average loss: 0.0127, Accuracy: 304/400 (76.00%)\n",
            "\n",
            "76.0\n",
            "\n",
            "Test set: Average loss: 0.0114, Accuracy: 321/400 (80.25%)\n",
            "\n",
            "80.25\n",
            "Train Epoch: 21 [108/300 (90%)]\tLoss: 2.391985\n",
            "\n",
            "Test set: Average loss: 0.0155, Accuracy: 312/400 (78.00%)\n",
            "\n",
            "78.0\n",
            "\n",
            "Test set: Average loss: 0.0134, Accuracy: 303/400 (75.75%)\n",
            "\n",
            "75.75\n",
            "\n",
            "Test set: Average loss: 0.0188, Accuracy: 277/400 (69.25%)\n",
            "\n",
            "69.25\n",
            "\n",
            "Test set: Average loss: 0.0131, Accuracy: 296/400 (74.00%)\n",
            "\n",
            "74.0\n",
            "\n",
            "Test set: Average loss: 0.0114, Accuracy: 320/400 (80.00%)\n",
            "\n",
            "80.0\n",
            "\n",
            "Test set: Average loss: 0.0124, Accuracy: 314/400 (78.50%)\n",
            "\n",
            "78.5\n",
            "\n",
            "Test set: Average loss: 0.0167, Accuracy: 304/400 (76.00%)\n",
            "\n",
            "76.0\n",
            "Train Epoch: 28 [108/300 (90%)]\tLoss: 1.728286\n",
            "\n",
            "Test set: Average loss: 0.0165, Accuracy: 305/400 (76.25%)\n",
            "\n",
            "76.25\n",
            "\n",
            "Test set: Average loss: 0.0153, Accuracy: 312/400 (78.00%)\n",
            "\n",
            "78.0\n",
            "\n",
            "Test set: Average loss: 0.0125, Accuracy: 320/400 (80.00%)\n",
            "\n",
            "80.0\n",
            "\n",
            "Test set: Average loss: 0.0127, Accuracy: 328/400 (82.00%)\n",
            "\n",
            "82.0\n",
            "\n",
            "Test set: Average loss: 0.0136, Accuracy: 318/400 (79.50%)\n",
            "\n",
            "79.5\n",
            "\n",
            "Test set: Average loss: 0.0243, Accuracy: 262/400 (65.50%)\n",
            "\n",
            "65.5\n",
            "\n",
            "Test set: Average loss: 0.0117, Accuracy: 319/400 (79.75%)\n",
            "\n",
            "79.75\n",
            "Train Epoch: 35 [108/300 (90%)]\tLoss: 0.830528\n",
            "\n",
            "Test set: Average loss: 0.0177, Accuracy: 305/400 (76.25%)\n",
            "\n",
            "76.25\n",
            "\n",
            "Test set: Average loss: 0.0158, Accuracy: 316/400 (79.00%)\n",
            "\n",
            "79.0\n",
            "\n",
            "Test set: Average loss: 0.0183, Accuracy: 309/400 (77.25%)\n",
            "\n",
            "77.25\n",
            "\n",
            "Test set: Average loss: 0.0154, Accuracy: 302/400 (75.50%)\n",
            "\n",
            "75.5\n",
            "\n",
            "Test set: Average loss: 0.0149, Accuracy: 312/400 (78.00%)\n",
            "\n",
            "78.0\n",
            "\n",
            "Test set: Average loss: 0.0165, Accuracy: 313/400 (78.25%)\n",
            "\n",
            "78.25\n",
            "\n",
            "Test set: Average loss: 0.0190, Accuracy: 297/400 (74.25%)\n",
            "\n",
            "74.25\n",
            "Train Epoch: 42 [108/300 (90%)]\tLoss: 1.587498\n",
            "\n",
            "Test set: Average loss: 0.0137, Accuracy: 314/400 (78.50%)\n",
            "\n",
            "78.5\n",
            "\n",
            "Test set: Average loss: 0.0146, Accuracy: 332/400 (83.00%)\n",
            "\n",
            "83.0\n",
            "\n",
            "Test set: Average loss: 0.0197, Accuracy: 288/400 (72.00%)\n",
            "\n",
            "72.0\n",
            "\n",
            "Test set: Average loss: 0.0172, Accuracy: 320/400 (80.00%)\n",
            "\n",
            "80.0\n",
            "\n",
            "Test set: Average loss: 0.0122, Accuracy: 316/400 (79.00%)\n",
            "\n",
            "79.0\n",
            "\n",
            "Test set: Average loss: 0.0147, Accuracy: 310/400 (77.50%)\n",
            "\n",
            "77.5\n",
            "\n",
            "Test set: Average loss: 0.0365, Accuracy: 264/400 (66.00%)\n",
            "\n",
            "66.0\n",
            "Train Epoch: 49 [108/300 (90%)]\tLoss: 0.690860\n",
            "\n",
            "Test set: Average loss: 0.0158, Accuracy: 304/400 (76.00%)\n",
            "\n",
            "76.0\n",
            "\n",
            "Test set: Average loss: 0.0120, Accuracy: 313/400 (78.25%)\n",
            "\n",
            "78.25\n",
            "\n",
            "Test set: Average loss: 0.0176, Accuracy: 316/400 (79.00%)\n",
            "\n",
            "79.0\n",
            "\n",
            "Test set: Average loss: 0.0216, Accuracy: 314/400 (78.50%)\n",
            "\n",
            "78.5\n",
            "\n",
            "Test set: Average loss: 0.0117, Accuracy: 314/400 (78.50%)\n",
            "\n",
            "78.5\n",
            "\n",
            "Test set: Average loss: 0.0144, Accuracy: 314/400 (78.50%)\n",
            "\n",
            "78.5\n",
            "\n",
            "Test set: Average loss: 0.0139, Accuracy: 317/400 (79.25%)\n",
            "\n",
            "79.25\n",
            "Train Epoch: 56 [108/300 (90%)]\tLoss: 0.400265\n",
            "\n",
            "Test set: Average loss: 0.0160, Accuracy: 315/400 (78.75%)\n",
            "\n",
            "78.75\n",
            "\n",
            "Test set: Average loss: 0.0160, Accuracy: 326/400 (81.50%)\n",
            "\n",
            "81.5\n",
            "\n",
            "Test set: Average loss: 0.0281, Accuracy: 297/400 (74.25%)\n",
            "\n",
            "74.25\n",
            "\n",
            "Test set: Average loss: 0.0244, Accuracy: 276/400 (69.00%)\n",
            "\n",
            "69.0\n",
            "\n",
            "Test set: Average loss: 0.0209, Accuracy: 279/400 (69.75%)\n",
            "\n",
            "69.75\n",
            "\n",
            "Test set: Average loss: 0.0164, Accuracy: 310/400 (77.50%)\n",
            "\n",
            "77.5\n",
            "\n",
            "Test set: Average loss: 0.0218, Accuracy: 304/400 (76.00%)\n",
            "\n",
            "76.0\n",
            "Train Epoch: 63 [108/300 (90%)]\tLoss: 0.241249\n",
            "\n",
            "Test set: Average loss: 0.0195, Accuracy: 304/400 (76.00%)\n",
            "\n",
            "76.0\n",
            "\n",
            "Test set: Average loss: 0.0169, Accuracy: 331/400 (82.75%)\n",
            "\n",
            "82.75\n",
            "\n",
            "Test set: Average loss: 0.0195, Accuracy: 320/400 (80.00%)\n",
            "\n",
            "80.0\n",
            "\n",
            "Test set: Average loss: 0.0199, Accuracy: 321/400 (80.25%)\n",
            "\n",
            "80.25\n",
            "\n",
            "Test set: Average loss: 0.0255, Accuracy: 287/400 (71.75%)\n",
            "\n",
            "71.75\n",
            "\n",
            "Test set: Average loss: 0.0187, Accuracy: 318/400 (79.50%)\n",
            "\n",
            "79.5\n",
            "\n",
            "Test set: Average loss: 0.0136, Accuracy: 324/400 (81.00%)\n",
            "\n",
            "81.0\n",
            "Train Epoch: 70 [108/300 (90%)]\tLoss: 0.267773\n",
            "\n",
            "Test set: Average loss: 0.0145, Accuracy: 321/400 (80.25%)\n",
            "\n",
            "80.25\n",
            "\n",
            "Test set: Average loss: 0.0213, Accuracy: 311/400 (77.75%)\n",
            "\n",
            "77.75\n",
            "\n",
            "Test set: Average loss: 0.0395, Accuracy: 276/400 (69.00%)\n",
            "\n",
            "69.0\n",
            "\n",
            "Test set: Average loss: 0.0250, Accuracy: 317/400 (79.25%)\n",
            "\n",
            "79.25\n",
            "\n",
            "Test set: Average loss: 0.0243, Accuracy: 311/400 (77.75%)\n",
            "\n",
            "77.75\n",
            "\n",
            "Test set: Average loss: 0.0192, Accuracy: 321/400 (80.25%)\n",
            "\n",
            "80.25\n",
            "\n",
            "Test set: Average loss: 0.0199, Accuracy: 317/400 (79.25%)\n",
            "\n",
            "79.25\n",
            "Train Epoch: 77 [108/300 (90%)]\tLoss: 3.388571\n",
            "\n",
            "Test set: Average loss: 0.0168, Accuracy: 326/400 (81.50%)\n",
            "\n",
            "81.5\n",
            "\n",
            "Test set: Average loss: 0.0183, Accuracy: 317/400 (79.25%)\n",
            "\n",
            "79.25\n",
            "\n",
            "Test set: Average loss: 0.0168, Accuracy: 322/400 (80.50%)\n",
            "\n",
            "80.5\n",
            "\n",
            "Test set: Average loss: 0.0210, Accuracy: 317/400 (79.25%)\n",
            "\n",
            "79.25\n",
            "\n",
            "Test set: Average loss: 0.0210, Accuracy: 322/400 (80.50%)\n",
            "\n",
            "80.5\n",
            "\n",
            "Test set: Average loss: 0.0225, Accuracy: 317/400 (79.25%)\n",
            "\n",
            "79.25\n",
            "\n",
            "Test set: Average loss: 0.0211, Accuracy: 317/400 (79.25%)\n",
            "\n",
            "79.25\n",
            "Train Epoch: 84 [108/300 (90%)]\tLoss: 0.055221\n",
            "\n",
            "Test set: Average loss: 0.0218, Accuracy: 319/400 (79.75%)\n",
            "\n",
            "79.75\n",
            "\n",
            "Test set: Average loss: 0.0208, Accuracy: 321/400 (80.25%)\n",
            "\n",
            "80.25\n",
            "\n",
            "Test set: Average loss: 0.0182, Accuracy: 327/400 (81.75%)\n",
            "\n",
            "81.75\n",
            "\n",
            "Test set: Average loss: 0.0201, Accuracy: 302/400 (75.50%)\n",
            "\n",
            "75.5\n",
            "\n",
            "Test set: Average loss: 0.0167, Accuracy: 326/400 (81.50%)\n",
            "\n",
            "81.5\n",
            "\n",
            "Test set: Average loss: 0.0172, Accuracy: 325/400 (81.25%)\n",
            "\n",
            "81.25\n",
            "\n",
            "Test set: Average loss: 0.0191, Accuracy: 321/400 (80.25%)\n",
            "\n",
            "80.25\n",
            "Train Epoch: 91 [108/300 (90%)]\tLoss: 0.230548\n",
            "\n",
            "Test set: Average loss: 0.0206, Accuracy: 319/400 (79.75%)\n",
            "\n",
            "79.75\n",
            "\n",
            "Test set: Average loss: 0.0198, Accuracy: 319/400 (79.75%)\n",
            "\n",
            "79.75\n",
            "\n",
            "Test set: Average loss: 0.0194, Accuracy: 322/400 (80.50%)\n",
            "\n",
            "80.5\n",
            "\n",
            "Test set: Average loss: 0.0197, Accuracy: 321/400 (80.25%)\n",
            "\n",
            "80.25\n",
            "\n",
            "Test set: Average loss: 0.0199, Accuracy: 321/400 (80.25%)\n",
            "\n",
            "80.25\n",
            "\n",
            "Test set: Average loss: 0.0200, Accuracy: 322/400 (80.50%)\n",
            "\n",
            "80.5\n",
            "\n",
            "Test set: Average loss: 0.0202, Accuracy: 322/400 (80.50%)\n",
            "\n",
            "80.5\n",
            "Train Epoch: 98 [108/300 (90%)]\tLoss: 0.034965\n",
            "\n",
            "Test set: Average loss: 0.0206, Accuracy: 322/400 (80.50%)\n",
            "\n",
            "80.5\n",
            "\n",
            "Test set: Average loss: 0.0207, Accuracy: 323/400 (80.75%)\n",
            "\n",
            "80.75\n",
            "\n",
            "Test set: Average loss: 0.0208, Accuracy: 322/400 (80.50%)\n",
            "\n",
            "80.5\n",
            "\n",
            "Test set: Average loss: 0.0208, Accuracy: 323/400 (80.75%)\n",
            "\n",
            "80.75\n",
            "\n",
            "Test set: Average loss: 0.0210, Accuracy: 320/400 (80.00%)\n",
            "\n",
            "80.0\n",
            "\n",
            "Test set: Average loss: 0.0215, Accuracy: 322/400 (80.50%)\n",
            "\n",
            "80.5\n",
            "\n",
            "Test set: Average loss: 0.0218, Accuracy: 322/400 (80.50%)\n",
            "\n",
            "80.5\n",
            "Train Epoch: 105 [108/300 (90%)]\tLoss: 0.077258\n",
            "\n",
            "Test set: Average loss: 0.0223, Accuracy: 322/400 (80.50%)\n",
            "\n",
            "80.5\n",
            "\n",
            "Test set: Average loss: 0.0221, Accuracy: 322/400 (80.50%)\n",
            "\n",
            "80.5\n",
            "\n",
            "Test set: Average loss: 0.0218, Accuracy: 323/400 (80.75%)\n",
            "\n",
            "80.75\n",
            "\n",
            "Test set: Average loss: 0.0217, Accuracy: 323/400 (80.75%)\n",
            "\n",
            "80.75\n",
            "\n",
            "Test set: Average loss: 0.0224, Accuracy: 322/400 (80.50%)\n",
            "\n",
            "80.5\n",
            "\n",
            "Test set: Average loss: 0.0227, Accuracy: 322/400 (80.50%)\n",
            "\n",
            "80.5\n",
            "\n",
            "Test set: Average loss: 0.0231, Accuracy: 320/400 (80.00%)\n",
            "\n",
            "80.0\n",
            "Train Epoch: 112 [108/300 (90%)]\tLoss: 0.072853\n",
            "\n",
            "Test set: Average loss: 0.0231, Accuracy: 318/400 (79.50%)\n",
            "\n",
            "79.5\n",
            "\n",
            "Test set: Average loss: 0.0230, Accuracy: 322/400 (80.50%)\n",
            "\n",
            "80.5\n",
            "\n",
            "Test set: Average loss: 0.0228, Accuracy: 322/400 (80.50%)\n",
            "\n",
            "80.5\n",
            "\n",
            "Test set: Average loss: 0.0230, Accuracy: 322/400 (80.50%)\n",
            "\n",
            "80.5\n",
            "\n",
            "Test set: Average loss: 0.0230, Accuracy: 321/400 (80.25%)\n",
            "\n",
            "80.25\n",
            "\n",
            "Test set: Average loss: 0.0230, Accuracy: 322/400 (80.50%)\n",
            "\n",
            "80.5\n",
            "\n",
            "Test set: Average loss: 0.0229, Accuracy: 323/400 (80.75%)\n",
            "\n",
            "80.75\n",
            "Train Epoch: 119 [108/300 (90%)]\tLoss: 0.136112\n",
            "\n",
            "Test set: Average loss: 0.0228, Accuracy: 322/400 (80.50%)\n",
            "\n",
            "80.5\n",
            "Seed Number is 22\n",
            "Num Samples For Training 50 Num Samples For Val 400\n",
            "Train Epoch: 0 [108/300 (90%)]\tLoss: 19.530273\n",
            "\n",
            "Test set: Average loss: 0.0376, Accuracy: 159/400 (39.75%)\n",
            "\n",
            "39.75\n",
            "\n",
            "Test set: Average loss: 0.0214, Accuracy: 181/400 (45.25%)\n",
            "\n",
            "45.25\n",
            "\n",
            "Test set: Average loss: 0.0170, Accuracy: 213/400 (53.25%)\n",
            "\n",
            "53.25\n",
            "\n",
            "Test set: Average loss: 0.0156, Accuracy: 218/400 (54.50%)\n",
            "\n",
            "54.5\n",
            "\n",
            "Test set: Average loss: 0.0124, Accuracy: 267/400 (66.75%)\n",
            "\n",
            "66.75\n",
            "\n",
            "Test set: Average loss: 0.0137, Accuracy: 244/400 (61.00%)\n",
            "\n",
            "61.0\n",
            "\n",
            "Test set: Average loss: 0.0120, Accuracy: 272/400 (68.00%)\n",
            "\n",
            "68.0\n",
            "Train Epoch: 7 [108/300 (90%)]\tLoss: 3.888011\n",
            "\n",
            "Test set: Average loss: 0.0109, Accuracy: 295/400 (73.75%)\n",
            "\n",
            "73.75\n",
            "\n",
            "Test set: Average loss: 0.0100, Accuracy: 305/400 (76.25%)\n",
            "\n",
            "76.25\n",
            "\n",
            "Test set: Average loss: 0.0112, Accuracy: 306/400 (76.50%)\n",
            "\n",
            "76.5\n",
            "\n",
            "Test set: Average loss: 0.0110, Accuracy: 295/400 (73.75%)\n",
            "\n",
            "73.75\n",
            "\n",
            "Test set: Average loss: 0.0140, Accuracy: 298/400 (74.50%)\n",
            "\n",
            "74.5\n",
            "\n",
            "Test set: Average loss: 0.0185, Accuracy: 279/400 (69.75%)\n",
            "\n",
            "69.75\n",
            "\n",
            "Test set: Average loss: 0.0114, Accuracy: 310/400 (77.50%)\n",
            "\n",
            "77.5\n",
            "Train Epoch: 14 [108/300 (90%)]\tLoss: 2.007940\n",
            "\n",
            "Test set: Average loss: 0.0118, Accuracy: 309/400 (77.25%)\n",
            "\n",
            "77.25\n",
            "\n",
            "Test set: Average loss: 0.0117, Accuracy: 313/400 (78.25%)\n",
            "\n",
            "78.25\n",
            "\n",
            "Test set: Average loss: 0.0111, Accuracy: 312/400 (78.00%)\n",
            "\n",
            "78.0\n",
            "\n",
            "Test set: Average loss: 0.0149, Accuracy: 295/400 (73.75%)\n",
            "\n",
            "73.75\n",
            "\n",
            "Test set: Average loss: 0.0174, Accuracy: 290/400 (72.50%)\n",
            "\n",
            "72.5\n",
            "\n",
            "Test set: Average loss: 0.0141, Accuracy: 306/400 (76.50%)\n",
            "\n",
            "76.5\n",
            "\n",
            "Test set: Average loss: 0.0400, Accuracy: 214/400 (53.50%)\n",
            "\n",
            "53.5\n",
            "Train Epoch: 21 [108/300 (90%)]\tLoss: 2.564066\n",
            "\n",
            "Test set: Average loss: 0.0109, Accuracy: 314/400 (78.50%)\n",
            "\n",
            "78.5\n",
            "\n",
            "Test set: Average loss: 0.0144, Accuracy: 300/400 (75.00%)\n",
            "\n",
            "75.0\n",
            "\n",
            "Test set: Average loss: 0.0135, Accuracy: 312/400 (78.00%)\n",
            "\n",
            "78.0\n",
            "\n",
            "Test set: Average loss: 0.0106, Accuracy: 315/400 (78.75%)\n",
            "\n",
            "78.75\n",
            "\n",
            "Test set: Average loss: 0.0095, Accuracy: 319/400 (79.75%)\n",
            "\n",
            "79.75\n",
            "\n",
            "Test set: Average loss: 0.0122, Accuracy: 311/400 (77.75%)\n",
            "\n",
            "77.75\n",
            "\n",
            "Test set: Average loss: 0.0131, Accuracy: 307/400 (76.75%)\n",
            "\n",
            "76.75\n",
            "Train Epoch: 28 [108/300 (90%)]\tLoss: 1.363052\n",
            "\n",
            "Test set: Average loss: 0.0096, Accuracy: 320/400 (80.00%)\n",
            "\n",
            "80.0\n",
            "\n",
            "Test set: Average loss: 0.0133, Accuracy: 299/400 (74.75%)\n",
            "\n",
            "74.75\n",
            "\n",
            "Test set: Average loss: 0.0115, Accuracy: 317/400 (79.25%)\n",
            "\n",
            "79.25\n",
            "\n",
            "Test set: Average loss: 0.0129, Accuracy: 300/400 (75.00%)\n",
            "\n",
            "75.0\n",
            "\n",
            "Test set: Average loss: 0.0186, Accuracy: 282/400 (70.50%)\n",
            "\n",
            "70.5\n",
            "\n",
            "Test set: Average loss: 0.0136, Accuracy: 306/400 (76.50%)\n",
            "\n",
            "76.5\n",
            "\n",
            "Test set: Average loss: 0.0101, Accuracy: 325/400 (81.25%)\n",
            "\n",
            "81.25\n",
            "Train Epoch: 35 [108/300 (90%)]\tLoss: 0.928873\n",
            "\n",
            "Test set: Average loss: 0.0152, Accuracy: 300/400 (75.00%)\n",
            "\n",
            "75.0\n",
            "\n",
            "Test set: Average loss: 0.0138, Accuracy: 314/400 (78.50%)\n",
            "\n",
            "78.5\n",
            "\n",
            "Test set: Average loss: 0.0186, Accuracy: 302/400 (75.50%)\n",
            "\n",
            "75.5\n",
            "\n",
            "Test set: Average loss: 0.0142, Accuracy: 314/400 (78.50%)\n",
            "\n",
            "78.5\n",
            "\n",
            "Test set: Average loss: 0.0236, Accuracy: 287/400 (71.75%)\n",
            "\n",
            "71.75\n",
            "\n",
            "Test set: Average loss: 0.0163, Accuracy: 301/400 (75.25%)\n",
            "\n",
            "75.25\n",
            "\n",
            "Test set: Average loss: 0.0129, Accuracy: 304/400 (76.00%)\n",
            "\n",
            "76.0\n",
            "Train Epoch: 42 [108/300 (90%)]\tLoss: 0.526486\n",
            "\n",
            "Test set: Average loss: 0.0141, Accuracy: 316/400 (79.00%)\n",
            "\n",
            "79.0\n",
            "\n",
            "Test set: Average loss: 0.0287, Accuracy: 278/400 (69.50%)\n",
            "\n",
            "69.5\n",
            "\n",
            "Test set: Average loss: 0.0143, Accuracy: 300/400 (75.00%)\n",
            "\n",
            "75.0\n",
            "\n",
            "Test set: Average loss: 0.0152, Accuracy: 319/400 (79.75%)\n",
            "\n",
            "79.75\n",
            "\n",
            "Test set: Average loss: 0.0205, Accuracy: 312/400 (78.00%)\n",
            "\n",
            "78.0\n",
            "\n",
            "Test set: Average loss: 0.0162, Accuracy: 273/400 (68.25%)\n",
            "\n",
            "68.25\n",
            "\n",
            "Test set: Average loss: 0.0133, Accuracy: 312/400 (78.00%)\n",
            "\n",
            "78.0\n",
            "Train Epoch: 49 [108/300 (90%)]\tLoss: 0.576871\n",
            "\n",
            "Test set: Average loss: 0.0150, Accuracy: 311/400 (77.75%)\n",
            "\n",
            "77.75\n",
            "\n",
            "Test set: Average loss: 0.0152, Accuracy: 313/400 (78.25%)\n",
            "\n",
            "78.25\n",
            "\n",
            "Test set: Average loss: 0.0257, Accuracy: 294/400 (73.50%)\n",
            "\n",
            "73.5\n",
            "\n",
            "Test set: Average loss: 0.0177, Accuracy: 306/400 (76.50%)\n",
            "\n",
            "76.5\n",
            "\n",
            "Test set: Average loss: 0.0135, Accuracy: 318/400 (79.50%)\n",
            "\n",
            "79.5\n",
            "\n",
            "Test set: Average loss: 0.0250, Accuracy: 286/400 (71.50%)\n",
            "\n",
            "71.5\n",
            "\n",
            "Test set: Average loss: 0.0134, Accuracy: 308/400 (77.00%)\n",
            "\n",
            "77.0\n",
            "Train Epoch: 56 [108/300 (90%)]\tLoss: 1.035251\n",
            "\n",
            "Test set: Average loss: 0.0173, Accuracy: 304/400 (76.00%)\n",
            "\n",
            "76.0\n",
            "\n",
            "Test set: Average loss: 0.0190, Accuracy: 295/400 (73.75%)\n",
            "\n",
            "73.75\n",
            "\n",
            "Test set: Average loss: 0.0134, Accuracy: 320/400 (80.00%)\n",
            "\n",
            "80.0\n",
            "\n",
            "Test set: Average loss: 0.0153, Accuracy: 317/400 (79.25%)\n",
            "\n",
            "79.25\n",
            "\n",
            "Test set: Average loss: 0.0191, Accuracy: 311/400 (77.75%)\n",
            "\n",
            "77.75\n",
            "\n",
            "Test set: Average loss: 0.0130, Accuracy: 318/400 (79.50%)\n",
            "\n",
            "79.5\n",
            "\n",
            "Test set: Average loss: 0.0131, Accuracy: 321/400 (80.25%)\n",
            "\n",
            "80.25\n",
            "Train Epoch: 63 [108/300 (90%)]\tLoss: 0.700123\n",
            "\n",
            "Test set: Average loss: 0.0188, Accuracy: 308/400 (77.00%)\n",
            "\n",
            "77.0\n",
            "\n",
            "Test set: Average loss: 0.0190, Accuracy: 308/400 (77.00%)\n",
            "\n",
            "77.0\n",
            "\n",
            "Test set: Average loss: 0.0174, Accuracy: 305/400 (76.25%)\n",
            "\n",
            "76.25\n",
            "\n",
            "Test set: Average loss: 0.0147, Accuracy: 319/400 (79.75%)\n",
            "\n",
            "79.75\n",
            "\n",
            "Test set: Average loss: 0.0167, Accuracy: 321/400 (80.25%)\n",
            "\n",
            "80.25\n",
            "\n",
            "Test set: Average loss: 0.0194, Accuracy: 317/400 (79.25%)\n",
            "\n",
            "79.25\n",
            "\n",
            "Test set: Average loss: 0.0176, Accuracy: 321/400 (80.25%)\n",
            "\n",
            "80.25\n",
            "Train Epoch: 70 [108/300 (90%)]\tLoss: 0.738652\n",
            "\n",
            "Test set: Average loss: 0.0159, Accuracy: 317/400 (79.25%)\n",
            "\n",
            "79.25\n",
            "\n",
            "Test set: Average loss: 0.0148, Accuracy: 322/400 (80.50%)\n",
            "\n",
            "80.5\n",
            "\n",
            "Test set: Average loss: 0.0157, Accuracy: 313/400 (78.25%)\n",
            "\n",
            "78.25\n",
            "\n",
            "Test set: Average loss: 0.0176, Accuracy: 314/400 (78.50%)\n",
            "\n",
            "78.5\n",
            "\n",
            "Test set: Average loss: 0.0130, Accuracy: 322/400 (80.50%)\n",
            "\n",
            "80.5\n",
            "\n",
            "Test set: Average loss: 0.0123, Accuracy: 315/400 (78.75%)\n",
            "\n",
            "78.75\n",
            "\n",
            "Test set: Average loss: 0.0130, Accuracy: 315/400 (78.75%)\n",
            "\n",
            "78.75\n",
            "Train Epoch: 77 [108/300 (90%)]\tLoss: 0.644972\n",
            "\n",
            "Test set: Average loss: 0.0135, Accuracy: 320/400 (80.00%)\n",
            "\n",
            "80.0\n",
            "\n",
            "Test set: Average loss: 0.0167, Accuracy: 327/400 (81.75%)\n",
            "\n",
            "81.75\n",
            "\n",
            "Test set: Average loss: 0.0156, Accuracy: 324/400 (81.00%)\n",
            "\n",
            "81.0\n",
            "\n",
            "Test set: Average loss: 0.0178, Accuracy: 302/400 (75.50%)\n",
            "\n",
            "75.5\n",
            "\n",
            "Test set: Average loss: 0.0157, Accuracy: 312/400 (78.00%)\n",
            "\n",
            "78.0\n",
            "\n",
            "Test set: Average loss: 0.0164, Accuracy: 310/400 (77.50%)\n",
            "\n",
            "77.5\n",
            "\n",
            "Test set: Average loss: 0.0157, Accuracy: 323/400 (80.75%)\n",
            "\n",
            "80.75\n",
            "Train Epoch: 84 [108/300 (90%)]\tLoss: 0.579006\n",
            "\n",
            "Test set: Average loss: 0.0150, Accuracy: 320/400 (80.00%)\n",
            "\n",
            "80.0\n",
            "\n",
            "Test set: Average loss: 0.0148, Accuracy: 328/400 (82.00%)\n",
            "\n",
            "82.0\n",
            "\n",
            "Test set: Average loss: 0.0162, Accuracy: 318/400 (79.50%)\n",
            "\n",
            "79.5\n",
            "\n",
            "Test set: Average loss: 0.0187, Accuracy: 317/400 (79.25%)\n",
            "\n",
            "79.25\n",
            "\n",
            "Test set: Average loss: 0.0167, Accuracy: 324/400 (81.00%)\n",
            "\n",
            "81.0\n",
            "\n",
            "Test set: Average loss: 0.0163, Accuracy: 326/400 (81.50%)\n",
            "\n",
            "81.5\n",
            "\n",
            "Test set: Average loss: 0.0159, Accuracy: 324/400 (81.00%)\n",
            "\n",
            "81.0\n",
            "Train Epoch: 91 [108/300 (90%)]\tLoss: 0.080228\n",
            "\n",
            "Test set: Average loss: 0.0163, Accuracy: 317/400 (79.25%)\n",
            "\n",
            "79.25\n",
            "\n",
            "Test set: Average loss: 0.0165, Accuracy: 316/400 (79.00%)\n",
            "\n",
            "79.0\n",
            "\n",
            "Test set: Average loss: 0.0158, Accuracy: 324/400 (81.00%)\n",
            "\n",
            "81.0\n",
            "\n",
            "Test set: Average loss: 0.0150, Accuracy: 326/400 (81.50%)\n",
            "\n",
            "81.5\n",
            "\n",
            "Test set: Average loss: 0.0147, Accuracy: 331/400 (82.75%)\n",
            "\n",
            "82.75\n",
            "\n",
            "Test set: Average loss: 0.0145, Accuracy: 332/400 (83.00%)\n",
            "\n",
            "83.0\n",
            "\n",
            "Test set: Average loss: 0.0147, Accuracy: 332/400 (83.00%)\n",
            "\n",
            "83.0\n",
            "Train Epoch: 98 [108/300 (90%)]\tLoss: 0.060800\n",
            "\n",
            "Test set: Average loss: 0.0154, Accuracy: 330/400 (82.50%)\n",
            "\n",
            "82.5\n",
            "\n",
            "Test set: Average loss: 0.0154, Accuracy: 329/400 (82.25%)\n",
            "\n",
            "82.25\n",
            "\n",
            "Test set: Average loss: 0.0149, Accuracy: 330/400 (82.50%)\n",
            "\n",
            "82.5\n",
            "\n",
            "Test set: Average loss: 0.0146, Accuracy: 332/400 (83.00%)\n",
            "\n",
            "83.0\n",
            "\n",
            "Test set: Average loss: 0.0146, Accuracy: 334/400 (83.50%)\n",
            "\n",
            "83.5\n",
            "\n",
            "Test set: Average loss: 0.0157, Accuracy: 333/400 (83.25%)\n",
            "\n",
            "83.25\n",
            "\n",
            "Test set: Average loss: 0.0157, Accuracy: 328/400 (82.00%)\n",
            "\n",
            "82.0\n",
            "Train Epoch: 105 [108/300 (90%)]\tLoss: 0.676721\n",
            "\n",
            "Test set: Average loss: 0.0154, Accuracy: 332/400 (83.00%)\n",
            "\n",
            "83.0\n",
            "\n",
            "Test set: Average loss: 0.0151, Accuracy: 333/400 (83.25%)\n",
            "\n",
            "83.25\n",
            "\n",
            "Test set: Average loss: 0.0153, Accuracy: 334/400 (83.50%)\n",
            "\n",
            "83.5\n",
            "\n",
            "Test set: Average loss: 0.0147, Accuracy: 334/400 (83.50%)\n",
            "\n",
            "83.5\n",
            "\n",
            "Test set: Average loss: 0.0144, Accuracy: 335/400 (83.75%)\n",
            "\n",
            "83.75\n",
            "\n",
            "Test set: Average loss: 0.0147, Accuracy: 335/400 (83.75%)\n",
            "\n",
            "83.75\n",
            "\n",
            "Test set: Average loss: 0.0148, Accuracy: 335/400 (83.75%)\n",
            "\n",
            "83.75\n",
            "Train Epoch: 112 [108/300 (90%)]\tLoss: 0.120703\n",
            "\n",
            "Test set: Average loss: 0.0149, Accuracy: 334/400 (83.50%)\n",
            "\n",
            "83.5\n",
            "\n",
            "Test set: Average loss: 0.0148, Accuracy: 334/400 (83.50%)\n",
            "\n",
            "83.5\n",
            "\n",
            "Test set: Average loss: 0.0150, Accuracy: 333/400 (83.25%)\n",
            "\n",
            "83.25\n",
            "\n",
            "Test set: Average loss: 0.0149, Accuracy: 332/400 (83.00%)\n",
            "\n",
            "83.0\n",
            "\n",
            "Test set: Average loss: 0.0150, Accuracy: 334/400 (83.50%)\n",
            "\n",
            "83.5\n",
            "\n",
            "Test set: Average loss: 0.0146, Accuracy: 333/400 (83.25%)\n",
            "\n",
            "83.25\n",
            "\n",
            "Test set: Average loss: 0.0150, Accuracy: 333/400 (83.25%)\n",
            "\n",
            "83.25\n",
            "Train Epoch: 119 [108/300 (90%)]\tLoss: 1.648263\n",
            "\n",
            "Test set: Average loss: 0.0150, Accuracy: 335/400 (83.75%)\n",
            "\n",
            "83.75\n",
            "Seed Number is 23\n",
            "Num Samples For Training 50 Num Samples For Val 400\n",
            "Train Epoch: 0 [108/300 (90%)]\tLoss: 18.255548\n",
            "\n",
            "Test set: Average loss: 0.0454, Accuracy: 47/400 (11.75%)\n",
            "\n",
            "11.75\n",
            "\n",
            "Test set: Average loss: 0.0255, Accuracy: 232/400 (58.00%)\n",
            "\n",
            "58.0\n",
            "\n",
            "Test set: Average loss: 0.0150, Accuracy: 230/400 (57.50%)\n",
            "\n",
            "57.5\n",
            "\n",
            "Test set: Average loss: 0.0140, Accuracy: 236/400 (59.00%)\n",
            "\n",
            "59.0\n",
            "\n",
            "Test set: Average loss: 0.0125, Accuracy: 258/400 (64.50%)\n",
            "\n",
            "64.5\n",
            "\n",
            "Test set: Average loss: 0.0130, Accuracy: 259/400 (64.75%)\n",
            "\n",
            "64.75\n",
            "\n",
            "Test set: Average loss: 0.0114, Accuracy: 292/400 (73.00%)\n",
            "\n",
            "73.0\n",
            "Train Epoch: 7 [108/300 (90%)]\tLoss: 1.067289\n",
            "\n",
            "Test set: Average loss: 0.0158, Accuracy: 244/400 (61.00%)\n",
            "\n",
            "61.0\n",
            "\n",
            "Test set: Average loss: 0.0125, Accuracy: 284/400 (71.00%)\n",
            "\n",
            "71.0\n",
            "\n",
            "Test set: Average loss: 0.0138, Accuracy: 277/400 (69.25%)\n",
            "\n",
            "69.25\n",
            "\n",
            "Test set: Average loss: 0.0135, Accuracy: 288/400 (72.00%)\n",
            "\n",
            "72.0\n",
            "\n",
            "Test set: Average loss: 0.0174, Accuracy: 268/400 (67.00%)\n",
            "\n",
            "67.0\n",
            "\n",
            "Test set: Average loss: 0.0273, Accuracy: 224/400 (56.00%)\n",
            "\n",
            "56.0\n",
            "\n",
            "Test set: Average loss: 0.0161, Accuracy: 267/400 (66.75%)\n",
            "\n",
            "66.75\n",
            "Train Epoch: 14 [108/300 (90%)]\tLoss: 1.236036\n",
            "\n",
            "Test set: Average loss: 0.0136, Accuracy: 284/400 (71.00%)\n",
            "\n",
            "71.0\n",
            "\n",
            "Test set: Average loss: 0.0131, Accuracy: 295/400 (73.75%)\n",
            "\n",
            "73.75\n",
            "\n",
            "Test set: Average loss: 0.0160, Accuracy: 280/400 (70.00%)\n",
            "\n",
            "70.0\n",
            "\n",
            "Test set: Average loss: 0.0165, Accuracy: 280/400 (70.00%)\n",
            "\n",
            "70.0\n",
            "\n",
            "Test set: Average loss: 0.0179, Accuracy: 283/400 (70.75%)\n",
            "\n",
            "70.75\n",
            "\n",
            "Test set: Average loss: 0.0315, Accuracy: 244/400 (61.00%)\n",
            "\n",
            "61.0\n",
            "\n",
            "Test set: Average loss: 0.0151, Accuracy: 292/400 (73.00%)\n",
            "\n",
            "73.0\n",
            "Train Epoch: 21 [108/300 (90%)]\tLoss: 0.964006\n",
            "\n",
            "Test set: Average loss: 0.0159, Accuracy: 277/400 (69.25%)\n",
            "\n",
            "69.25\n",
            "\n",
            "Test set: Average loss: 0.0169, Accuracy: 268/400 (67.00%)\n",
            "\n",
            "67.0\n",
            "\n",
            "Test set: Average loss: 0.0167, Accuracy: 292/400 (73.00%)\n",
            "\n",
            "73.0\n",
            "\n",
            "Test set: Average loss: 0.0195, Accuracy: 267/400 (66.75%)\n",
            "\n",
            "66.75\n",
            "\n",
            "Test set: Average loss: 0.0151, Accuracy: 277/400 (69.25%)\n",
            "\n",
            "69.25\n",
            "\n",
            "Test set: Average loss: 0.0181, Accuracy: 284/400 (71.00%)\n",
            "\n",
            "71.0\n",
            "\n",
            "Test set: Average loss: 0.0148, Accuracy: 288/400 (72.00%)\n",
            "\n",
            "72.0\n",
            "Train Epoch: 28 [108/300 (90%)]\tLoss: 1.189674\n",
            "\n",
            "Test set: Average loss: 0.0152, Accuracy: 281/400 (70.25%)\n",
            "\n",
            "70.25\n",
            "\n",
            "Test set: Average loss: 0.0182, Accuracy: 283/400 (70.75%)\n",
            "\n",
            "70.75\n",
            "\n",
            "Test set: Average loss: 0.0199, Accuracy: 282/400 (70.50%)\n",
            "\n",
            "70.5\n",
            "\n",
            "Test set: Average loss: 0.0195, Accuracy: 279/400 (69.75%)\n",
            "\n",
            "69.75\n",
            "\n",
            "Test set: Average loss: 0.0214, Accuracy: 281/400 (70.25%)\n",
            "\n",
            "70.25\n",
            "\n",
            "Test set: Average loss: 0.0203, Accuracy: 290/400 (72.50%)\n",
            "\n",
            "72.5\n",
            "\n",
            "Test set: Average loss: 0.0151, Accuracy: 294/400 (73.50%)\n",
            "\n",
            "73.5\n",
            "Train Epoch: 35 [108/300 (90%)]\tLoss: 1.797353\n",
            "\n",
            "Test set: Average loss: 0.0224, Accuracy: 275/400 (68.75%)\n",
            "\n",
            "68.75\n",
            "\n",
            "Test set: Average loss: 0.0208, Accuracy: 267/400 (66.75%)\n",
            "\n",
            "66.75\n",
            "\n",
            "Test set: Average loss: 0.0236, Accuracy: 276/400 (69.00%)\n",
            "\n",
            "69.0\n",
            "\n",
            "Test set: Average loss: 0.0214, Accuracy: 280/400 (70.00%)\n",
            "\n",
            "70.0\n",
            "\n",
            "Test set: Average loss: 0.0224, Accuracy: 273/400 (68.25%)\n",
            "\n",
            "68.25\n",
            "\n",
            "Test set: Average loss: 0.0186, Accuracy: 264/400 (66.00%)\n",
            "\n",
            "66.0\n",
            "\n",
            "Test set: Average loss: 0.0286, Accuracy: 256/400 (64.00%)\n",
            "\n",
            "64.0\n",
            "Train Epoch: 42 [108/300 (90%)]\tLoss: 2.358418\n",
            "\n",
            "Test set: Average loss: 0.0314, Accuracy: 255/400 (63.75%)\n",
            "\n",
            "63.75\n",
            "\n",
            "Test set: Average loss: 0.0192, Accuracy: 266/400 (66.50%)\n",
            "\n",
            "66.5\n",
            "\n",
            "Test set: Average loss: 0.0196, Accuracy: 276/400 (69.00%)\n",
            "\n",
            "69.0\n",
            "\n",
            "Test set: Average loss: 0.0230, Accuracy: 281/400 (70.25%)\n",
            "\n",
            "70.25\n",
            "\n",
            "Test set: Average loss: 0.0285, Accuracy: 272/400 (68.00%)\n",
            "\n",
            "68.0\n",
            "\n",
            "Test set: Average loss: 0.0179, Accuracy: 265/400 (66.25%)\n",
            "\n",
            "66.25\n",
            "\n",
            "Test set: Average loss: 0.0226, Accuracy: 261/400 (65.25%)\n",
            "\n",
            "65.25\n",
            "Train Epoch: 49 [108/300 (90%)]\tLoss: 0.317782\n",
            "\n",
            "Test set: Average loss: 0.0244, Accuracy: 257/400 (64.25%)\n",
            "\n",
            "64.25\n",
            "\n",
            "Test set: Average loss: 0.0188, Accuracy: 279/400 (69.75%)\n",
            "\n",
            "69.75\n",
            "\n",
            "Test set: Average loss: 0.0205, Accuracy: 286/400 (71.50%)\n",
            "\n",
            "71.5\n",
            "\n",
            "Test set: Average loss: 0.0243, Accuracy: 272/400 (68.00%)\n",
            "\n",
            "68.0\n",
            "\n",
            "Test set: Average loss: 0.0149, Accuracy: 282/400 (70.50%)\n",
            "\n",
            "70.5\n",
            "\n",
            "Test set: Average loss: 0.0201, Accuracy: 284/400 (71.00%)\n",
            "\n",
            "71.0\n",
            "\n",
            "Test set: Average loss: 0.0247, Accuracy: 275/400 (68.75%)\n",
            "\n",
            "68.75\n",
            "Train Epoch: 56 [108/300 (90%)]\tLoss: 0.601961\n",
            "\n",
            "Test set: Average loss: 0.0224, Accuracy: 281/400 (70.25%)\n",
            "\n",
            "70.25\n",
            "\n",
            "Test set: Average loss: 0.0336, Accuracy: 260/400 (65.00%)\n",
            "\n",
            "65.0\n",
            "\n",
            "Test set: Average loss: 0.0205, Accuracy: 283/400 (70.75%)\n",
            "\n",
            "70.75\n",
            "\n",
            "Test set: Average loss: 0.0148, Accuracy: 282/400 (70.50%)\n",
            "\n",
            "70.5\n",
            "\n",
            "Test set: Average loss: 0.0184, Accuracy: 274/400 (68.50%)\n",
            "\n",
            "68.5\n",
            "\n",
            "Test set: Average loss: 0.0365, Accuracy: 265/400 (66.25%)\n",
            "\n",
            "66.25\n",
            "\n",
            "Test set: Average loss: 0.0273, Accuracy: 268/400 (67.00%)\n",
            "\n",
            "67.0\n",
            "Train Epoch: 63 [108/300 (90%)]\tLoss: 0.064270\n",
            "\n",
            "Test set: Average loss: 0.0206, Accuracy: 278/400 (69.50%)\n",
            "\n",
            "69.5\n",
            "\n",
            "Test set: Average loss: 0.0205, Accuracy: 276/400 (69.00%)\n",
            "\n",
            "69.0\n",
            "\n",
            "Test set: Average loss: 0.0191, Accuracy: 266/400 (66.50%)\n",
            "\n",
            "66.5\n",
            "\n",
            "Test set: Average loss: 0.0230, Accuracy: 279/400 (69.75%)\n",
            "\n",
            "69.75\n",
            "\n",
            "Test set: Average loss: 0.0315, Accuracy: 267/400 (66.75%)\n",
            "\n",
            "66.75\n",
            "\n",
            "Test set: Average loss: 0.0318, Accuracy: 284/400 (71.00%)\n",
            "\n",
            "71.0\n",
            "\n",
            "Test set: Average loss: 0.0358, Accuracy: 267/400 (66.75%)\n",
            "\n",
            "66.75\n",
            "Train Epoch: 70 [108/300 (90%)]\tLoss: 0.423691\n",
            "\n",
            "Test set: Average loss: 0.0427, Accuracy: 269/400 (67.25%)\n",
            "\n",
            "67.25\n",
            "\n",
            "Test set: Average loss: 0.0460, Accuracy: 276/400 (69.00%)\n",
            "\n",
            "69.0\n",
            "\n",
            "Test set: Average loss: 0.0387, Accuracy: 275/400 (68.75%)\n",
            "\n",
            "68.75\n",
            "\n",
            "Test set: Average loss: 0.0372, Accuracy: 256/400 (64.00%)\n",
            "\n",
            "64.0\n",
            "\n",
            "Test set: Average loss: 0.0276, Accuracy: 269/400 (67.25%)\n",
            "\n",
            "67.25\n",
            "\n",
            "Test set: Average loss: 0.0254, Accuracy: 274/400 (68.50%)\n",
            "\n",
            "68.5\n",
            "\n",
            "Test set: Average loss: 0.0230, Accuracy: 273/400 (68.25%)\n",
            "\n",
            "68.25\n",
            "Train Epoch: 77 [108/300 (90%)]\tLoss: 0.234019\n",
            "\n",
            "Test set: Average loss: 0.0243, Accuracy: 260/400 (65.00%)\n",
            "\n",
            "65.0\n",
            "\n",
            "Test set: Average loss: 0.0231, Accuracy: 271/400 (67.75%)\n",
            "\n",
            "67.75\n",
            "\n",
            "Test set: Average loss: 0.0268, Accuracy: 271/400 (67.75%)\n",
            "\n",
            "67.75\n",
            "\n",
            "Test set: Average loss: 0.0278, Accuracy: 273/400 (68.25%)\n",
            "\n",
            "68.25\n",
            "\n",
            "Test set: Average loss: 0.0276, Accuracy: 273/400 (68.25%)\n",
            "\n",
            "68.25\n",
            "\n",
            "Test set: Average loss: 0.0299, Accuracy: 263/400 (65.75%)\n",
            "\n",
            "65.75\n",
            "\n",
            "Test set: Average loss: 0.0239, Accuracy: 280/400 (70.00%)\n",
            "\n",
            "70.0\n",
            "Train Epoch: 84 [108/300 (90%)]\tLoss: 0.397459\n",
            "\n",
            "Test set: Average loss: 0.0271, Accuracy: 278/400 (69.50%)\n",
            "\n",
            "69.5\n",
            "\n",
            "Test set: Average loss: 0.0270, Accuracy: 271/400 (67.75%)\n",
            "\n",
            "67.75\n",
            "\n",
            "Test set: Average loss: 0.0237, Accuracy: 287/400 (71.75%)\n",
            "\n",
            "71.75\n",
            "\n",
            "Test set: Average loss: 0.0243, Accuracy: 286/400 (71.50%)\n",
            "\n",
            "71.5\n",
            "\n",
            "Test set: Average loss: 0.0271, Accuracy: 283/400 (70.75%)\n",
            "\n",
            "70.75\n",
            "\n",
            "Test set: Average loss: 0.0281, Accuracy: 277/400 (69.25%)\n",
            "\n",
            "69.25\n",
            "\n",
            "Test set: Average loss: 0.0304, Accuracy: 273/400 (68.25%)\n",
            "\n",
            "68.25\n",
            "Train Epoch: 91 [108/300 (90%)]\tLoss: 0.002798\n",
            "\n",
            "Test set: Average loss: 0.0302, Accuracy: 274/400 (68.50%)\n",
            "\n",
            "68.5\n",
            "\n",
            "Test set: Average loss: 0.0302, Accuracy: 275/400 (68.75%)\n",
            "\n",
            "68.75\n",
            "\n",
            "Test set: Average loss: 0.0295, Accuracy: 280/400 (70.00%)\n",
            "\n",
            "70.0\n",
            "\n",
            "Test set: Average loss: 0.0291, Accuracy: 280/400 (70.00%)\n",
            "\n",
            "70.0\n",
            "\n",
            "Test set: Average loss: 0.0290, Accuracy: 280/400 (70.00%)\n",
            "\n",
            "70.0\n",
            "\n",
            "Test set: Average loss: 0.0293, Accuracy: 280/400 (70.00%)\n",
            "\n",
            "70.0\n",
            "\n",
            "Test set: Average loss: 0.0290, Accuracy: 280/400 (70.00%)\n",
            "\n",
            "70.0\n",
            "Train Epoch: 98 [108/300 (90%)]\tLoss: 0.005889\n",
            "\n",
            "Test set: Average loss: 0.0295, Accuracy: 273/400 (68.25%)\n",
            "\n",
            "68.25\n",
            "\n",
            "Test set: Average loss: 0.0299, Accuracy: 273/400 (68.25%)\n",
            "\n",
            "68.25\n",
            "\n",
            "Test set: Average loss: 0.0299, Accuracy: 272/400 (68.00%)\n",
            "\n",
            "68.0\n",
            "\n",
            "Test set: Average loss: 0.0301, Accuracy: 276/400 (69.00%)\n",
            "\n",
            "69.0\n",
            "\n",
            "Test set: Average loss: 0.0304, Accuracy: 273/400 (68.25%)\n",
            "\n",
            "68.25\n",
            "\n",
            "Test set: Average loss: 0.0303, Accuracy: 273/400 (68.25%)\n",
            "\n",
            "68.25\n",
            "\n",
            "Test set: Average loss: 0.0298, Accuracy: 272/400 (68.00%)\n",
            "\n",
            "68.0\n",
            "Train Epoch: 105 [108/300 (90%)]\tLoss: 0.398331\n",
            "\n",
            "Test set: Average loss: 0.0302, Accuracy: 274/400 (68.50%)\n",
            "\n",
            "68.5\n",
            "\n",
            "Test set: Average loss: 0.0302, Accuracy: 274/400 (68.50%)\n",
            "\n",
            "68.5\n",
            "\n",
            "Test set: Average loss: 0.0305, Accuracy: 273/400 (68.25%)\n",
            "\n",
            "68.25\n",
            "\n",
            "Test set: Average loss: 0.0304, Accuracy: 274/400 (68.50%)\n",
            "\n",
            "68.5\n",
            "\n",
            "Test set: Average loss: 0.0308, Accuracy: 272/400 (68.00%)\n",
            "\n",
            "68.0\n",
            "\n",
            "Test set: Average loss: 0.0309, Accuracy: 273/400 (68.25%)\n",
            "\n",
            "68.25\n",
            "\n",
            "Test set: Average loss: 0.0303, Accuracy: 273/400 (68.25%)\n",
            "\n",
            "68.25\n",
            "Train Epoch: 112 [108/300 (90%)]\tLoss: 0.180182\n",
            "\n",
            "Test set: Average loss: 0.0305, Accuracy: 272/400 (68.00%)\n",
            "\n",
            "68.0\n",
            "\n",
            "Test set: Average loss: 0.0299, Accuracy: 269/400 (67.25%)\n",
            "\n",
            "67.25\n",
            "\n",
            "Test set: Average loss: 0.0308, Accuracy: 272/400 (68.00%)\n",
            "\n",
            "68.0\n",
            "\n",
            "Test set: Average loss: 0.0310, Accuracy: 272/400 (68.00%)\n",
            "\n",
            "68.0\n",
            "\n",
            "Test set: Average loss: 0.0307, Accuracy: 272/400 (68.00%)\n",
            "\n",
            "68.0\n",
            "\n",
            "Test set: Average loss: 0.0311, Accuracy: 276/400 (69.00%)\n",
            "\n",
            "69.0\n",
            "\n",
            "Test set: Average loss: 0.0310, Accuracy: 275/400 (68.75%)\n",
            "\n",
            "68.75\n",
            "Train Epoch: 119 [108/300 (90%)]\tLoss: 0.246795\n",
            "\n",
            "Test set: Average loss: 0.0316, Accuracy: 270/400 (67.50%)\n",
            "\n",
            "67.5\n",
            "Seed Number is 24\n",
            "Num Samples For Training 50 Num Samples For Val 400\n",
            "Train Epoch: 0 [108/300 (90%)]\tLoss: 16.248524\n",
            "\n",
            "Test set: Average loss: 0.0349, Accuracy: 126/400 (31.50%)\n",
            "\n",
            "31.5\n",
            "\n",
            "Test set: Average loss: 0.0202, Accuracy: 193/400 (48.25%)\n",
            "\n",
            "48.25\n",
            "\n",
            "Test set: Average loss: 0.0154, Accuracy: 209/400 (52.25%)\n",
            "\n",
            "52.25\n",
            "\n",
            "Test set: Average loss: 0.0132, Accuracy: 255/400 (63.75%)\n",
            "\n",
            "63.75\n",
            "\n",
            "Test set: Average loss: 0.0130, Accuracy: 267/400 (66.75%)\n",
            "\n",
            "66.75\n",
            "\n",
            "Test set: Average loss: 0.0152, Accuracy: 236/400 (59.00%)\n",
            "\n",
            "59.0\n",
            "\n",
            "Test set: Average loss: 0.0119, Accuracy: 287/400 (71.75%)\n",
            "\n",
            "71.75\n",
            "Train Epoch: 7 [108/300 (90%)]\tLoss: 2.345008\n",
            "\n",
            "Test set: Average loss: 0.0141, Accuracy: 265/400 (66.25%)\n",
            "\n",
            "66.25\n",
            "\n",
            "Test set: Average loss: 0.0162, Accuracy: 257/400 (64.25%)\n",
            "\n",
            "64.25\n",
            "\n",
            "Test set: Average loss: 0.0130, Accuracy: 289/400 (72.25%)\n",
            "\n",
            "72.25\n",
            "\n",
            "Test set: Average loss: 0.0140, Accuracy: 288/400 (72.00%)\n",
            "\n",
            "72.0\n",
            "\n",
            "Test set: Average loss: 0.0144, Accuracy: 284/400 (71.00%)\n",
            "\n",
            "71.0\n",
            "\n",
            "Test set: Average loss: 0.0190, Accuracy: 272/400 (68.00%)\n",
            "\n",
            "68.0\n",
            "\n",
            "Test set: Average loss: 0.0154, Accuracy: 283/400 (70.75%)\n",
            "\n",
            "70.75\n",
            "Train Epoch: 14 [108/300 (90%)]\tLoss: 2.038974\n",
            "\n",
            "Test set: Average loss: 0.0141, Accuracy: 279/400 (69.75%)\n",
            "\n",
            "69.75\n",
            "\n",
            "Test set: Average loss: 0.0163, Accuracy: 273/400 (68.25%)\n",
            "\n",
            "68.25\n",
            "\n",
            "Test set: Average loss: 0.0153, Accuracy: 280/400 (70.00%)\n",
            "\n",
            "70.0\n",
            "\n",
            "Test set: Average loss: 0.0223, Accuracy: 245/400 (61.25%)\n",
            "\n",
            "61.25\n",
            "\n",
            "Test set: Average loss: 0.0240, Accuracy: 236/400 (59.00%)\n",
            "\n",
            "59.0\n",
            "\n",
            "Test set: Average loss: 0.0140, Accuracy: 274/400 (68.50%)\n",
            "\n",
            "68.5\n",
            "\n",
            "Test set: Average loss: 0.0200, Accuracy: 272/400 (68.00%)\n",
            "\n",
            "68.0\n",
            "Train Epoch: 21 [108/300 (90%)]\tLoss: 0.962187\n",
            "\n",
            "Test set: Average loss: 0.0145, Accuracy: 282/400 (70.50%)\n",
            "\n",
            "70.5\n",
            "\n",
            "Test set: Average loss: 0.0190, Accuracy: 276/400 (69.00%)\n",
            "\n",
            "69.0\n",
            "\n",
            "Test set: Average loss: 0.0164, Accuracy: 276/400 (69.00%)\n",
            "\n",
            "69.0\n",
            "\n",
            "Test set: Average loss: 0.0179, Accuracy: 269/400 (67.25%)\n",
            "\n",
            "67.25\n",
            "\n",
            "Test set: Average loss: 0.0244, Accuracy: 265/400 (66.25%)\n",
            "\n",
            "66.25\n",
            "\n",
            "Test set: Average loss: 0.0182, Accuracy: 274/400 (68.50%)\n",
            "\n",
            "68.5\n",
            "\n",
            "Test set: Average loss: 0.0193, Accuracy: 285/400 (71.25%)\n",
            "\n",
            "71.25\n",
            "Train Epoch: 28 [108/300 (90%)]\tLoss: 1.875329\n",
            "\n",
            "Test set: Average loss: 0.0182, Accuracy: 277/400 (69.25%)\n",
            "\n",
            "69.25\n",
            "\n",
            "Test set: Average loss: 0.0191, Accuracy: 273/400 (68.25%)\n",
            "\n",
            "68.25\n",
            "\n",
            "Test set: Average loss: 0.0195, Accuracy: 274/400 (68.50%)\n",
            "\n",
            "68.5\n",
            "\n",
            "Test set: Average loss: 0.0177, Accuracy: 277/400 (69.25%)\n",
            "\n",
            "69.25\n",
            "\n",
            "Test set: Average loss: 0.0232, Accuracy: 260/400 (65.00%)\n",
            "\n",
            "65.0\n",
            "\n",
            "Test set: Average loss: 0.0197, Accuracy: 276/400 (69.00%)\n",
            "\n",
            "69.0\n",
            "\n",
            "Test set: Average loss: 0.0215, Accuracy: 274/400 (68.50%)\n",
            "\n",
            "68.5\n",
            "Train Epoch: 35 [108/300 (90%)]\tLoss: 1.576484\n",
            "\n",
            "Test set: Average loss: 0.0207, Accuracy: 273/400 (68.25%)\n",
            "\n",
            "68.25\n",
            "\n",
            "Test set: Average loss: 0.0186, Accuracy: 269/400 (67.25%)\n",
            "\n",
            "67.25\n",
            "\n",
            "Test set: Average loss: 0.0260, Accuracy: 266/400 (66.50%)\n",
            "\n",
            "66.5\n",
            "\n",
            "Test set: Average loss: 0.0206, Accuracy: 270/400 (67.50%)\n",
            "\n",
            "67.5\n",
            "\n",
            "Test set: Average loss: 0.0159, Accuracy: 271/400 (67.75%)\n",
            "\n",
            "67.75\n",
            "\n",
            "Test set: Average loss: 0.0215, Accuracy: 279/400 (69.75%)\n",
            "\n",
            "69.75\n",
            "\n",
            "Test set: Average loss: 0.0292, Accuracy: 268/400 (67.00%)\n",
            "\n",
            "67.0\n",
            "Train Epoch: 42 [108/300 (90%)]\tLoss: 1.182774\n",
            "\n",
            "Test set: Average loss: 0.0201, Accuracy: 246/400 (61.50%)\n",
            "\n",
            "61.5\n",
            "\n",
            "Test set: Average loss: 0.0185, Accuracy: 272/400 (68.00%)\n",
            "\n",
            "68.0\n",
            "\n",
            "Test set: Average loss: 0.0303, Accuracy: 267/400 (66.75%)\n",
            "\n",
            "66.75\n",
            "\n",
            "Test set: Average loss: 0.0241, Accuracy: 273/400 (68.25%)\n",
            "\n",
            "68.25\n",
            "\n",
            "Test set: Average loss: 0.0174, Accuracy: 275/400 (68.75%)\n",
            "\n",
            "68.75\n",
            "\n",
            "Test set: Average loss: 0.0189, Accuracy: 275/400 (68.75%)\n",
            "\n",
            "68.75\n",
            "\n",
            "Test set: Average loss: 0.0212, Accuracy: 280/400 (70.00%)\n",
            "\n",
            "70.0\n",
            "Train Epoch: 49 [108/300 (90%)]\tLoss: 1.960019\n",
            "\n",
            "Test set: Average loss: 0.0332, Accuracy: 274/400 (68.50%)\n",
            "\n",
            "68.5\n",
            "\n",
            "Test set: Average loss: 0.0216, Accuracy: 268/400 (67.00%)\n",
            "\n",
            "67.0\n",
            "\n",
            "Test set: Average loss: 0.0196, Accuracy: 269/400 (67.25%)\n",
            "\n",
            "67.25\n",
            "\n",
            "Test set: Average loss: 0.0291, Accuracy: 268/400 (67.00%)\n",
            "\n",
            "67.0\n",
            "\n",
            "Test set: Average loss: 0.0321, Accuracy: 261/400 (65.25%)\n",
            "\n",
            "65.25\n",
            "\n",
            "Test set: Average loss: 0.0324, Accuracy: 258/400 (64.50%)\n",
            "\n",
            "64.5\n",
            "\n",
            "Test set: Average loss: 0.0197, Accuracy: 274/400 (68.50%)\n",
            "\n",
            "68.5\n",
            "Train Epoch: 56 [108/300 (90%)]\tLoss: 0.690505\n",
            "\n",
            "Test set: Average loss: 0.0285, Accuracy: 260/400 (65.00%)\n",
            "\n",
            "65.0\n",
            "\n",
            "Test set: Average loss: 0.0200, Accuracy: 276/400 (69.00%)\n",
            "\n",
            "69.0\n",
            "\n",
            "Test set: Average loss: 0.0282, Accuracy: 259/400 (64.75%)\n",
            "\n",
            "64.75\n",
            "\n",
            "Test set: Average loss: 0.0226, Accuracy: 272/400 (68.00%)\n",
            "\n",
            "68.0\n",
            "\n",
            "Test set: Average loss: 0.0244, Accuracy: 284/400 (71.00%)\n",
            "\n",
            "71.0\n",
            "\n",
            "Test set: Average loss: 0.0269, Accuracy: 273/400 (68.25%)\n",
            "\n",
            "68.25\n",
            "\n",
            "Test set: Average loss: 0.0365, Accuracy: 274/400 (68.50%)\n",
            "\n",
            "68.5\n",
            "Train Epoch: 63 [108/300 (90%)]\tLoss: 0.165931\n",
            "\n",
            "Test set: Average loss: 0.0225, Accuracy: 280/400 (70.00%)\n",
            "\n",
            "70.0\n",
            "\n",
            "Test set: Average loss: 0.0194, Accuracy: 272/400 (68.00%)\n",
            "\n",
            "68.0\n",
            "\n",
            "Test set: Average loss: 0.0246, Accuracy: 286/400 (71.50%)\n",
            "\n",
            "71.5\n",
            "\n",
            "Test set: Average loss: 0.0349, Accuracy: 262/400 (65.50%)\n",
            "\n",
            "65.5\n",
            "\n",
            "Test set: Average loss: 0.0232, Accuracy: 275/400 (68.75%)\n",
            "\n",
            "68.75\n",
            "\n",
            "Test set: Average loss: 0.0206, Accuracy: 279/400 (69.75%)\n",
            "\n",
            "69.75\n",
            "\n",
            "Test set: Average loss: 0.0203, Accuracy: 279/400 (69.75%)\n",
            "\n",
            "69.75\n",
            "Train Epoch: 70 [108/300 (90%)]\tLoss: 0.329774\n",
            "\n",
            "Test set: Average loss: 0.0210, Accuracy: 282/400 (70.50%)\n",
            "\n",
            "70.5\n",
            "\n",
            "Test set: Average loss: 0.0202, Accuracy: 276/400 (69.00%)\n",
            "\n",
            "69.0\n",
            "\n",
            "Test set: Average loss: 0.0217, Accuracy: 268/400 (67.00%)\n",
            "\n",
            "67.0\n",
            "\n",
            "Test set: Average loss: 0.0225, Accuracy: 271/400 (67.75%)\n",
            "\n",
            "67.75\n",
            "\n",
            "Test set: Average loss: 0.0261, Accuracy: 272/400 (68.00%)\n",
            "\n",
            "68.0\n",
            "\n",
            "Test set: Average loss: 0.0270, Accuracy: 276/400 (69.00%)\n",
            "\n",
            "69.0\n",
            "\n",
            "Test set: Average loss: 0.0269, Accuracy: 279/400 (69.75%)\n",
            "\n",
            "69.75\n",
            "Train Epoch: 77 [108/300 (90%)]\tLoss: 2.267755\n",
            "\n",
            "Test set: Average loss: 0.0262, Accuracy: 278/400 (69.50%)\n",
            "\n",
            "69.5\n",
            "\n",
            "Test set: Average loss: 0.0266, Accuracy: 275/400 (68.75%)\n",
            "\n",
            "68.75\n",
            "\n",
            "Test set: Average loss: 0.0291, Accuracy: 273/400 (68.25%)\n",
            "\n",
            "68.25\n",
            "\n",
            "Test set: Average loss: 0.0326, Accuracy: 267/400 (66.75%)\n",
            "\n",
            "66.75\n",
            "\n",
            "Test set: Average loss: 0.0300, Accuracy: 266/400 (66.50%)\n",
            "\n",
            "66.5\n",
            "\n",
            "Test set: Average loss: 0.0302, Accuracy: 262/400 (65.50%)\n",
            "\n",
            "65.5\n",
            "\n",
            "Test set: Average loss: 0.0256, Accuracy: 265/400 (66.25%)\n",
            "\n",
            "66.25\n",
            "Train Epoch: 84 [108/300 (90%)]\tLoss: 0.658611\n",
            "\n",
            "Test set: Average loss: 0.0240, Accuracy: 266/400 (66.50%)\n",
            "\n",
            "66.5\n",
            "\n",
            "Test set: Average loss: 0.0269, Accuracy: 264/400 (66.00%)\n",
            "\n",
            "66.0\n",
            "\n",
            "Test set: Average loss: 0.0295, Accuracy: 263/400 (65.75%)\n",
            "\n",
            "65.75\n",
            "\n",
            "Test set: Average loss: 0.0313, Accuracy: 272/400 (68.00%)\n",
            "\n",
            "68.0\n",
            "\n",
            "Test set: Average loss: 0.0347, Accuracy: 265/400 (66.25%)\n",
            "\n",
            "66.25\n",
            "\n",
            "Test set: Average loss: 0.0310, Accuracy: 273/400 (68.25%)\n",
            "\n",
            "68.25\n",
            "\n",
            "Test set: Average loss: 0.0308, Accuracy: 271/400 (67.75%)\n",
            "\n",
            "67.75\n",
            "Train Epoch: 91 [108/300 (90%)]\tLoss: 0.067843\n",
            "\n",
            "Test set: Average loss: 0.0313, Accuracy: 266/400 (66.50%)\n",
            "\n",
            "66.5\n",
            "\n",
            "Test set: Average loss: 0.0324, Accuracy: 264/400 (66.00%)\n",
            "\n",
            "66.0\n",
            "\n",
            "Test set: Average loss: 0.0311, Accuracy: 271/400 (67.75%)\n",
            "\n",
            "67.75\n",
            "\n",
            "Test set: Average loss: 0.0298, Accuracy: 275/400 (68.75%)\n",
            "\n",
            "68.75\n",
            "\n",
            "Test set: Average loss: 0.0298, Accuracy: 271/400 (67.75%)\n",
            "\n",
            "67.75\n",
            "\n",
            "Test set: Average loss: 0.0299, Accuracy: 269/400 (67.25%)\n",
            "\n",
            "67.25\n",
            "\n",
            "Test set: Average loss: 0.0303, Accuracy: 272/400 (68.00%)\n",
            "\n",
            "68.0\n",
            "Train Epoch: 98 [108/300 (90%)]\tLoss: 0.034406\n",
            "\n",
            "Test set: Average loss: 0.0303, Accuracy: 269/400 (67.25%)\n",
            "\n",
            "67.25\n",
            "\n",
            "Test set: Average loss: 0.0313, Accuracy: 264/400 (66.00%)\n",
            "\n",
            "66.0\n",
            "\n",
            "Test set: Average loss: 0.0310, Accuracy: 267/400 (66.75%)\n",
            "\n",
            "66.75\n",
            "\n",
            "Test set: Average loss: 0.0312, Accuracy: 266/400 (66.50%)\n",
            "\n",
            "66.5\n",
            "\n",
            "Test set: Average loss: 0.0316, Accuracy: 268/400 (67.00%)\n",
            "\n",
            "67.0\n",
            "\n",
            "Test set: Average loss: 0.0316, Accuracy: 267/400 (66.75%)\n",
            "\n",
            "66.75\n",
            "\n",
            "Test set: Average loss: 0.0320, Accuracy: 268/400 (67.00%)\n",
            "\n",
            "67.0\n",
            "Train Epoch: 105 [108/300 (90%)]\tLoss: 1.298450\n",
            "\n",
            "Test set: Average loss: 0.0321, Accuracy: 271/400 (67.75%)\n",
            "\n",
            "67.75\n",
            "\n",
            "Test set: Average loss: 0.0315, Accuracy: 267/400 (66.75%)\n",
            "\n",
            "66.75\n",
            "\n",
            "Test set: Average loss: 0.0310, Accuracy: 267/400 (66.75%)\n",
            "\n",
            "66.75\n",
            "\n",
            "Test set: Average loss: 0.0311, Accuracy: 266/400 (66.50%)\n",
            "\n",
            "66.5\n",
            "\n",
            "Test set: Average loss: 0.0309, Accuracy: 269/400 (67.25%)\n",
            "\n",
            "67.25\n",
            "\n",
            "Test set: Average loss: 0.0313, Accuracy: 268/400 (67.00%)\n",
            "\n",
            "67.0\n",
            "\n",
            "Test set: Average loss: 0.0314, Accuracy: 269/400 (67.25%)\n",
            "\n",
            "67.25\n",
            "Train Epoch: 112 [108/300 (90%)]\tLoss: 0.031034\n",
            "\n",
            "Test set: Average loss: 0.0320, Accuracy: 267/400 (66.75%)\n",
            "\n",
            "66.75\n",
            "\n",
            "Test set: Average loss: 0.0322, Accuracy: 266/400 (66.50%)\n",
            "\n",
            "66.5\n",
            "\n",
            "Test set: Average loss: 0.0314, Accuracy: 268/400 (67.00%)\n",
            "\n",
            "67.0\n",
            "\n",
            "Test set: Average loss: 0.0321, Accuracy: 265/400 (66.25%)\n",
            "\n",
            "66.25\n",
            "\n",
            "Test set: Average loss: 0.0318, Accuracy: 267/400 (66.75%)\n",
            "\n",
            "66.75\n",
            "\n",
            "Test set: Average loss: 0.0320, Accuracy: 268/400 (67.00%)\n",
            "\n",
            "67.0\n",
            "\n",
            "Test set: Average loss: 0.0323, Accuracy: 265/400 (66.25%)\n",
            "\n",
            "66.25\n",
            "Train Epoch: 119 [108/300 (90%)]\tLoss: 1.277517\n",
            "\n",
            "Test set: Average loss: 0.0319, Accuracy: 269/400 (67.25%)\n",
            "\n",
            "67.25\n",
            "Mean Train Acc over 25 seeds: 77.70% +- 0.095\n",
            "Mean Loss Acc over 25 seeds: 1.72% +- 0.0075\n"
          ]
        }
      ],
      "source": [
        "normalize = transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))\n",
        "\n",
        "\n",
        "transform__ = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.RandomResizedCrop(32, scale=(0.75, 1.0), ratio=(1.0, 1.0)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandAugment(num_ops=1, magnitude=8),\n",
        "    transforms.ColorJitter(0.1, 0.1, 0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261)),\n",
        "    transforms.RandomErasing(p=0.25)\n",
        "])\n",
        "'''\n",
        "transforms.Compose([\n",
        "          transforms.ToPILImage(),\n",
        "          #transforms.Resize(64),\n",
        "          transforms.RandomAffine(0,               # Shear transformation with max angle of 0\n",
        "                              translate=(0.2, 0.2),  # Translation range of 20%\n",
        "                              scale=None,\n",
        "                              shear=40),\n",
        "          transforms.TrivialAugmentWide(),\n",
        "          transforms.RandomAdjustSharpness(2),\n",
        "          transforms.AugMix(),\n",
        "          transforms.ToTensor(),\n",
        "          transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261)),\n",
        "      ])#transforms.Compose([normalize])\n",
        "'''\n",
        "\n",
        "\n",
        "transform_val = transforms.Compose([transforms.ToTensor(), normalize])\n",
        "#transforms.Compose([transforms.Resize(224),transforms.ToTensor(), normalize])\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "##### Cifar Data\n",
        "cifar_data = datasets.CIFAR10(root='.',train=True, download=True)\n",
        "\n",
        "#We need two copies of this due to weird dataset api\n",
        "cifar_data_val = datasets.CIFAR10(root='.',train=True, transform = transform_val, download=True)\n",
        "\n",
        "\n",
        "accs = []\n",
        "num_epochs = 120\n",
        "step = 7\n",
        "l2_lambda = 0.001\n",
        "num_classes = 120\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "#vit = ViT(image_size, patch_size, num_classes, dim, depth, heads, mlp_dim,dropout)\n",
        "print(device)\n",
        "print('Hyperparameter set')\n",
        "#mixer = ConvMixer(256, 8, kernel_size=5, n_classes=10)\n",
        "lambda1 = lambda epoch : 0.1\n",
        "acc = 100\n",
        "alpha = [0.0, 0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,1.0]\n",
        "lr_max = 0.01\n",
        "wd = 0.01\n",
        "Valid_accuracies = []\n",
        "Valid_loss = []\n",
        "Train_loss = []\n",
        "Train_accuracy = []\n",
        "for seed in range(25):\n",
        "  print('Seed Number is',seed)\n",
        "  # save results\n",
        "  #model = MyEnsemble(Net1(),Net2(),Net3(),Net4(),Net5(),Net6(),Net7(),Net8(),Net9(),Net10()).to(device)#vit\n",
        "  model1 = Net2().to(device)\n",
        "  model2 = Net3().to(device)\n",
        "  model3 = Net7().to(device)\n",
        "  model4 = Net10().to(device)\n",
        "  F_model = Classifier().to(device)\n",
        "\n",
        "  lr = 0.001\n",
        "  optimizer = torch.optim.AdamW(list(model1.parameters())+list(model2.parameters())+list(model3.parameters())+list(model4.parameters())+list(F_model.parameters()), lr=lr_max, weight_decay=wd) #torch.optim.Adam(model.parameters(), lr=lr, betas=(0.9, 0.999),weight_decay = 2e-5)\n",
        "  #optimizer = torch.optim.AdamW(model.parameters(), lr=lr_max, weight_decay=wd) #torch.optim.Adam(model.parameters(), lr=lr, betas=(0.9, 0.999),weight_decay = 2e-5)\n",
        "\n",
        "  lr_schedule = lambda t: np.interp([t], [0, num_epochs*2//5, num_epochs*4//5, num_epochs],\n",
        "                                  [0, lr_max, lr_max/20.0, 0])[0]\n",
        "  prng = RandomState(seed)\n",
        "  random_permute = prng.permutation(np.arange(0, 1000))\n",
        "  classes =  prng.permutation(np.arange(0,10))\n",
        "  indx_train = np.concatenate([np.where(np.array(cifar_data.targets) == classe)[0][random_permute[0:25]] for classe in classes[0:2]])\n",
        "  indx_val = np.concatenate([np.where(np.array(cifar_data.targets) == classe)[0][random_permute[25:225]] for classe in classes[0:2]])\n",
        "\n",
        "\n",
        "  train_data = Subset(cifar_data, indx_train)\n",
        "  val_data = Subset(cifar_data_val, indx_val)\n",
        "\n",
        "  print('Num Samples For Training %d Num Samples For Val %d'%(train_data.indices.shape[0],val_data.indices.shape[0]))\n",
        "  train_data.dataset.transform = transforms.ToTensor()\n",
        "\n",
        "  train_loader = torch.utils.data.DataLoader(train_data,\n",
        "                                             batch_size=50,\n",
        "                                             shuffle=False)\n",
        "\n",
        "  val_loader = torch.utils.data.DataLoader(val_data,\n",
        "                                           batch_size=50,\n",
        "                                           shuffle=True)\n",
        "  trainset = []\n",
        "  targetset = []\n",
        "  for data, target in (train_loader):\n",
        "    trainset.append(data)\n",
        "    targetset.append(target)\n",
        "  trainset = torch.stack(trainset).squeeze()\n",
        "  targetset = torch.stack(targetset)\n",
        "\n",
        "  X,Y = Offline_aug(trainset,targetset)\n",
        "\n",
        "  train_dataset_normal = CustomTensorDataset(tensors=(X, Y), transform= transform__ )#transforms.Compose([trans,transforms.ToTensor()])) #\n",
        "\n",
        "  train_loader_new = torch.utils.data.DataLoader(train_dataset_normal,\n",
        "                                              batch_size=32,\n",
        "                                              shuffle=True)\n",
        "  for epoch in range (num_epochs):\n",
        "    model1.train()\n",
        "    model2.train()\n",
        "    model3.train()\n",
        "    F_model.train()\n",
        "    model4.train()\n",
        "    train_loss = 0\n",
        "    correct_tr = 0\n",
        "    for batch_idx,(data, target) in enumerate(train_loader_new):\n",
        "\n",
        "      data, target = data.to(device), target.to(device)\n",
        "      optimizer.zero_grad()\n",
        "      choice1 = random.randint(1, len(data)-1)\n",
        "      choice2 = random.randint(1, len(data)-1)\n",
        "      lr = lr_schedule(epoch + (batch_idx + 1)/len(train_loader_new))\n",
        "      optimizer.param_groups[0].update(lr=lr)\n",
        "      '''\n",
        "      datas = [data[choice1].unsqueeze(0)*a+data[choice2].unsqueeze(0)*(1.0-a) for a in alpha]\n",
        "      datas = torch.vstack(datas)\n",
        "\n",
        "      data_new = torch.vstack((data,datas))\n",
        "\n",
        "      tars = [target[choice1]*a+target[choice2]*(1.0-a) for a in alpha]\n",
        "      tars = torch.vstack(tars).squeeze()\n",
        "\n",
        "      target_new = torch.hstack((target,tars))\n",
        "      '''\n",
        "\n",
        "      output1 = (model1(data.float())).squeeze()\n",
        "      output2 = (model2(data.float())).squeeze()\n",
        "      output3 = (model3(data.float())).squeeze()\n",
        "      output4 = (model4(data.float())).squeeze()\n",
        "      out = torch.cat((output1,output2,output3,output4),dim = 1)\n",
        "      output_F = (F_model(out)).squeeze()\n",
        "\n",
        "\n",
        "      loss1 = F.cross_entropy(output1, target)\n",
        "      loss2 = F.cross_entropy(output2, target)\n",
        "      loss3 = F.cross_entropy(output3, target)\n",
        "      loss_F = F.cross_entropy(output_F, target)\n",
        "      loss4 = F.cross_entropy(output4, target)\n",
        "\n",
        "      loss = loss1+loss2+loss3+loss_F+loss4\n",
        "\n",
        "      train_loss +=loss\n",
        "      #loss = F.cross_entropy(output, target)\n",
        "      pred = output_F.max(1, keepdim=True)[1]\n",
        "      correct_tr += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "    train_loss /= len(train_loader_new.dataset)\n",
        "    Train_loss.append(train_loss)\n",
        "    Train_accuracy.append(correct_tr / len(train_loader_new.dataset))\n",
        "    #print(f\"Loss at epoch {epoch} = {mean_loss}\")\n",
        "    if epoch%step==0:\n",
        "      print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "        epoch, batch_idx * len(data), len(train_loader_new.dataset),\n",
        "      100. * batch_idx / len(train_loader_new), loss.item()))\n",
        "      '''\n",
        "      if epoch !=0:\n",
        "        lr_scheduler.step()\n",
        "        print('Learning Rate is %4f'%(optimizer.param_groups[0]['lr']))\n",
        "        print('Epoch Number %d'%(epoch+1))\n",
        "      '''\n",
        "    model1.eval()\n",
        "    model2.eval()\n",
        "    model3.eval()\n",
        "    F_model.eval()\n",
        "    model4.eval()\n",
        "\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for data, target in val_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "\n",
        "            output1 = (model1(data.float())).squeeze()\n",
        "            output2 = (model2(data.float())).squeeze()\n",
        "            output3 = (model3(data.float())).squeeze()\n",
        "            output4 = (model4(data.float())).squeeze()\n",
        "            out = torch.cat((output1,output2,output3,output4),dim = 1)\n",
        "            output_F = (F_model(out)).squeeze()\n",
        "\n",
        "            #output = (model(data.float())).squeeze()\n",
        "\n",
        "            test_loss += F.cross_entropy(output_F, target).item() # sum up batch loss\n",
        "            pred = output_F.max(1, keepdim=True)[1]\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    #print(f\"Loss at epoch {epoch} = {mean_loss}\")\n",
        "    test_loss /= len(val_loader.dataset)\n",
        "    Valid_loss.append(test_loss)\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
        "        test_loss, correct, len(val_loader.dataset),\n",
        "        100. * correct / len(val_loader.dataset)))\n",
        "    print(100. * correct / len(val_loader.dataset))\n",
        "\n",
        "    Valid_accuracies.append(correct / len(val_loader.dataset))\n",
        "\n",
        "  with open('/content/drive/MyDrive/Results1/Train_loss', 'wb') as fp:\n",
        "      pickle.dump(Train_accuracy, fp)\n",
        "  with open('/content/drive/MyDrive/Results1/Train_Accuracy', 'wb') as fp:\n",
        "      pickle.dump(Train_loss, fp)\n",
        "  with open('/content/drive/MyDrive/Results1/Valid_loss', 'wb') as fp:\n",
        "      pickle.dump(Valid_loss, fp)\n",
        "  with open('/content/drive/MyDrive/Results1/Valid_accuracy', 'wb') as fp:\n",
        "      pickle.dump(Valid_accuracies, fp)\n",
        "\n",
        "print(f'Mean Train Acc over 25 seeds: '\\\n",
        "      f'{np.mean(Valid_accuracies):.2%} '\\\n",
        "      f'+- {np.std(Valid_accuracies):.2}')\n",
        "\n",
        "print(f'Mean Loss Acc over 25 seeds: '\\\n",
        "      f'{np.mean(Valid_loss):.2%} '\\\n",
        "      f'+- {np.std(Valid_loss):.2}')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "with open('/content/drive/MyDrive/Results1/Train_Accuracy','rb') as f:\n",
        "  x = pickle.load(f)\n",
        "\n",
        "x1 = torch.stack(x)\n",
        "#x2 = []\n",
        "for i in range(25):\n",
        "  x2 = x1[i*120:i*120+120]\n",
        "  plt.plot(x2.detach().cpu())\n",
        "  plt.title('Loss Curve Over 25 Seeds')\n",
        "  plt.xlabel('Epoch')\n",
        "\n",
        "with open('/content/drive/MyDrive/Results1/Train_loss','rb') as f:\n",
        "  x12 =(pickle.load(f))\n",
        "\n",
        "x22 = torch.tensor(x12)\n",
        "#x2 = []\n",
        "plt.figure()\n",
        "for i in range(25):\n",
        "  x2 = x22[i*120:i*120+120]\n",
        "  plt.plot(x2.detach().cpu())\n",
        "  plt.title('Accuracy Curve Over 25 Seeds')\n",
        "  plt.xlabel('Epoch')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 927
        },
        "id": "kiE89ZDzhGNb",
        "outputId": "09754ce6-228b-4a24-af04-a78cc96e0a3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}